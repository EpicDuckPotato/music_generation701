{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_notes = 128\n",
    "num_time_shifts = 100\n",
    "num_velocities = 32\n",
    "message_dim = 2*num_notes + num_velocities + num_time_shifts\n",
    "instrument_numbers = [0, 6, 40, 41, 42, 43, 45, 60, 68, 70, 71, 73]\n",
    "num_instruments = len(instrument_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Music Transformer version of our network ######\n",
    "# Uses the relative position representation from the Music Transformer\n",
    "\n",
    "\n",
    "# HuangMHA: multi-headed attention using relative position representation\n",
    "# (specifically, the representation introduced by Shaw and optimized by Huang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TransformerXL version of our network #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Transformer definition\n",
    "Uses absolute position representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://pytorch.org/tutorials/beginner/transformer_tutorial.html.\n",
    "# Only change is the view/expand in forward (accounts for multiple instruments and batches)\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.shape[0], :].view(x.shape[0], 1, 1, -1).expand(-1, x.shape[1], x.shape[2], -1)\n",
    "        return self.dropout(x)\n",
    "        \n",
    "\n",
    "# EnsembleTransformer: takes a history of MIDI messages \n",
    "# for instruments in an ensemble and generates a distribution for the next message\n",
    "# for a specific instrument\n",
    "class EnsembleTransformer(torch.nn.Module):\n",
    "    # CONSTRUCTOR\n",
    "    # ARGUMENTS\n",
    "    # message_dim: dimension of a MIDI message\n",
    "    # embed_dim: dimension of message embedding\n",
    "    # num_instruments: number of instrument labels\n",
    "    # heads: number of attention heads\n",
    "    # attention_layers: number of attention layers\n",
    "    # ff_size: size of the feedforward output at the end of the decoder\n",
    "    def __init__(self, message_dim, embed_dim, num_instruments, heads, attention_layers, ff_size):\n",
    "        super(EnsembleTransformer, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # We add the tanhed instrument embedding to each input message \n",
    "        # (this is the global conditioning idea from DeepJ, which comes from WaveNet)\n",
    "        self.i_embedding = torch.nn.Embedding(num_instruments, embed_dim)\n",
    "        \n",
    "        self.position_encoding = PositionalEncoding(embed_dim)\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(message_dim, embed_dim)\n",
    "        \n",
    "        # A decoder is used to transform the history of the instrument we're generating\n",
    "        # music for, then combine this with the other histories to generate the next message\n",
    "        decoder_layer = torch.nn.TransformerDecoderLayer(embed_dim, heads, ff_size)\n",
    "        self.decoder = torch.nn.TransformerDecoder(decoder_layer, attention_layers)\n",
    "        \n",
    "        self.logits = torch.nn.Linear(embed_dim, message_dim)\n",
    "    \n",
    "    # forward: generates a probability distribution for the next MIDI message\n",
    "    # for an instrument, given the message history of the ensemble\n",
    "    # ARGUMENTS\n",
    "    # history: an LxNxB tensor, where L is the length of the longest history in\n",
    "    # the batch, N is the max number of instruments in the batch, and B is the batch size. As we\n",
    "    # walk along the first dimension, we should see indices of MIDI events for a particular\n",
    "    # instrument\n",
    "    # mask: an LxNxB tensor, containing True where a message or instrument doesn't exist\n",
    "    # instruments: a 1xNxB tensor indicating the instrument numbers for each batch\n",
    "    # gen_idx: a the index (along dimension 1 of history) of the instrument we want to generate music for\n",
    "    # seq_lengths: an NxB tensor indicating the length of each unpadded sequence\n",
    "    # RETURN: an LxBxD tensor, representing the distribution for\n",
    "    # the next MIDI message for the instrument indicated by gen_idx at each time step. \n",
    "    # Note, to get the actual probabilities you'll have to take the softmax\n",
    "    # of this tensor along dimension 1\n",
    "    def forward(self, history, mask, instruments, gen_idx, seq_lengths):\n",
    "        L = history.shape[0] # longest length\n",
    "        N = history.shape[1] # max instruments\n",
    "        B = history.shape[2] # batch size\n",
    "        assert(instruments.shape == (1, N, B))\n",
    "        assert(mask.shape == history.shape)\n",
    "        \n",
    "        inputs = self.embedding(history) + torch.tanh(self.i_embedding(instruments)).expand(L, -1, -1, -1)\n",
    "        \n",
    "        inputs = self.position_encoding(inputs)\n",
    "        \n",
    "        memory_key_padding_mask = torch.zeros((1, 1), dtype=torch.bool)\n",
    "        if N == 1:\n",
    "            memory = torch.zeros((1, B, self.embed_dim))\n",
    "            mem_lengths = None\n",
    "        else:\n",
    "            memory_idx = [i for i in range(N) if i != gen_idx]\n",
    "            memory = inputs[:, memory_idx]\n",
    "            memory_key_padding_mask = mask[:, memory_idx]\n",
    "            mem_lengths = seq_lengths[memory_idx]\n",
    "\n",
    "        decoder_inputs = inputs[:, gen_idx]\n",
    "        tgt_key_padding_mask = mask[:, gen_idx].transpose(0, 1)\n",
    "        \n",
    "        chunk_size = 200\n",
    "        decoding = torch.zeros((L, B, embed_dim))\n",
    "        \n",
    "        # Only works for batch size 1\n",
    "        decode_L = seq_lengths[gen_idx, 0]\n",
    "        \n",
    "        for i in range(decode_L%chunk_size, decode_L + 1, chunk_size):\n",
    "            if i == 0:\n",
    "                continue\n",
    "                \n",
    "            end = i\n",
    "            start = max(end - chunk_size, 0)\n",
    "            size = end - start\n",
    "            \n",
    "            tgt_mask = torch.triu(torch.ones((size, size), dtype=torch.bool))\n",
    "            tgt_mask.fill_diagonal_(False)\n",
    "            \n",
    "            decoder_input_chunk = decoder_inputs[start:end]\n",
    "            \n",
    "            tgt_chunk_key_padding_mask = tgt_key_padding_mask[:, start:end]\n",
    "            \n",
    "            if N == 1:\n",
    "                memory_chunk = memory\n",
    "                memory_chunk_key_padding_mask = memory_key_padding_mask\n",
    "                memory_mask = None\n",
    "            else:\n",
    "                memory_chunk = torch.zeros((size, N - 1, B, embed_dim))\n",
    "                memory_chunk_key_padding_mask = torch.ones((size, N - 1, B), dtype=torch.bool)\n",
    "                memory_mask = torch.zeros((size, size*(N - 1)), dtype=torch.bool)\n",
    "                for i in range(N - 1):\n",
    "                    for b in range(B):\n",
    "                        mem_end = min(mem_lengths[i, b], end)\n",
    "                        mem_start = max(mem_end - size, 0)\n",
    "                        mem_size = mem_end - mem_start\n",
    "                        memory_chunk[:mem_size, i, b] = memory[mem_start:mem_end, i, b]\n",
    "                        memory_chunk_key_padding_mask[:mem_size, i, b] = memory_key_padding_mask[mem_start:mem_end, i, b]\n",
    "                        \n",
    "                        # This is a hack that only works for batch size B = 1\n",
    "                        mem_mask_end = size*(i + 1)\n",
    "                        mem_mask_start = min(size*i + start - mem_start + 1, mem_mask_end)\n",
    "                        memory_mask[:, mem_mask_start:mem_mask_end] = torch.triu(torch.ones((size, mem_mask_end - mem_mask_start), dtype=torch.bool))\n",
    "                    \n",
    "                memory_chunk = memory_chunk.view(-1, B, embed_dim)\n",
    "                memory_chunk_key_padding_mask = memory_chunk_key_padding_mask.view(-1, B).transpose(0, 1)\n",
    "                \n",
    "            assert(not memory_mask.all())               \n",
    "            assert(not memory_chunk_key_padding_mask.all())\n",
    "            assert(not tgt_mask.all())\n",
    "            assert(not tgt_chunk_key_padding_mask.all())\n",
    "            assert(tgt_mask.shape[0] == decoder_input_chunk.shape[0])\n",
    "\n",
    "            decoding[start:end] = self.decoder(decoder_input_chunk, \\\n",
    "                                               memory_chunk, \\\n",
    "                                               tgt_mask=tgt_mask, \\\n",
    "                                               memory_mask=memory_mask, \\\n",
    "                                               tgt_key_padding_mask=tgt_chunk_key_padding_mask, \\\n",
    "                                               memory_key_padding_mask=memory_chunk_key_padding_mask)\n",
    "                \n",
    "        return self.logits(decoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests for baseline transformer\n",
    "We train with model.eval() to disable dropout, since these tests try to get the model to overfit to a small sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model to overfit to a single instrument's part in a single song (only the first 100 time steps). Tests decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_size = 512\n",
    "heads = 8\n",
    "attention_layers=  6\n",
    "embed_dim = 256\n",
    "\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleTransformer(message_dim, embed_dim, num_instruments, heads, attention_layers, ff_size)\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "    \n",
    "model.eval() # Training with eval just to see if we can overfit without dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('overfit_single_instrument.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = np.load('preprocessed_data/recording318.npy', allow_pickle=True)\n",
    "instruments_np = np.load('preprocessed_data/instruments318.npy', allow_pickle=True)\n",
    "\n",
    "inst = 1\n",
    "\n",
    "max_seq_length = 1000\n",
    "\n",
    "history = torch.tensor(recording[inst][:max_seq_length], dtype=torch.long).view(-1, 1, 1)\n",
    "mask = torch.zeros(history.shape, dtype=torch.bool)\n",
    "instruments = torch.tensor(instrument_numbers.index(instruments_np[inst]), dtype=torch.long).view(1, 1, 1)\n",
    "seq_lengths = torch.tensor(max_seq_length).view(1, 1)\n",
    "\n",
    "max_instruments = history.shape[1]\n",
    "\n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 150\n",
    "train_losses = np.zeros(epochs)\n",
    "\n",
    "target_messages = history[1:].flatten()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    # Index of the instrument we want to generate music for (there's only one instrument)\n",
    "    gen_idx = 0\n",
    "    \n",
    "    # Move forward in time\n",
    "    logits = model(history[:-1], mask[:-1], instruments, gen_idx, seq_lengths).view(-1, message_dim)\n",
    "                \n",
    "    loss = loss_fn(logits, target_messages)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses[epoch] = loss.data\n",
    "    print('Loss: %f' %(loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'overfit_single_instrument2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f88880d6a90>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Sample from model\n",
    "gen_history = torch.tensor(recording[inst][0], dtype=torch.long).view(-1, 1, 1)\n",
    "mask = torch.zeros((1, 1, 1), dtype=torch.bool)\n",
    "instruments = torch.tensor(instrument_numbers.index(instruments_np[inst]), dtype=torch.long).view(1, 1, 1)\n",
    "model.eval() # Turns off the dropout for evaluation. Need to do this to get repeatable evaluation outputs\n",
    "\n",
    "seq_length = 100\n",
    "max_instruments = history.shape[1]\n",
    "\n",
    "# Index of the instrument we want to generate music for (there's only one instrument)\n",
    "gen_idx = 0\n",
    "\n",
    "# Move forward in time\n",
    "wrong_cnt = 0\n",
    "for t in range(1, seq_length):\n",
    "    logits = model(gen_history, mask, instruments, gen_idx)\n",
    "    probs = torch.nn.functional.softmax(logits[t - 1], dim=1)\n",
    "    gen_history = torch.cat((gen_history, torch.multinomial(probs, 1).view(1, 1, -1)))\n",
    "    #gen_history = torch.cat((gen_history, torch.argmax(probs.flatten()).view(1, 1, 1)))\n",
    "    if torch.argmax(probs.flatten()) != history[t].flatten():\n",
    "        wrong_cnt += 1\n",
    "        print(torch.topk(probs.flatten(), 10))\n",
    "        print(history[t])\n",
    "    \n",
    "    mask = torch.cat((mask, torch.zeros((1, 1, 1), dtype=torch.bool)))\n",
    "\n",
    "print(wrong_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test_history.npy', np.array([history.flatten().numpy()], dtype='object'))\n",
    "np.save('test_instruments.npy', np.array([instrument_numbers[instruments[0]]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model to overfit to two instruments' parts in a single song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_size = 512\n",
    "heads = 8\n",
    "attention_layers=  6\n",
    "embed_dim = 256\n",
    "\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleTransformer(message_dim, embed_dim, num_instruments, heads, attention_layers, ff_size)\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "    \n",
    "model.eval() # Training with eval just to see if we can overfit without dropout\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('overfit_two_instruments_memchunk.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = np.load('preprocessed_data/recording318.npy', allow_pickle=True)\n",
    "instruments_np = np.load('preprocessed_data/instruments318.npy', allow_pickle=True)\n",
    "\n",
    "max_seq_length = 1000\n",
    "max_instruments = instruments_np.shape[0]\n",
    "batch_size = 1\n",
    "\n",
    "history = torch.zeros((max_seq_length, max_instruments, batch_size), dtype=torch.long)\n",
    "mask = torch.ones(history.shape, dtype=torch.bool)\n",
    "instruments = torch.tensor([instrument_numbers.index(instruments_np[i]) for i in range(max_instruments)], dtype=torch.long).view(1, -1, 1)\n",
    "\n",
    "for inst in range(max_instruments):\n",
    "    history[:max_seq_length, inst, 0] = torch.tensor(recording[inst][:max_seq_length], dtype=torch.long)\n",
    "    mask[:max_seq_length, inst, 0] = False\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 300\n",
    "train_losses = np.zeros(epochs)\n",
    "num_targets = max_seq_length - 1\n",
    "\n",
    "seq_lengths = torch.tensor(max_seq_length).view(1, 1).expand(2, -1)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    \n",
    "    loss = torch.tensor([0], dtype=torch.float32)\n",
    "    for inst in range(max_instruments):\n",
    "        logits = model(history[:-1], mask[:-1], instruments, inst, seq_lengths)\n",
    "\n",
    "        logits = logits.view(-1, message_dim)\n",
    "        target_messages = history[1:, inst].flatten()\n",
    "        output_mask = torch.logical_not(mask[1:, inst].flatten())\n",
    "        loss = loss + loss_fn(logits[output_mask], target_messages[output_mask])\n",
    "                \n",
    "    loss = loss/max_instruments\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    train_losses[epoch] = loss.data\n",
    "    print('Loss = %f' %(loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'overfit_two_instruments_memchunk.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9a84317290>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = np.load('preprocessed_data/recording318.npy', allow_pickle=True)\n",
    "instruments_np = np.load('preprocessed_data/instruments318.npy', allow_pickle=True)\n",
    "\n",
    "max_seq_length = 1000\n",
    "max_instruments = instruments_np.shape[0]\n",
    "batch_size = 1\n",
    "\n",
    "history = torch.zeros((max_seq_length, max_instruments, batch_size), dtype=torch.long)\n",
    "mask = torch.ones(history.shape, dtype=torch.bool)\n",
    "instruments = torch.tensor([instrument_numbers.index(instruments_np[i]) for i in range(max_instruments)], dtype=torch.long).view(1, -1, 1)\n",
    "\n",
    "for inst in range(max_instruments):\n",
    "    history[:max_seq_length, inst, 0] = torch.tensor(recording[inst][:max_seq_length], dtype=torch.long)\n",
    "    mask[:max_seq_length, inst, 0] = False\n",
    "\n",
    "# Check if the instruments can jointly reconstruct the piece\n",
    "gen_history = history.clone()\n",
    "\n",
    "model.eval() # Turns off the dropout for evaluation. Need to do this to get repeatable evaluation outputs\n",
    "\n",
    "wrong_cnt = 0\n",
    "\n",
    "for t in range(1, max_seq_length):\n",
    "    input_mask = mask.clone()\n",
    "    input_mask[t:] = True\n",
    "    seq_lengths = torch.tensor(t).view(1, 1).expand(max_instruments, 1)\n",
    "    for inst in range(max_instruments):\n",
    "        logits = model(gen_history, input_mask, instruments, inst, seq_lengths)\n",
    "        probs = torch.nn.functional.softmax(logits[t - 1], dim=1)\n",
    "        #gen_history[t, inst, 0] = torch.multinomial(probs, 1)\n",
    "        gen_history[t, inst, 0] = torch.argmax(probs.flatten())\n",
    "        if torch.argmax(probs.flatten()) != history[t, inst].flatten():\n",
    "            wrong_cnt += 1\n",
    "            print('wrong')\n",
    "           \n",
    "    if t%50 == 0:\n",
    "        print(t)\n",
    "\n",
    "print(wrong_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test sequences of different length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_size = 512\n",
    "heads = 8\n",
    "attention_layers=  6\n",
    "embed_dim = 256\n",
    "\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleTransformer(message_dim, embed_dim, num_instruments, heads, attention_layers, ff_size)\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "    \n",
    "model.eval() # Training with eval just to see if we can overfit without dropout\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = np.load('preprocessed_data/recording318.npy', allow_pickle=True)\n",
    "instruments_np = np.load('preprocessed_data/instruments318.npy', allow_pickle=True)\n",
    "\n",
    "length1 = 500\n",
    "length2 = 1000\n",
    "max_seq_length = 1000\n",
    "\n",
    "seq_lengths = torch.tensor([length1, length2]).unsqueeze(1)\n",
    "\n",
    "max_instruments = instruments_np.shape[0]\n",
    "batch_size = 1\n",
    "\n",
    "history = torch.zeros((max_seq_length, max_instruments, batch_size), dtype=torch.long)\n",
    "mask = torch.ones(history.shape, dtype=torch.bool)\n",
    "instruments = torch.tensor([instrument_numbers.index(instruments_np[i]) for i in range(max_instruments)], dtype=torch.long).view(1, -1, 1)\n",
    "\n",
    "for inst in range(max_instruments):\n",
    "    history[:seq_lengths[inst], inst, 0] = torch.tensor(recording[inst][:seq_lengths[inst]], dtype=torch.long)\n",
    "    mask[:seq_lengths[inst], inst, 0] = False\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 300\n",
    "train_losses = np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    \n",
    "    loss = torch.tensor([0], dtype=torch.float32)\n",
    "    for inst in range(max_instruments):\n",
    "        num_targets = seq_lengths[inst] - 1\n",
    "        \n",
    "        logits = model(history[:-1], mask[:-1], instruments, inst, seq_lengths - 1)\n",
    "\n",
    "        logits = logits.view(-1, message_dim)\n",
    "        target_messages = history[1:, inst].flatten()\n",
    "        output_mask = torch.logical_not(mask[1:, inst].flatten())\n",
    "        loss = loss + loss_fn(logits[output_mask], target_messages[output_mask])\n",
    "                \n",
    "    loss = loss/max_instruments\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    '''\n",
    "    for name, p in model.named_parameters():\n",
    "        if p.grad is not None:\n",
    "            print(name)\n",
    "            print(p.grad.norm())\n",
    "    '''\n",
    "    \n",
    "    optimizer.step()\n",
    "    train_losses[epoch] = loss.data\n",
    "    print('Loss = %f' %(loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the instruments can jointly reconstruct the piece\n",
    "gen_history = history.clone()\n",
    "\n",
    "model.eval() # Turns off the dropout for evaluation. Need to do this to get repeatable evaluation outputs\n",
    "\n",
    "wrong_cnt = 0\n",
    "\n",
    "gen_seq_lengths = torch.tensor([1, 1]).view(max_instruments, 1)\n",
    "\n",
    "for t in range(1, max_seq_length):\n",
    "    input_mask = mask.clone()\n",
    "    input_mask[t:] = True\n",
    "    for inst in range(max_instruments):\n",
    "        if seq_lengths[inst, 0] >= t:\n",
    "            continue\n",
    "            \n",
    "        logits = model(gen_history, input_mask, instruments, inst, gen_seq_lengths)\n",
    "        probs = torch.nn.functional.softmax(logits[t - 1], dim=1)\n",
    "        #gen_history[t, inst, 0] = torch.multinomial(probs, 1)\n",
    "        gen_history[t, inst, 0] = torch.argmax(probs.flatten())\n",
    "        if torch.argmax(probs.flatten()) != history[t, inst].flatten():\n",
    "            wrong_cnt += 1\n",
    "            print('wrong')\n",
    "           \n",
    "    if t%50 == 0:\n",
    "        print(t)\n",
    "\n",
    "print(wrong_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class MIDIDataset(torch.utils.data.Dataset):\n",
    "    # CONSTRUCTOR: creates a list of recording files and a list\n",
    "    # of instrument files in root_dir. Assumes that the directory\n",
    "    # contains recording0.npy to recordingM.npy,\n",
    "    # as well as instruments0.npy to instrumentsM.npy\n",
    "    # ARGUMENTS\n",
    "    # root_dir: the directory to search\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        files = os.listdir(root_dir)\n",
    "        self.recordings = []\n",
    "        self.instrument_files = []\n",
    "        for file in files:\n",
    "            if 'recording' in file:\n",
    "                self.recordings.append(os.path.join(root_dir, file))\n",
    "            elif 'instruments' in file:\n",
    "                self.instrument_files.append(os.path.join(root_dir, file))\n",
    "                \n",
    "        assert(len(self.recordings) == len(self.instrument_files))\n",
    "        self.recordings.sort()\n",
    "        self.instrument_files.sort()\n",
    "        self.transform = transform\n",
    "\n",
    "    # __len__\n",
    "    # RETURN: the number of recording files in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.recordings)\n",
    "\n",
    "    # __getitem__\n",
    "    # ARGUMENTS\n",
    "    # idx: indicates which file to get\n",
    "    # RETURN: an instance with keys 'instruments', 'history'\n",
    "    # instance['history'] is a numpy array of message sequences for each instrument\n",
    "    # instance['instruments'] a numpy array of instrument numbers\n",
    "    def __getitem__(self, idx):\n",
    "        instance = {'history': np.load(self.recordings[idx], allow_pickle=True), \\\n",
    "                    'instruments': np.load(self.instrument_files[idx], allow_pickle=True)}\n",
    "        \n",
    "        assert(len(instance['history']) == len(instance['instruments']))\n",
    "        \n",
    "        if self.transform:\n",
    "            instance = self.transform(instance)\n",
    "            \n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn: takes a list of samples from the dataset and turns them into a batch.\n",
    "# ARGUMENTS\n",
    "# batch: a list of dictionaries\n",
    "# RETURN: a sample with keys 'history', 'instruments', and 'mask'\n",
    "# sample['history']: an LxNxB tensor containing messages\n",
    "# sample['instruments']: a 1xNxB tensor containing instrument numbers\n",
    "# sample['mask']: an LxNxB tensor containing False where a message is\n",
    "# valid, and True where it isn't (accounts for variable length sequences\n",
    "# and zero padding)\n",
    "# sample['seq_lengths']: an NxB tensor containing the length of each sequence\n",
    "def collate_fn(batch):\n",
    "    length_cap = 10000\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    # We size our tensors to accomodate the longest sequence and the largest ensemble\n",
    "    max_instruments = max([len(instance['history']) for instance in batch])\n",
    "    longest_len = min(max([max([seq.shape[0] for seq in instance['history']]) for instance in batch]), length_cap)\n",
    "\n",
    "    sample = {'history': torch.ones((longest_len, max_instruments, batch_size), dtype=torch.long), \\\n",
    "              'instruments': torch.zeros((1, max_instruments, batch_size), dtype=torch.long), \\\n",
    "              'mask': torch.ones((longest_len, max_instruments, batch_size), dtype=torch.bool), \\\n",
    "              'seq_lengths': torch.zeros((max_instruments, batch_size), dtype=torch.long)}\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        instrument_idx = [instrument_numbers.index(inst) for inst in batch[b]['instruments']]\n",
    "        sample['instruments'][0, :len(instrument_idx), b] = torch.tensor(instrument_idx, dtype=torch.long)\n",
    "        \n",
    "        for inst_idx in range(len(batch[b]['history'])):\n",
    "            seq_length = min(length_cap, len(batch[b]['history'][inst_idx]))\n",
    "            sample['seq_lengths'][inst_idx, b] = seq_length\n",
    "            sample['history'][:seq_length, inst_idx, b] = torch.tensor(batch[b]['history'][inst_idx][:seq_length], dtype=torch.long)\n",
    "            sample['mask'][:seq_length, inst_idx, b] = False\n",
    "            \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_size = 512\n",
    "heads = 8\n",
    "attention_layers=  6\n",
    "embed_dim = 256\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleTransformer(message_dim, embed_dim, len(instrument_numbers), heads, attention_layers, ff_size)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Starting iteration 0\n",
      "tensor([6.1357])\n",
      "Starting iteration 1\n",
      "tensor([5.9965])\n",
      "Starting iteration 2\n",
      "tensor([5.8795])\n",
      "Starting iteration 3\n",
      "tensor([6.1061])\n",
      "Starting iteration 4\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "python_error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-04131157ae42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mmax_instruments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \"\"\"\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    121\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    122\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: python_error"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "dataset = MIDIDataset('preprocessed_data')\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "train_losses = np.zeros(epochs)\n",
    "test_losses = np.zeros(epochs) # TODO: train/test split. Can we do this with Dataloader?\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    for b, batch in enumerate(dataloader):\n",
    "        print('Starting iteration %d' %(b))\n",
    "        max_seq_length = batch['history'].shape[0]\n",
    "        num_targets = max_seq_length - 1 # Messages start from t = 0, but we start generating at t = 1\n",
    "        max_instruments = batch['history'].shape[1]\n",
    "        loss = torch.tensor([0])\n",
    "        for inst in range(max_instruments):         \n",
    "            mask = batch['mask']\n",
    "            \n",
    "            logits = model(batch['history'][:-1], mask[:-1], batch['instruments'], inst, batch['seq_lengths'] - 1)\n",
    "            logits = logits.view(-1, message_dim)\n",
    "            target_messages = batch['history'][1:, inst].flatten()\n",
    "            output_mask = torch.logical_not(mask[1:, inst].flatten())\n",
    "            loss = loss + loss_fn(logits[output_mask], target_messages[output_mask])\n",
    "        \n",
    "        loss /= max_instruments\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses[epoch] += loss.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # Disable dropout to make results repeatable\n",
    "\n",
    "time_steps = 5000 # How many time steps do we sample?\n",
    "\n",
    "max_instruments = 3\n",
    "\n",
    "# Piano, violin, viola\n",
    "instruments = torch.tensor([0, 2, 3]).view(1, max_instruments, 1)\n",
    "\n",
    "# Suppose they all start with the same velocity message\n",
    "# TODO: should we have SOS and EOS tokens like in NLP?\n",
    "gen_history = 24*torch.ones((1, max_instruments, 1), dtype=torch.long)\n",
    "mask = torch.zeros((1, 1, 1), dtype=torch.bool)\n",
    "\n",
    "for t in range(1, time_steps):\n",
    "    # Sanity check\n",
    "    if t%100 == 0:\n",
    "        print(t)\n",
    "    next_messages = torch.zeros((1, max_instruments, 1), dtype=torch.long)\n",
    "    seq_lengths = torch.tensor(gen_history.shape[0]).view(1, 1).expand(max_instruments, -1)\n",
    "    for inst in range(max_instruments):\n",
    "        logits = model(gen_history, mask.expand(t, max_instruments, -1), instruments, inst, seq_lengths)\n",
    "        probs = torch.nn.functional.softmax(logits[t - 1], dim=1)\n",
    "        next_messages[0, inst, 0] = torch.multinomial(probs, 1)\n",
    "    \n",
    "    gen_history = torch.cat((gen_history, next_messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
