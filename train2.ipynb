{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_notes = 128\n",
    "num_time_shifts = 100\n",
    "num_velocities = 32\n",
    "message_dim = 2*num_notes + num_velocities + num_time_shifts\n",
    "instrument_numbers = [0, 6, 40, 41, 42, 43, 45, 60, 68, 70, 71, 73]\n",
    "num_instruments = len(instrument_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Music Transformer version of our network ######\n",
    "# Uses the relative position representation from the Music Transformer\n",
    "\n",
    "\n",
    "# HuangMHA: multi-headed attention using relative position representation\n",
    "# (specifically, the representation introduced by Shaw and optimized by Huang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TransformerXL version of our network #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Transformer definition\n",
    "Uses absolute position representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://pytorch.org/tutorials/beginner/transformer_tutorial.html.\n",
    "# Only change is the view/expand in forward (accounts for multiple instruments and batches)\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.shape[0], :].view(x.shape[0], 1, 1, -1).expand(-1, x.shape[1], x.shape[2], -1)\n",
    "        return self.dropout(x)\n",
    "        \n",
    "\n",
    "# EnsembleTransformer: takes a history of MIDI messages \n",
    "# for instruments in an ensemble and generates a distribution for the next message\n",
    "# for a specific instrument\n",
    "class EnsembleTransformer(torch.nn.Module):\n",
    "    # CONSTRUCTOR\n",
    "    # ARGUMENTS\n",
    "    # message_dim: dimension of a MIDI message\n",
    "    # embed_dim: dimension of message embedding\n",
    "    # num_instruments: number of instrument labels\n",
    "    # heads: number of attention heads\n",
    "    # attention_layers: number of attention layers\n",
    "    # ff_size: size of the feedforward output at the end of the decoder\n",
    "    def __init__(self, message_dim, embed_dim, num_instruments, heads, attention_layers, ff_size):\n",
    "        super(EnsembleTransformer, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # We add the tanhed instrument embedding to each input message \n",
    "        # (this is the global conditioning idea from DeepJ, which comes from WaveNet)\n",
    "        self.i_embedding = torch.nn.Embedding(num_instruments, embed_dim)\n",
    "        \n",
    "        self.position_encoding = PositionalEncoding(embed_dim)\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(message_dim, embed_dim)\n",
    "        \n",
    "        # An encoder is used to transform histories of all instruments in the ensemble\n",
    "        # except the instrument we're generating music for\n",
    "        encoder_layer = torch.nn.TransformerEncoderLayer(embed_dim, heads, ff_size)\n",
    "        self.encoder = torch.nn.TransformerEncoder(encoder_layer, attention_layers)\n",
    "        \n",
    "        # A decoder is used to transform the history of the instrument we're generating\n",
    "        # music for, then combine this with the encoder output to generate the next message\n",
    "        decoder_layer = torch.nn.TransformerDecoderLayer(embed_dim, heads, ff_size)\n",
    "        self.decoder = torch.nn.TransformerDecoder(decoder_layer, attention_layers)\n",
    "        \n",
    "        self.logits = torch.nn.Linear(embed_dim, message_dim)\n",
    "    \n",
    "    # forward: generates a probability distribution for the next MIDI message\n",
    "    # for an instrument, given the message history of the ensemble\n",
    "    # ARGUMENTS\n",
    "    # history: an LxNxB tensor, where L is the length of the longest history in\n",
    "    # the batch, N is the max number of instruments in the batch, and B is the batch size. As we\n",
    "    # walk along the first dimension, we should see indices of MIDI events for a particular\n",
    "    # instrument\n",
    "    # mask: an LxNxB tensor, containing True where a message or instrument doesn't exist\n",
    "    # instruments: a 1xNxB tensor indicating the instrument numbers for each batch\n",
    "    # gen_idx: a the index (along dimension 1 of history) of the instrument we want to generate music for\n",
    "    # RETURN: an LxBxD tensor, representing the distribution for\n",
    "    # the next MIDI message for the instrument indicated by gen_idx at each time step. \n",
    "    # Note, to get the actual probabilities you'll have to take the softmax\n",
    "    # of this tensor along dimension 1\n",
    "    def forward(self, history, mask, instruments, gen_idx):\n",
    "        L = history.shape[0] # longest length\n",
    "        N = history.shape[1] # max instruments\n",
    "        B = history.shape[2] # batch size\n",
    "        assert(instruments.shape == (1, N, B))\n",
    "        assert(mask.shape == history.shape)\n",
    "        \n",
    "        inputs = self.embedding(history) + torch.tanh(self.i_embedding(instruments)).expand(L, -1, -1, -1)\n",
    "        \n",
    "        inputs = self.position_encoding(inputs)\n",
    "        \n",
    "        encoder_mask = None\n",
    "        memory_mask = None\n",
    "        tgt_mask = torch.triu(torch.ones((L, L), dtype=torch.bool))\n",
    "        tgt_mask.fill_diagonal_(False)\n",
    "        \n",
    "        # If only one instrument, only run the decoder\n",
    "        if N == 1:\n",
    "            encoding = torch.zeros((1, B, self.embed_dim))\n",
    "        else:\n",
    "            encode_idx = [i for i in range(N) if i != gen_idx]\n",
    "            encoder_inputs = inputs[:, encode_idx].view(-1, B, self.embed_dim)\n",
    "            encoder_mask = mask[:, encode_idx].view(-1, B).transpose(0, 1)\n",
    "            #encoding = self.encoder(encoder_inputs, src_key_padding_mask=encoder_mask) # TODO: why doesn't this work?\n",
    "            encoding = encoder_inputs\n",
    "            \n",
    "            memory_mask = tgt_mask.repeat(1, N - 1)\n",
    "        \n",
    "        decoder_inputs = inputs[:, gen_idx]\n",
    "        decoder_mask = mask[:, gen_idx].transpose(0, 1)\n",
    "\n",
    "        decoding = self.decoder(decoder_inputs, encoding, tgt_mask=tgt_mask, memory_mask=memory_mask, \\\n",
    "                                tgt_key_padding_mask=decoder_mask, memory_key_padding_mask=encoder_mask)\n",
    "        \n",
    "        return self.logits(decoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests for baseline transformer\n",
    "We train with model.eval() to disable dropout, since these tests try to get the model to overfit to a small sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model to overfit to a single instrument's part in a single song (only the first 100 time steps). Tests decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_size = 512\n",
    "heads = 8\n",
    "attention_layers=  6\n",
    "embed_dim = 256\n",
    "\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleTransformer(message_dim, embed_dim, num_instruments, heads, attention_layers, ff_size)\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "    \n",
    "model.eval() # Training with eval just to see if we can overfit without dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('overfit_single_instrument.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = np.load('preprocessed_data/recording318.npy', allow_pickle=True)\n",
    "instruments_np = np.load('preprocessed_data/instruments318.npy', allow_pickle=True)\n",
    "\n",
    "inst = 1\n",
    "\n",
    "max_seq_length = 100\n",
    "\n",
    "history = torch.tensor(recording[inst][:max_seq_length], dtype=torch.long).view(-1, 1, 1)\n",
    "mask = torch.zeros(history.shape, dtype=torch.bool)\n",
    "instruments = torch.tensor(instrument_numbers.index(instruments_np[inst]), dtype=torch.long).view(1, 1, 1)\n",
    "\n",
    "max_instruments = history.shape[1]\n",
    "\n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 150\n",
    "train_losses = np.zeros(epochs)\n",
    "\n",
    "target_messages = history[1:].flatten()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    # Index of the instrument we want to generate music for (there's only one instrument)\n",
    "    gen_idx = 0\n",
    "    \n",
    "    # Move forward in time\n",
    "    logits = model(history[:-1], mask[:-1], instruments, gen_idx).view(-1, message_dim)\n",
    "                \n",
    "    loss = loss_fn(logits, target_messages)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses[epoch] = loss.data\n",
    "    print('Loss: %f' %(loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'overfit_single_instrument2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f01ac23b550>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Sample from model\n",
    "gen_history = torch.tensor(recording[inst][0], dtype=torch.long).view(-1, 1, 1)\n",
    "mask = torch.zeros((1, 1, 1), dtype=torch.bool)\n",
    "instruments = torch.tensor(instrument_numbers.index(instruments_np[inst]), dtype=torch.long).view(1, 1, 1)\n",
    "model.eval() # Turns off the dropout for evaluation. Need to do this to get repeatable evaluation outputs\n",
    "\n",
    "seq_length = 100\n",
    "max_instruments = history.shape[1]\n",
    "\n",
    "# Index of the instrument we want to generate music for (there's only one instrument)\n",
    "gen_idx = 0\n",
    "\n",
    "# Move forward in time\n",
    "wrong_cnt = 0\n",
    "for t in range(1, seq_length):\n",
    "    logits = model(gen_history, mask, instruments, gen_idx)\n",
    "    probs = torch.nn.functional.softmax(logits[t - 1], dim=1)\n",
    "    gen_history = torch.cat((gen_history, torch.multinomial(probs, 1).view(1, 1, -1)))\n",
    "    #gen_history = torch.cat((gen_history, torch.argmax(probs.flatten()).view(1, 1, 1)))\n",
    "    if torch.argmax(probs.flatten()) != history[t].flatten():\n",
    "        wrong_cnt += 1\n",
    "        print(torch.topk(probs.flatten(), 10))\n",
    "        print(history[t])\n",
    "    \n",
    "    mask = torch.cat((mask, torch.zeros((1, 1, 1), dtype=torch.bool)))\n",
    "\n",
    "print(wrong_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test_history.npy', np.array([history.flatten().numpy()], dtype='object'))\n",
    "np.save('test_instruments.npy', np.array([instrument_numbers[instruments[0]]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model to overfit to two instruments' parts in a single song. Tests encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_size = 512\n",
    "heads = 8\n",
    "attention_layers=  6\n",
    "embed_dim = 256\n",
    "\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleTransformer(message_dim, embed_dim, num_instruments, heads, attention_layers, ff_size)\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "    \n",
    "model.eval() # Training with eval just to see if we can overfit without dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('overfit_two_instruments.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = np.load('preprocessed_data/recording318.npy', allow_pickle=True)\n",
    "instruments_np = np.load('preprocessed_data/instruments318.npy', allow_pickle=True)\n",
    "\n",
    "max_seq_length = 100\n",
    "max_instruments = instruments_np.shape[0]\n",
    "batch_size = 1\n",
    "\n",
    "history = torch.zeros((max_seq_length, max_instruments, batch_size), dtype=torch.long)\n",
    "mask = torch.ones(history.shape, dtype=torch.bool)\n",
    "instruments = torch.tensor([instrument_numbers.index(instruments_np[i]) for i in range(max_instruments)], dtype=torch.long).view(1, -1, 1)\n",
    "\n",
    "for inst in range(max_instruments):\n",
    "    history[:max_seq_length, inst, 0] = torch.tensor(recording[inst][:max_seq_length], dtype=torch.long)\n",
    "    mask[:max_seq_length, inst, 0] = False\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 300\n",
    "train_losses = np.zeros(epochs)\n",
    "num_targets = max_seq_length - 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    \n",
    "    loss = torch.tensor([0], dtype=torch.float32)\n",
    "    for inst in range(max_instruments):\n",
    "        logits = model(history[:-1], mask[:-1], instruments, inst)\n",
    "\n",
    "        logits = logits.view(-1, message_dim)\n",
    "        target_messages = history[1:, inst].flatten()\n",
    "        output_mask = torch.logical_not(mask[1:, inst].flatten())\n",
    "        loss = loss + loss_fn(logits[output_mask], target_messages[output_mask])\n",
    "                \n",
    "    loss = loss/max_instruments\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    train_losses[epoch] = loss.data\n",
    "    print('Loss = %f' %(loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'overfit_two_instruments.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0198191bd0>]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check if each instrument can reconstruct its part, given the other instrument's part\n",
    "model.eval() # Turns off the dropout for evaluation. Need to do this to get repeatable evaluation outputs\n",
    "\n",
    "for inst in range(max_instruments):\n",
    "    gen_history = history.clone()\n",
    "\n",
    "    # Move forward in time\n",
    "    wrong_cnt = 0\n",
    "    for t in range(1, max_seq_length):\n",
    "        input_mask = mask.clone()\n",
    "        input_mask[t:] = True\n",
    "        logits = model(gen_history, input_mask, instruments, inst)\n",
    "        probs = torch.nn.functional.softmax(logits[t - 1], dim=1)\n",
    "        gen_history[t, inst, 0] = torch.multinomial(probs, 1)\n",
    "        if torch.argmax(probs.flatten()) != history[t, inst].flatten():\n",
    "            wrong_cnt += 1\n",
    "            print(torch.topk(probs.flatten(), 10))\n",
    "            print(history[t, inst])\n",
    "\n",
    "    print(wrong_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check if the instruments can jointly reconstruct the piece\n",
    "gen_history = history.clone()\n",
    "\n",
    "model.eval() # Turns off the dropout for evaluation. Need to do this to get repeatable evaluation outputs\n",
    "\n",
    "wrong_cnt = 0\n",
    "\n",
    "for t in range(1, max_seq_length):\n",
    "    input_mask = mask.clone()\n",
    "    input_mask[t:] = True\n",
    "    for inst in range(max_instruments):\n",
    "        logits = model(gen_history, input_mask, instruments, inst)\n",
    "        probs = torch.nn.functional.softmax(logits[t - 1], dim=1)\n",
    "        gen_history[t, inst, 0] = torch.multinomial(probs, 1)\n",
    "        if torch.argmax(probs.flatten()) != history[t, inst].flatten():\n",
    "            wrong_cnt += 1\n",
    "\n",
    "print(wrong_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class MIDIDataset(torch.utils.data.Dataset):\n",
    "    # CONSTRUCTOR: creates a list of recording files and a list\n",
    "    # of instrument files in root_dir. Assumes that the directory\n",
    "    # contains recording0.npy to recordingM.npy,\n",
    "    # as well as instruments0.npy to instrumentsM.npy\n",
    "    # ARGUMENTS\n",
    "    # root_dir: the directory to search\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        files = os.listdir(root_dir)\n",
    "        self.recordings = []\n",
    "        self.instrument_files = []\n",
    "        for file in files:\n",
    "            if 'recording' in file:\n",
    "                self.recordings.append(os.path.join(root_dir, file))\n",
    "            elif 'instruments' in file:\n",
    "                self.instrument_files.append(os.path.join(root_dir, file))\n",
    "                \n",
    "        assert(len(self.recordings) == len(self.instrument_files))\n",
    "        self.recordings.sort()\n",
    "        self.instrument_files.sort()\n",
    "        self.transform = transform\n",
    "\n",
    "    # __len__\n",
    "    # RETURN: the number of recording files in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.recordings)\n",
    "\n",
    "    # __getitem__\n",
    "    # ARGUMENTS\n",
    "    # idx: indicates which file to get\n",
    "    # RETURN: an instance with keys 'instruments', 'history', and 'times'\n",
    "    # instance['history'] is a numpy array of message sequences for each instrument\n",
    "    # instance['instruments'] a numpy array of instrument numbers\n",
    "    # instance['times'] is a numpy array of message time sequences for each instrument\n",
    "    def __getitem__(self, idx):\n",
    "        instance = {'history': np.load(self.recordings[idx], allow_pickle=True), \\\n",
    "                    'instruments': np.load(self.instrument_files[idx], allow_pickle=True)}\n",
    "        \n",
    "        assert(len(instance['history']) == len(instance['instruments']))\n",
    "        \n",
    "        if self.transform:\n",
    "            instance = self.transform(instance)\n",
    "            \n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn: takes a list of samples from the dataset and turns them into a batch.\n",
    "# ARGUMENTS\n",
    "# batch: a list of dictionaries\n",
    "# RETURN: a sample with keys 'history', 'instruments', 'times', and 'mask'\n",
    "# sample['history']: an LxNxB tensor containing messages\n",
    "# sample['instruments']: a 1xNxB tensor containing instrument numbers\n",
    "# sample['mask']: an LxNxB tensor containing False where a message is\n",
    "# valid, and True where it isn't (accounts for variable length sequences\n",
    "# and zero padding)\n",
    "# sample['times']: an LxNxB tensor containing times of each message\n",
    "def collate_fn(batch):\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    # We size our tensors to accomodate the longest sequence and the largest ensemble\n",
    "    max_instruments = max([len(instance['history']) for instance in batch])\n",
    "    longest_len = max([max([seq.shape[0] for seq in instance['history']]) for instance in batch])\n",
    "\n",
    "    sample = {'history': torch.ones((longest_len, max_instruments, batch_size), dtype=torch.long), \\\n",
    "              'instruments': torch.zeros((1, max_instruments, batch_size), dtype=torch.long), \\\n",
    "              'mask': torch.ones((longest_len, max_instruments, batch_size), dtype=torch.bool)}\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        instrument_idx = [instrument_numbers.index(inst) for inst in batch[b]['instruments']]\n",
    "        sample['instruments'][0, :len(instrument_idx), b] = torch.tensor(instrument_idx, dtype=torch.long)\n",
    "        \n",
    "        for inst_idx in range(len(batch[b]['history'])):\n",
    "            seq_length = len(batch[b]['history'][inst_idx])\n",
    "            sample['history'][:seq_length, inst_idx, b] = torch.tensor(batch[b]['history'][inst_idx], dtype=torch.long)\n",
    "            sample['mask'][:seq_length, inst_idx, b] = False\n",
    "            sample['times'][:seq_length, inst_idx, b] = torch.tensor(batch[b]['times'][inst_idx])\n",
    "            \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_size = 512\n",
    "heads = 8\n",
    "attention_layers=  6\n",
    "embed_dim = 256\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleTransformer(message_dim, embed_dim, len(instrument_numbers), heads, attention_layers, ff_size)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "dataset = MIDIDataset('preprocessed_data')\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "train_losses = np.zeros(epochs)\n",
    "test_losses = np.zeros(epochs) # TODO: train/test split. Can we do this with Dataloader?\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    for b, batch in enumerate(dataloader):\n",
    "        print('Starting iteration %d' %(b))\n",
    "        max_seq_length = batch['history'].shape[0]\n",
    "        num_targets = max_seq_length - 1 # Messages start from t = 0, but we start generating at t = 1\n",
    "        max_instruments = batch['history'].shape[1]\n",
    "        loss = torch.tensor([0])\n",
    "        for inst in range(max_instruments):         \n",
    "            mask = batch['mask']\n",
    "            \n",
    "            logits = model(batch['history'][:-1], mask[:-1], batch['instruments'], inst)\n",
    "            logits = logits.view(-1, message_dim)\n",
    "            target_messages = batch['history'][1:, inst].flatten()\n",
    "            output_mask = torch.logical_not(mask[1:, inst].flatten())\n",
    "            loss = loss + loss_fn(logits[output_mask], target_messages[output_mask])\n",
    "        \n",
    "        loss /= max_instruments\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses[epoch] += loss.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # Disable dropout to make results repeatable\n",
    "\n",
    "time_steps = 5000 # How many time steps do we sample?\n",
    "\n",
    "max_instruments = 3\n",
    "\n",
    "# Piano, violin, viola\n",
    "instruments = torch.tensor([0, 2, 3]).view(1, max_instruments, 1)\n",
    "\n",
    "# Suppose they all start with the same velocity message\n",
    "# TODO: should we have SOS and EOS tokens like in NLP?\n",
    "gen_history = 24*torch.ones((1, max_instruments, 1), dtype=torch.long)\n",
    "mask = torch.zeros((1, 1, 1), dtype=torch.bool)\n",
    "\n",
    "for t in range(1, time_steps):\n",
    "    # Sanity check\n",
    "    if t%100 == 0:\n",
    "        print(t)\n",
    "    next_messages = torch.zeros((1, max_instruments, 1), dtype=torch.long)\n",
    "    for inst in range(max_instruments):\n",
    "        logits = model(gen_history, mask.expand(t, max_instruments, -1), instruments, inst)\n",
    "        probs = torch.nn.functional.softmax(logits[t - 1], dim=1)\n",
    "        next_messages[0, inst, 0] = torch.multinomial(probs, 1)\n",
    "    \n",
    "    gen_history = torch.cat((gen_history, next_messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
