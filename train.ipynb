{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_notes = 128\n",
    "num_time_shifts = 100\n",
    "num_velocities = 32\n",
    "message_dim = 2*num_notes + num_velocities + num_time_shifts\n",
    "instrument_numbers = [0, 6, 40, 41, 42, 43, 45, 60, 68, 70, 71, 73]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Music Transformer version of our network ######\n",
    "# Uses the relative position representation from the Music Transformer\n",
    "\n",
    "\n",
    "# HuangMHA: multi-headed attention using relative position representation\n",
    "# (specifically, the representation introduced by Shaw and optimized by Huang)\n",
    "class HuangMHA(torch.nn.Module):\n",
    "    def __init__(self, heads, embed_dim, heads):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TransformerXL version of our network #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Baseline Transformer version of our network #####\n",
    "# Vanilla transformer, uses absolute position representation\n",
    "\n",
    "# Taken from https://pytorch.org/tutorials/beginner/transformer_tutorial.html.\n",
    "# Only change is the unsqueeze/expand in forward (accounts for multiple instruments)\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :].expand(-1, x.shape[1], -1)\n",
    "        return self.dropout(x)\n",
    "        \n",
    "\n",
    "# EnsembleTransformer: takes a history of MIDI messages \n",
    "# for instruments in an ensemble and generates a distribution for the next message\n",
    "# for a specific instrument\n",
    "class EnsembleTransformer(torch.nn.Module):\n",
    "    # CONSTRUCTOR\n",
    "    # ARGUMENTS\n",
    "    # message_dim: dimension of a MIDI message\n",
    "    # embed_dim: dimension of message embedding\n",
    "    # num_instruments: number of possible instruments\n",
    "    # heads: number of attention heads\n",
    "    # attention_layers: number of attention layers\n",
    "    # ff_size: size of the feedforward output at the end of the decoder\n",
    "    def __init__(self, message_dim, embed_dim, num_instruments, heads, attention_layers, ff_size):\n",
    "        super(EnsembleTransformer, self).__init__()\n",
    "        \n",
    "        # We add the tanhed instrument embedding to each input message \n",
    "        # (this is the global conditioning idea from DeepJ, which comes from WaveNet)\n",
    "        self.i_projection = torch.nn.Embedding(num_instruments, embed_dim)\n",
    "        \n",
    "        self.position_encoding = PositionalEncoding(embed_dim)\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(message_dim, embed_dim)\n",
    "        \n",
    "        # An encoder is used to transform histories of all instruments in the ensemble\n",
    "        # except the instrument we're generating music for\n",
    "        encoder_layer = torch.nn.TransformerEncoderLayer(embed_dim, heads, ff_size)\n",
    "        self.encoder = torch.nn.TransformerEncoder(encoder_layer, attention_layers)\n",
    "        \n",
    "        # A decoder is used to transform the history of the instrument we're generating\n",
    "        # music for, then combine this with the encoder output to generate the next message\n",
    "        decoder_layer = torch.nn.TransformerDecoderLayer(embed_dim, heads, ff_size)\n",
    "        self.decoder = torch.nn.TransformerDecoder(decoder_layer, attention_layers)\n",
    "        \n",
    "        self.logits = torch.nn.Linear(embed_dim, message_dim)\n",
    "    \n",
    "    # forward: generates a probability distribution for the next MIDI message\n",
    "    # for an instrument, given the message history of the ensemble\n",
    "    # ARGUMENTS\n",
    "    # history: an LxN tensor, where L is the length of the longest history in\n",
    "    # the ensemble, N is the number of instruments in the ensemble. As we\n",
    "    # walk along the first dimension, we should see indices of MIDI events for a particular\n",
    "    # instrument\n",
    "    # mask: an LxN tensor, containing False where the sequence index is valid for an instrument,\n",
    "    # and True elsewhere\n",
    "    # gen_idx: index along the second dimension of history indicating which instrument\n",
    "    # we want to generate a message for\n",
    "    # RETURN: a 1xD tensor, representing the distribution for\n",
    "    # the next MIDI message for each sequence. Note, to get the actual probabilities\n",
    "    # you'll have to take the softmax of this tensor along dimension 1\n",
    "    def forward(self, history, mask, instruments, gen_idx):\n",
    "        inputs = self.embedding(history) + torch.tanh(self.i_projection(instruments)).expand(history.shape[0], 1, -1)\n",
    "        \n",
    "        inputs = self.position_encoding(inputs)\n",
    "        \n",
    "        encode_idx = np.array([i for i in range(inputs.shape[1]) if i != gen_idx])\n",
    "        \n",
    "        # 2nd dimension is batch size. We'll probably just use batch size 1\n",
    "        encoder_inputs = inputs[:, encode_idx].view(-1, 1, inputs.shape[2])\n",
    "        encoder_mask = mask[:, encode_idx].flatten().expand(encoder_inputs.shape[0])\n",
    "        encoding = self.encoder(encoder_inputs, encoder_mask)\n",
    "        \n",
    "        decoding = self.decoder(inputs[:, gen_idx].unsqueeze(1), encoding) # Unsqueeze is for batch dimension\n",
    "        \n",
    "        return self.logits(decoding[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Tests for baseline transformer #####\n",
    "recording = np.load('preprocessed_data/recording0.npy', allow_pickle=True)\n",
    "instruments = np.load('preprocessed_data/instruments0.npy', allow_pickle=True)\n",
    "\n",
    "history = torch.tensor(recording[0]).unsqueeze(1).long() # Test for a single instrument\n",
    "instruments = torch.tensor(instrument_numbers.index(instruments[0])).unsqueeze(0).long()\n",
    "mask = torch.zeros((history.shape[0], 1), dtype=torch.bool)\n",
    "\n",
    "et = EnsembleTransformer(message_dim, 256, 12, 4, 6, 2048).double()\n",
    "\n",
    "print(et(history, mask, instruments, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class MIDIDataset(torch.utils.data.Dataset):\n",
    "    # CONSTRUCTOR: creates a list of recording files and a list\n",
    "    # of instrument files in root_dir. Assumes that the directory\n",
    "    # contains recording0.npy to recordingN.npy,\n",
    "    # as well as instruments0.npy to instrumentsN.npy\n",
    "    # ARGUMENTS\n",
    "    # root_dir: the directory to search\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        files = os.listdir(root_dir)\n",
    "        self.recordings = []\n",
    "        self.instruments = []\n",
    "        self.times = []\n",
    "        for file in files:\n",
    "            if 'recording' in file:\n",
    "                self.recordings.append(os.path.join(root_dir, file))\n",
    "            elif 'instruments' in file:\n",
    "                self.instruments.append(os.path.join(root_dir, file))\n",
    "            elif 'times' in file:\n",
    "                self.times.append(os.path.join(root_dir, file))\n",
    "                \n",
    "        assert(len(self.recordings) == len(self.instruments))\n",
    "        assert(len(self.recordings) == len(self.times))\n",
    "        self.recordings.sort()\n",
    "        self.instruments.sort()\n",
    "        self.times.sort()\n",
    "        self.transform = transform\n",
    "\n",
    "    # __len__\n",
    "    # RETURN: the number of recording files in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.recordings)\n",
    "\n",
    "    # __getitem__\n",
    "    # RETURN: a dictionary d with keys 'instruments'\n",
    "    # 'messages', and 'times'. Assuming idx is a list of indices i1, i2, i3...\n",
    "    # d['instruments'][0] is a 1d numpy array containing the instrument\n",
    "    # numbers for recording i1.\n",
    "    # d['messages'][0] is a numpy array of numpy arrays corresponding\n",
    "    # to the same file. Each inner numpy array is LxD, where L\n",
    "    # is the number of MIDI messages associated with the instrument,\n",
    "    # and D is the message dimension.\n",
    "    # d['times'][0] is a numpy array of sequences of message times\n",
    "    # for each instrument\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        message_arrs = []\n",
    "        inst_arrs = []\n",
    "        time_arrs = []\n",
    "        \n",
    "        for i in idx:\n",
    "            message_arrs.append(np.load(self.recordings[i]))\n",
    "            inst_arrs.append(np.load(self.intruments[i]))\n",
    "            time_arrs.append(np.load(self.times[i]))\n",
    "            \n",
    "        sample = {'messages': message_arrs, 'instruments': inst_arrs, 'times': time_arrs}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MIDIDataset('preprocessed_data')\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "# Training - Tian\n",
    "\n",
    "# Instantiate the model\n",
    "# PyTorch boilerplate, see tutorial if needed (optimizer and dataloader intialization, stuff like that)\n",
    "\n",
    "# DataLoader spits out one file\n",
    "# For loop over instruments in file\n",
    "# Generate the next message of this instrument given all previous midi messages from this instrument and ALL OTHER INSTRUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m58",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m58"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
