{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Music Transformer version of our network ######\n",
    "# Uses the relative position representation from the Music Transformer\n",
    "\n",
    "\n",
    "# HuangMHA: multi-headed attention using relative position representation\n",
    "# (specifically, the representation introduced by Shaw and optimized by Huang)\n",
    "class HuangMHA(torch.nn.Module):\n",
    "    def __init__(self, heads, embed_dim, heads):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TransformerXL version of our network #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Baseline Transformer version of our network #####\n",
    "# Vanilla transformer, uses absolute position representation\n",
    "\n",
    "# Taken from https://pytorch.org/tutorials/beginner/transformer_tutorial.html.\n",
    "# Only change is the unsqueeze/expand in forward (accounts for multiple instruments)\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :].expand(-1, x.shape[1], -1)\n",
    "        return self.dropout(x)\n",
    "        \n",
    "\n",
    "# EnsembleTransformer: takes a history of MIDI messages \n",
    "# for instruments in an ensemble and generates a distribution for the next message\n",
    "# for a specific instrument\n",
    "class EnsembleTransformer(torch.nn.Module):\n",
    "    # CONSTRUCTOR\n",
    "    # ARGUMENTS\n",
    "    # message_dim: dimension of a MIDI message\n",
    "    # embed_dim: dimension of message embedding\n",
    "    # num_instruments: number of possible instruments\n",
    "    # heads: number of attention heads\n",
    "    # attention_layers: number of attention layers\n",
    "    # ff_size: size of the feedforward output at the end of the decoder\n",
    "    def __init__(self, message_dim, embed_dim, num_instruments, heads, attention_layers, ff_size):\n",
    "        super(EnsembleTransformer, self).__init__()\n",
    "        \n",
    "        # We project the one-hot instrument identity using a linear layer, then pass it through\n",
    "        # a tanh, finally adding it to each input message (this is the global conditioning idea\n",
    "        # from DeepJ, which comes from WaveNet)\n",
    "        self.i_projection = torch.nn.Linear(num_instruments, embed_dim, False)\n",
    "        \n",
    "        self.position_encoding = PositionalEncoding(embed_dim)\n",
    "        \n",
    "        # TODO: replace this and instrument projection with torch.nn.Embedding\n",
    "        # (would require us to switch from one-hots to indices)\n",
    "        self.embedding = torch.nn.Linear(message_dim, embed_dim, False)\n",
    "        \n",
    "        # An encoder is used to transform histories of all instruments in the ensemble\n",
    "        # except the instrument we're generating music for\n",
    "        encoder_layer = torch.nn.TransformerEncoderLayer(embed_dim, heads, ff_size)\n",
    "        self.encoder = torch.nn.TransformerEncoder(encoder_layer, attention_layers)\n",
    "        \n",
    "        # A decoder is used to transform the history of the instrument we're generating\n",
    "        # music for, then combine this with the encoder output to generate the next message\n",
    "        decoder_layer = torch.nn.TransformerDecoderLayer(embed_dim, heads, ff_size)\n",
    "        self.decoder = torch.nn.TransformerDecoder(decoder_layer, attention_layers)\n",
    "        \n",
    "        self.logits = torch.nn.Linear(embed_dim, message_dim)\n",
    "    \n",
    "    # forward: generates a probability distribution for the next MIDI message\n",
    "    # for an instrument, given the message history of the ensemble\n",
    "    # ARGUMENTS\n",
    "    # history: a LxNxD tensor, where L is the length of the longest history in\n",
    "    # the ensemble, N is the number of instruments in the ensemble, and D\n",
    "    # is the MIDI message dimension. Histories with length less than L should\n",
    "    # be LEFT-ALIGNED and RIGHT-ZERO-PADDED\n",
    "    # instruments: a 1xNxI tensor, where I is the number of possible instruments\n",
    "    # gen_idx: index along the second dimension of history indicating which instrument\n",
    "    # we want to generate a message for\n",
    "    # RETURN: a 1x1xD tensor, representing the log probabilities of\n",
    "    # the next MIDI message for each sequence\n",
    "    def forward(self, history, instruments, gen_idx):\n",
    "        inputs = self.embedding(history) + torch.tanh(self.i_projection(instruments)).expand(history.shape[0], 1, -1)\n",
    "        \n",
    "        # TODO: we'll probably have to chunk the data, and we'll have to keep track of the starting position for\n",
    "        # this chunk\n",
    "        inputs = self.position_encoding(inputs)\n",
    "        \n",
    "        encode_idx = np.array([i for i in range(inputs.shape[1]) if i != gen_idx])\n",
    "        encoding = self.encoder(inputs[:, encode_idx].view(-1, 1, inputs.shape[2])) # 2nd dimension is batch size. We'll probably just use batch size 1\n",
    "        \n",
    "        decoding = self.decoder(inputs[:, gen_idx].unsqueeze(1), encoding) # Unsqueeze is for batch dimension\n",
    "        \n",
    "        return torch.nn.functional.log_softmax(self.logits(decoding[-1].unsqueeze(0)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-6.3004, -6.0479, -6.1753, -5.6479, -6.1512, -5.2086, -7.3799,\n",
      "          -7.9261, -5.7544, -5.7205, -5.8728, -6.7821, -7.0666, -6.8350,\n",
      "          -6.1641, -7.4795, -6.3174, -6.1323, -6.3944, -6.0106, -6.9416,\n",
      "          -5.3092, -5.6186, -5.4328, -5.5351, -5.9735, -7.3433, -6.6063,\n",
      "          -6.2782, -6.8659, -6.2641, -6.7544, -6.8649, -5.3094, -5.3802,\n",
      "          -6.3272, -6.3699, -6.6545, -7.2354, -5.7922, -5.3774, -6.9441,\n",
      "          -6.8730, -4.8403, -5.5289, -6.8899, -6.4514, -6.3966, -5.4889,\n",
      "          -5.6810, -6.6744, -5.0264, -5.4491, -6.9853, -5.9075, -5.9432,\n",
      "          -4.9297, -7.2022, -6.9893, -6.0436, -6.5146, -5.4863, -6.6307,\n",
      "          -5.6968, -5.7590, -5.9590, -6.2410, -6.7688, -6.7982, -6.6794,\n",
      "          -5.9398, -5.7591, -6.3195, -6.3400, -6.2436, -6.5169, -5.7151,\n",
      "          -5.4733, -5.3360, -6.5196, -5.6794, -6.6981, -6.4688, -6.4744,\n",
      "          -6.2493, -6.3226, -6.7693, -7.2626, -4.6927, -6.2605, -7.1629,\n",
      "          -8.0683, -6.5177, -6.2967, -7.2308, -6.9378, -5.9143, -6.4022,\n",
      "          -5.0711, -5.9282, -6.8025, -6.4655, -6.5229, -6.8051, -5.0658,\n",
      "          -6.0660, -5.9613, -5.3287, -6.3684, -6.4500, -5.2267, -5.8655,\n",
      "          -5.4928, -7.1146, -5.6587, -5.3990, -5.6179, -6.1407, -6.7970,\n",
      "          -5.8969, -6.1343, -6.3744, -6.2608, -5.7582, -5.5844, -7.7229,\n",
      "          -5.7539, -6.8917, -7.2709, -5.0116, -6.2588, -5.7526, -6.7284,\n",
      "          -6.0858, -6.3579, -6.8850, -5.9555, -5.6725, -6.1207, -7.2946,\n",
      "          -6.2830, -6.4243, -4.8847, -5.8190, -6.1633, -6.1650, -6.4698,\n",
      "          -5.1551, -6.1115, -5.8680, -6.2773, -6.1759, -5.3498, -7.2250,\n",
      "          -5.9807, -5.2149, -5.8633, -6.6521, -5.5392, -5.4740, -6.0744,\n",
      "          -6.6411, -6.1106, -5.9922, -7.0132, -6.0009, -5.7734, -5.2733,\n",
      "          -5.0900, -6.6208, -5.9563, -6.4144, -6.2098, -5.8450, -6.4280,\n",
      "          -5.5809, -6.8746, -5.9414, -7.0890, -5.6159, -7.1141, -5.7643,\n",
      "          -6.1588, -5.8049, -5.0639, -5.2356, -6.5937, -5.1364, -5.6506,\n",
      "          -5.7005, -6.0748, -7.5277, -6.1426, -6.4712, -6.9215, -6.8534,\n",
      "          -4.2330, -6.8357, -5.0657, -5.8260, -6.4868, -5.9797, -6.1086,\n",
      "          -7.6442, -6.9407, -5.0708, -5.4882, -6.8040, -7.1680, -7.1052,\n",
      "          -5.1879, -5.9770, -6.7604, -6.2358, -6.1356, -5.5879, -6.3836,\n",
      "          -5.8814, -6.2171, -5.7058, -5.9865, -5.8393, -5.7760, -6.3488,\n",
      "          -5.3890, -5.9301, -5.9385, -6.0656, -6.3422, -5.1547, -5.4616,\n",
      "          -6.4832, -7.2738, -6.3506, -5.8950, -6.1374, -6.1747, -6.7503,\n",
      "          -5.6486, -5.6054, -5.4058, -6.0607, -6.0865, -5.6847, -5.6144,\n",
      "          -6.6109, -5.6268, -6.0432, -6.6350, -5.9161, -6.3646, -6.2843,\n",
      "          -5.4131, -6.1633, -5.8706, -6.5486, -7.4680, -4.1991, -6.8170,\n",
      "          -6.5734, -5.8848, -5.5010, -6.5180, -5.9962, -6.4615, -5.3614,\n",
      "          -5.4048, -5.9521, -5.9439, -6.2008, -6.2063, -6.6165, -5.0951,\n",
      "          -6.8703, -6.4303, -7.1913, -5.9488, -7.0181, -5.5879, -5.6543,\n",
      "          -6.6044, -6.6499, -6.0857, -6.2351, -7.0441, -5.6393, -5.8657,\n",
      "          -5.6534, -5.8110, -5.2769, -6.7433, -6.4042, -5.8566, -6.3840,\n",
      "          -6.9750, -5.9337, -6.1247, -5.5623, -6.3635, -6.1873, -7.0103,\n",
      "          -6.6691, -6.5495, -5.7672, -7.1636, -6.2818, -4.8948, -6.0031,\n",
      "          -6.1686, -6.1944, -5.8280, -5.5916, -5.6453, -5.9274, -5.8735,\n",
      "          -7.3981, -5.3657, -6.1185, -5.5986, -6.2023, -6.1663, -6.6057,\n",
      "          -6.1505, -5.9400, -5.3144, -6.1467, -6.6101, -5.7142, -6.1650,\n",
      "          -6.6567, -6.4578, -5.3109, -6.3640, -6.2028, -6.2845, -5.6788,\n",
      "          -5.7646, -6.5090, -5.6035, -7.2253, -6.2467, -6.3913, -5.9098,\n",
      "          -6.4656, -5.4169, -5.8837, -5.6624, -5.3051, -5.5485, -6.5732,\n",
      "          -5.9296, -6.4025, -6.7200, -5.8055, -5.3632, -5.0148, -6.4788,\n",
      "          -6.1229, -5.5856, -7.3295, -6.2948, -5.0898, -5.8850, -6.4372,\n",
      "          -6.0004, -6.3336, -5.8346, -6.1212, -6.7846, -6.9865, -6.3092,\n",
      "          -5.9599, -6.4980, -6.1582, -6.7263, -6.0042, -6.3702, -6.0998,\n",
      "          -6.1648, -6.9541, -6.0318, -5.9473, -6.0333, -5.0798, -6.0566,\n",
      "          -6.6655, -6.0189, -7.2414]]], dtype=torch.float64,\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "##### Tests for baseline transformer #####\n",
    "recording = np.load('preprocessed_data/recording0.npy', allow_pickle=True)\n",
    "\n",
    "history = torch.tensor(recording[0]).unsqueeze(1) # Test for a single instrument\n",
    "instruments = torch.zeros((1, 1, 12), dtype=torch.double)\n",
    "instruments[0, 0, 0] = 1 # Pretend it's piano\n",
    "\n",
    "et = EnsembleTransformer(history.shape[2], 256, 12, 4, 6, 2048).double()\n",
    "\n",
    "print(et(history, instruments, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom DataLoader - Tushar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training - Tian\n",
    "\n",
    "# Instantiate the model\n",
    "# PyTorch boilerplate, see tutorial if needed (optimizer and dataloader intialization, stuff like that)\n",
    "\n",
    "# DataLoader spits out one file\n",
    "# For loop over instruments in file\n",
    "# Generate the next message of this instrument given all previous midi messages from this instrument and ALL OTHER INSTRUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from model"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m58",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m58"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
