{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_notes = 128\n",
    "num_time_shifts = 100\n",
    "num_velocities = 32\n",
    "message_dim = 2*num_notes + num_velocities + num_time_shifts\n",
    "instrument_numbers = [0, 6, 40, 41, 42, 43, 45, 60, 68, 70, 71, 73]\n",
    "num_instruments = len(instrument_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "duplicate argument 'heads' in function definition (<ipython-input-119-da3088c90439>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-119-da3088c90439>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m duplicate argument 'heads' in function definition\n"
     ]
    }
   ],
   "source": [
    "###### Music Transformer version of our network ######\n",
    "# Uses the relative position representation from the Music Transformer\n",
    "\n",
    "\n",
    "# HuangMHA: multi-headed attention using relative position representation\n",
    "# (specifically, the representation introduced by Shaw and optimized by Huang)\n",
    "class HuangMHA(torch.nn.Module):\n",
    "    def __init__(self, heads, embed_dim, heads):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TransformerXL version of our network #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Baseline Transformer version of our network #####\n",
    "# Vanilla transformer, uses absolute position representation\n",
    "\n",
    "# Taken from https://pytorch.org/tutorials/beginner/transformer_tutorial.html.\n",
    "# Only change is the view/expand in forward (accounts for multiple instruments and batches)\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.shape[0], :].view(x.shape[0], 1, 1, -1).expand(-1, x.shape[1], x.shape[2], -1)\n",
    "        return self.dropout(x)\n",
    "        \n",
    "\n",
    "# EnsembleTransformer: takes a history of MIDI messages \n",
    "# for instruments in an ensemble and generates a distribution for the next message\n",
    "# for a specific instrument\n",
    "class EnsembleTransformer(torch.nn.Module):\n",
    "    # CONSTRUCTOR\n",
    "    # ARGUMENTS\n",
    "    # message_dim: dimension of a MIDI message\n",
    "    # embed_dim: dimension of message embedding\n",
    "    # num_instruments: number of instrument labels\n",
    "    # heads: number of attention heads\n",
    "    # attention_layers: number of attention layers\n",
    "    # ff_size: size of the feedforward output at the end of the decoder\n",
    "    def __init__(self, message_dim, embed_dim, num_instruments, heads, attention_layers, ff_size):\n",
    "        super(EnsembleTransformer, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # We add the tanhed instrument embedding to each input message \n",
    "        # (this is the global conditioning idea from DeepJ, which comes from WaveNet)\n",
    "        self.i_embedding = torch.nn.Embedding(num_instruments, embed_dim)\n",
    "        \n",
    "        self.position_encoding = PositionalEncoding(embed_dim)\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(message_dim, embed_dim)\n",
    "        \n",
    "        # An encoder is used to transform histories of all instruments in the ensemble\n",
    "        # except the instrument we're generating music for\n",
    "        encoder_layer = torch.nn.TransformerEncoderLayer(embed_dim, heads, ff_size)\n",
    "        self.encoder = torch.nn.TransformerEncoder(encoder_layer, attention_layers)\n",
    "        \n",
    "        # A decoder is used to transform the history of the instrument we're generating\n",
    "        # music for, then combine this with the encoder output to generate the next message\n",
    "        decoder_layer = torch.nn.TransformerDecoderLayer(embed_dim, heads, ff_size)\n",
    "        self.decoder = torch.nn.TransformerDecoder(decoder_layer, attention_layers)\n",
    "        \n",
    "        self.logits = torch.nn.Linear(embed_dim, message_dim)\n",
    "    \n",
    "    # forward: generates a probability distribution for the next MIDI message\n",
    "    # for an instrument, given the message history of the ensemble\n",
    "    # ARGUMENTS\n",
    "    # history: an LxNxB tensor, where L is the length of the longest history in\n",
    "    # the batch, N is the max number of instruments in the batch, and B is the batch size. As we\n",
    "    # walk along the first dimension, we should see indices of MIDI events for a particular\n",
    "    # instrument\n",
    "    # mask: an LxNxB tensor, containing True where a message or instrument doesn't exist\n",
    "    # instruments: a 1xNxB tensor indicating the instrument numbers for each batch\n",
    "    # gen_idx: a length B tensor indicating the index of the instrument we want to generate music\n",
    "    # for, for each batch\n",
    "    # RETURN: a BxD tensor, representing the distribution for\n",
    "    # the next MIDI message for the instrument indicated by gen_idx. \n",
    "    # Note, to get the actual probabilities you'll have to take the softmax\n",
    "    # of this tensor along dimension 1\n",
    "    def forward(self, history, mask, instruments, gen_idx):\n",
    "        longest_length = history.shape[0]\n",
    "        batch_size = history.shape[2]\n",
    "        max_instruments = instruments.shape[1]\n",
    "        assert(history.shape[1] == max_instruments)\n",
    "        assert(mask.shape[1] == max_instruments)\n",
    "        assert(instruments.shape[2] == batch_size)\n",
    "        assert(mask.shape[2] == batch_size)\n",
    "        assert(mask.shape[0] == longest_length)\n",
    "        \n",
    "        batch_size = history.shape[2]\n",
    "        \n",
    "        inputs = self.embedding(history) + torch.tanh(self.i_embedding(instruments)).expand(history.shape[0], -1, -1, -1)\n",
    "        \n",
    "        inputs = self.position_encoding(inputs)\n",
    "        \n",
    "        encode_idx = torch.tensor([[i for i in range(max_instruments) if i != gen_idx[b]] for b in range(gen_idx.shape[0])])\n",
    "        encode_idx = encode_idx.transpose(0, 1).unsqueeze(0)\n",
    "        encoder_inputs = torch.gather(inputs, 1, encode_idx.unsqueeze(3).expand(inputs.shape[0], -1, -1, self.embed_dim)).view(-1, batch_size, self.embed_dim)\n",
    "\n",
    "        encoder_mask = torch.gather(mask, 1, encode_idx.expand(inputs.shape[0], -1, -1)).view(-1, batch_size).transpose(0, 1)\n",
    "        encoding = self.encoder(encoder_inputs, src_key_padding_mask=encoder_mask)\n",
    "        \n",
    "        decode_idx = gen_idx.view(1, 1, -1).expand(inputs.shape[0], -1, -1)\n",
    "        decoder_inputs = torch.gather(inputs, 1, decode_idx)\n",
    "        decoder_mask = torch.gather(mask, 1, decode_idx).transpose(0, 1)\n",
    "        decoding = self.decoder(decoder_inputs, encoding, tgt_key_padding_mask=decoder_mask)\n",
    "        \n",
    "        return self.logits(decoding[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Tests for baseline transformer #####\n",
    "recording = np.load('preprocessed_data/recording0.npy', allow_pickle=True)\n",
    "instruments_np = np.load('preprocessed_data/instruments0.npy', allow_pickle=True)\n",
    "\n",
    "instrument_idx = [instrument_numbers.index(i) for i in instruments_np]\n",
    "\n",
    "# Form a single-element batch from this recording\n",
    "seq_lengths = [messages.shape[0] for messages in recording]\n",
    "longest_len = max(seq_lengths)\n",
    "batch = torch.ones((longest_len, recording.shape[0], 1), dtype=torch.long)\n",
    "mask = torch.zeros((longest_len, recording.shape[0], 1), dtype=torch.bool)\n",
    "instruments = torch.zeros((1, recording.shape[0], 1), dtype=torch.long)\n",
    "\n",
    "for i, messages in enumerate(recording):\n",
    "    batch[:seq_lengths[i], i, 0] = torch.tensor(messages, dtype=torch.long)\n",
    "    mask[:seq_lengths[i], i, 0] = 1\n",
    "    \n",
    "instruments[0, :len(instrument_idx), 0] = torch.tensor(instrument_idx, dtype=torch.long)\n",
    "\n",
    "et = EnsembleTransformer(message_dim, 256, num_instruments, 4, 6, 2048).double()\n",
    "\n",
    "print(et(batch, mask, instruments, torch.tensor([0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class MIDIDataset(torch.utils.data.Dataset):\n",
    "    # CONSTRUCTOR: creates a list of recording files and a list\n",
    "    # of instrument files in root_dir. Assumes that the directory\n",
    "    # contains recording0.npy to recordingM.npy,\n",
    "    # as well as instruments0.npy to instrumentsM.npy\n",
    "    # ARGUMENTS\n",
    "    # root_dir: the directory to search\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        files = os.listdir(root_dir)\n",
    "        self.recordings = []\n",
    "        self.instrument_files = []\n",
    "        self.time_files = []\n",
    "        for file in files:\n",
    "            if 'recording' in file:\n",
    "                self.recordings.append(os.path.join(root_dir, file))\n",
    "            elif 'instruments' in file:\n",
    "                self.instrument_files.append(os.path.join(root_dir, file))\n",
    "            elif 'times' in file:\n",
    "                self.time_files.append(os.path.join(root_dir, file))\n",
    "                \n",
    "        assert(len(self.recordings) == len(self.instrument_files))\n",
    "        assert(len(self.recordings) == len(self.time_files))\n",
    "        self.recordings.sort()\n",
    "        self.instrument_files.sort()\n",
    "        self.time_files.sort()\n",
    "        self.transform = transform\n",
    "\n",
    "    # __len__\n",
    "    # RETURN: the number of recording files in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.recordings)\n",
    "\n",
    "    # __getitem__\n",
    "    # ARGUMENTS\n",
    "    # idx: indicates which file to get\n",
    "    # RETURN: an instance with keys 'instruments', 'history', and 'times'\n",
    "    # instance['history'] is a numpy array of message sequences for each instrument\n",
    "    # instance['instruments'] a numpy array of instrument numbers\n",
    "    # instance['times'] is a numpy array of message time sequences for each instrument\n",
    "    def __getitem__(self, idx):\n",
    "        instance = {'history': np.load(self.recordings[idx], allow_pickle=True), \\\n",
    "                    'instruments': np.load(self.instrument_files[idx], allow_pickle=True), \\\n",
    "                    'times': np.load(self.time_files[idx], allow_pickle=True)}\n",
    "        \n",
    "        assert(len(instance['history']) == len(instance['times']))\n",
    "        assert(len(instance['history']) == len(instance['instruments']))\n",
    "        \n",
    "        for i in range(len(instance['history'])):\n",
    "            assert(len(instance['history'][i]) == len(instance['times'][i]))\n",
    "        \n",
    "        if self.transform:\n",
    "            instance = self.transform(instance)\n",
    "            \n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn: takes a list of samples from the dataset and turns them into a batch\n",
    "# ARGUMENTS\n",
    "# batch: a list of dictionaries\n",
    "# RETURN: a sample with keys 'history', 'instruments', 'times', and 'mask'\n",
    "# sample['history']: an LxNxB tensor containing messages\n",
    "# sample['instruments']: a 1xNxB tensor containing instrument numbers\n",
    "# sample['mask']: an LxNxB tensor containing False where a message is\n",
    "# valid, and True where it isn't (accounts for variable length sequences\n",
    "# and zero padding)\n",
    "# sample['times']: an LxNxB tensor containing times of each message\n",
    "def collate_fn(batch):\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    # We size our tensors to accomodate the longest sequence and the largest ensemble\n",
    "    max_instruments = max([len(instance['history']) for instance in batch])\n",
    "    longest_len = max([max([seq.shape[0] for seq in instance['history']]) for instance in batch])\n",
    "\n",
    "    sample = {'history': torch.ones((longest_len, max_instruments, batch_size), dtype=torch.long), \\\n",
    "              'instruments': torch.zeros((1, max_instruments, batch_size), dtype=torch.long), \\\n",
    "              'times': torch.zeros((longest_len, max_instruments, batch_size), dtype=torch.double), \\\n",
    "              'mask': torch.ones((longest_len, max_instruments, batch_size), dtype=torch.bool)}\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        instrument_idx = [instrument_numbers.index(inst) for inst in batch[b]['instruments']]\n",
    "        sample['instruments'][0, :len(instrument_idx), b] = torch.tensor(instrument_idx, dtype=torch.long)\n",
    "        \n",
    "        for inst_idx in range(len(batch[b]['history'])):\n",
    "            seq_length = len(batch[b]['history'][inst_idx])\n",
    "            assert(seq_length == len(batch[b]['times'][inst_idx]))\n",
    "            sample['history'][:seq_length, inst_idx, b] = torch.tensor(batch[b]['history'][inst_idx], dtype=torch.long)\n",
    "            sample['mask'][:seq_length, inst_idx, b] = False\n",
    "            sample['times'][:seq_length, inst_idx, b] = torch.tensor(batch[b]['times'][inst_idx], dtype=torch.double)\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "ff_size = 512\n",
    "heads = 8\n",
    "attention_layers=  6\n",
    "embed_dim = 256\n",
    "\n",
    "dataset = MIDIDataset('preprocessed_data')\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Training\n",
    "model = EnsembleTransformer(message_dim, embed_dim, len(instrument_numbers), heads, attention_layers, ff_size)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "train_losses = np.zeros(epochs)\n",
    "test_losses = np.zeros(epochs) # TODO: train/test split. Can we do this with Dataloader?\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    for batch in dataloader:\n",
    "        max_seq_length = batch['history'].shape[0]\n",
    "        max_instruments = batch['history'].shape[1]\n",
    "        loss = 0\n",
    "        for inst in range(max_instruments):\n",
    "            # Index of the instrument we want to generate music for (same for each batch element as of now)\n",
    "            gen_idx = torch.tensor([inst for i in range(batch_size)], dtype=torch.long)\n",
    "            \n",
    "            target_messages = torch.gather(batch['history'], 1, gen_idx.view(1, 1, -1).expand(max_seq_length, -1, -1))\n",
    "            \n",
    "            mask = batch['mask']\n",
    "\n",
    "            # Move forward in time\n",
    "            for t in range(1, max_seq_length):\n",
    "                # Get current time for each batch element (1x1xB) and mask out messages with a greater time stamp\n",
    "                times = torch.gather(batch['times'][t], 0, gen_idx.view(1, -1)).unsqueeze(0)\n",
    "                input_mask = torch.logical_and(mask, batch['times'][t] < times.expand(max_seq_length, max_instruments, -1))\n",
    "                output_mask = torch.gather(mask[t], 0, gen_idx.view(1, -1))\n",
    "                loss += output_mask*loss_fn(model(batch['history'], input_mask, batch['instruments'], gen_idx), target_messages[t])\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses[epoch] += loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
