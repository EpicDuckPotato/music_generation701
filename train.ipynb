{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_notes = 128\n",
    "num_time_shifts = 100\n",
    "num_velocities = 32\n",
    "message_dim = 2*num_notes + num_velocities + num_time_shifts\n",
    "instrument_numbers = [0, 6, 40, 41, 42, 43, 45, 60, 68, 70, 71, 73]\n",
    "num_instruments = len(instrument_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Music Transformer version of our network ######\n",
    "# Uses the relative position representation from the Music Transformer\n",
    "\n",
    "\n",
    "# HuangMHA: multi-headed attention using relative position representation\n",
    "# (specifically, the representation introduced by Shaw and optimized by Huang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TransformerXL version of our network #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Transformer definition\n",
    "Uses absolute position representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://pytorch.org/tutorials/beginner/transformer_tutorial.html.\n",
    "# Only change is the view/expand in forward (accounts for multiple instruments and batches)\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.shape[0], :].view(x.shape[0], 1, 1, -1).expand(-1, x.shape[1], x.shape[2], -1)\n",
    "        return self.dropout(x)\n",
    "        \n",
    "\n",
    "# EnsembleTransformer: takes a history of MIDI messages \n",
    "# for instruments in an ensemble and generates a distribution for the next message\n",
    "# for a specific instrument\n",
    "class EnsembleTransformer(torch.nn.Module):\n",
    "    # CONSTRUCTOR\n",
    "    # ARGUMENTS\n",
    "    # message_dim: dimension of a MIDI message\n",
    "    # embed_dim: dimension of message embedding\n",
    "    # num_instruments: number of instrument labels\n",
    "    # heads: number of attention heads\n",
    "    # attention_layers: number of attention layers\n",
    "    # ff_size: size of the feedforward output at the end of the decoder\n",
    "    def __init__(self, message_dim, embed_dim, num_instruments, heads, attention_layers, ff_size):\n",
    "        super(EnsembleTransformer, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # We add the tanhed instrument embedding to each input message \n",
    "        # (this is the global conditioning idea from DeepJ, which comes from WaveNet)\n",
    "        self.i_embedding = torch.nn.Embedding(num_instruments, embed_dim)\n",
    "        \n",
    "        self.position_encoding = PositionalEncoding(embed_dim)\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(message_dim, embed_dim)\n",
    "        \n",
    "        # An encoder is used to transform histories of all instruments in the ensemble\n",
    "        # except the instrument we're generating music for\n",
    "        encoder_layer = torch.nn.TransformerEncoderLayer(embed_dim, heads, ff_size)\n",
    "        self.encoder = torch.nn.TransformerEncoder(encoder_layer, attention_layers)\n",
    "        \n",
    "        # A decoder is used to transform the history of the instrument we're generating\n",
    "        # music for, then combine this with the encoder output to generate the next message\n",
    "        decoder_layer = torch.nn.TransformerDecoderLayer(embed_dim, heads, ff_size)\n",
    "        self.decoder = torch.nn.TransformerDecoder(decoder_layer, attention_layers)\n",
    "        \n",
    "        self.logits = torch.nn.Linear(embed_dim, message_dim)\n",
    "    \n",
    "    # forward: generates a probability distribution for the next MIDI message\n",
    "    # for an instrument, given the message history of the ensemble\n",
    "    # ARGUMENTS\n",
    "    # history: an LxNxB tensor, where L is the length of the longest history in\n",
    "    # the batch, N is the max number of instruments in the batch, and B is the batch size. As we\n",
    "    # walk along the first dimension, we should see indices of MIDI events for a particular\n",
    "    # instrument\n",
    "    # mask: an LxNxB tensor, containing True where a message or instrument doesn't exist\n",
    "    # instruments: a 1xNxB tensor indicating the instrument numbers for each batch\n",
    "    # gen_idx: a length B tensor indicating the index of the instrument we want to generate music\n",
    "    # for, for each batch\n",
    "    # t: which index of the decoder output do we want (along dimension 0)?\n",
    "    # RETURN: a BxD tensor, representing the distribution for\n",
    "    # the next MIDI message for the instrument indicated by gen_idx. \n",
    "    # Note, to get the actual probabilities you'll have to take the softmax\n",
    "    # of this tensor along dimension 1\n",
    "    def forward(self, history, mask, instruments, gen_idx, t):\n",
    "        L = history.shape[0] # longest length\n",
    "        N = history.shape[1] # max instruments\n",
    "        B = history.shape[2] # batch size\n",
    "        assert(instruments.shape == (1, N, B))\n",
    "        assert(mask.shape == history.shape)\n",
    "        assert(gen_idx.shape == (B,))\n",
    "        \n",
    "        inputs = self.embedding(history) + torch.tanh(self.i_embedding(instruments)).expand(L, -1, -1, -1)\n",
    "        \n",
    "        inputs = self.position_encoding(inputs)\n",
    "        \n",
    "        encoder_mask = None\n",
    "        \n",
    "        # If only one instrument, only run the decoder\n",
    "        if N == 1:\n",
    "            encoding = torch.zeros((1, B, self.embed_dim))\n",
    "        else:\n",
    "            encode_idx = torch.tensor([[i for i in range(N) if i != gen_idx[b]] for b in range(B)])\n",
    "            encode_idx = encode_idx.transpose(0, 1).unsqueeze(0).expand(L, -1, -1)\n",
    "            encoder_inputs = torch.gather(inputs, 1, encode_idx.unsqueeze(3).expand(-1, -1, -1, self.embed_dim)).view(-1, B, self.embed_dim)\n",
    "\n",
    "            encoder_mask = torch.gather(mask, 1, encode_idx).view(-1, B).transpose(0, 1)\n",
    "            encoding = self.encoder(encoder_inputs, src_key_padding_mask=encoder_mask)\n",
    "        \n",
    "        decode_idx = gen_idx.view(1, 1, -1).expand(L, -1, -1)\n",
    "        decoder_inputs = torch.gather(inputs, 1, decode_idx.unsqueeze(3).expand(-1, -1, -1, self.embed_dim)).squeeze(1)\n",
    "        decoder_mask = torch.gather(mask, 1, decode_idx).squeeze(1).transpose(0, 1)\n",
    "        \n",
    "        # TODO: do I need the memory_key_padding_mask? I think so, because some of the encoder outputs are from padding values\n",
    "        decoding = self.decoder(decoder_inputs, encoding, tgt_key_padding_mask=decoder_mask, memory_key_padding_mask=encoder_mask)\n",
    "        \n",
    "        return self.logits(decoding[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests for baseline transformer\n",
    "We train with model.eval() to disable dropout, since these tests try to get the model to overfit to a small sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model to overfit to a single instrument's part in a single song (only the first 100 time steps). Tests decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_size = 512\n",
    "heads = 8\n",
    "attention_layers=  6\n",
    "embed_dim = 256\n",
    "\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleTransformer(message_dim, embed_dim, num_instruments, heads, attention_layers, ff_size)\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "    \n",
    "model.eval() # Training with eval just to see if we can overfit without dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('overfit_single_instrument.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = np.load('preprocessed_data/recording318.npy', allow_pickle=True)\n",
    "instruments_np = np.load('preprocessed_data/instruments318.npy', allow_pickle=True)\n",
    "times_np = np.load('preprocessed_data/times318.npy', allow_pickle=True)\n",
    "\n",
    "inst = 1\n",
    "\n",
    "max_seq_length = 100\n",
    "\n",
    "history = torch.tensor(recording[inst][:max_seq_length], dtype=torch.long).view(-1, 1, 1)\n",
    "mask = torch.zeros(history.shape, dtype=torch.bool)\n",
    "instruments = torch.tensor(instrument_numbers.index(instruments_np[inst]), dtype=torch.long).view(1, 1, 1)\n",
    "times = torch.tensor(times_np[inst][:max_seq_length]).view(-1, 1, 1)\n",
    "\n",
    "max_instruments = history.shape[1]\n",
    "\n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 450\n",
    "train_losses = np.zeros(epochs)\n",
    "\n",
    "target_messages = history[1:].flatten()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    # Index of the instrument we want to generate music for (there's only one instrument)\n",
    "    gen_idx = torch.tensor([0], dtype=torch.long)\n",
    "    \n",
    "    # Move forward in time\n",
    "    logits = torch.zeros((max_seq_length - 1, batch_size, message_dim))\n",
    "    for t in range(1, max_seq_length):\n",
    "        input_mask = mask.clone()\n",
    "        input_mask[t:] = True\n",
    "        input_mask = torch.logical_or(input_mask, times > times[t].unsqueeze(0).expand(max_seq_length, max_instruments, -1))\n",
    "        logits[t - 1] = model(history, input_mask, instruments, gen_idx)\n",
    "                \n",
    "    loss = loss_fn(logits.view(-1, message_dim), target_messages)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses[epoch] = loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'overfit_single_instrument.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from model\n",
    "gen_history = torch.tensor(recording[inst][0], dtype=torch.long).view(-1, 1, 1)\n",
    "mask = torch.zeros((1, 1, 1), dtype=torch.bool)\n",
    "instruments = torch.tensor(instrument_numbers.index(instruments_np[inst]), dtype=torch.long).view(1, 1, 1)\n",
    "times = torch.tensor([0]).view(-1, 1, 1)\n",
    "model.eval() # Turns off the dropout for evaluation. Need to do this to get repeatable evaluation outputs\n",
    "\n",
    "seq_length = 100\n",
    "max_instruments = history.shape[1]\n",
    "\n",
    "# Index of the instrument we want to generate music for (there's only one instrument)\n",
    "gen_idx = torch.tensor([0], dtype=torch.long)\n",
    "\n",
    "# Move forward in time\n",
    "wrong_cnt = 0\n",
    "for t in range(1, seq_length):\n",
    "    probs = torch.nn.functional.softmax(model(gen_history, mask, instruments, gen_idx), dim=1)\n",
    "    gen_history = torch.cat((gen_history, torch.multinomial(probs, 1).view(1, 1, -1)))\n",
    "    #probs = torch.nn.functional.softmax(model(history[:t], mask, instruments, gen_idx), dim=1)\n",
    "    #gen_history = torch.cat((gen_history, torch.argmax(probs.flatten()).view(1, 1, 1)))\n",
    "    if torch.argmax(probs.flatten()) != history[t].flatten():\n",
    "        wrong_cnt += 1\n",
    "        print(torch.topk(probs.flatten(), 10))\n",
    "        print(history[t])\n",
    "    \n",
    "    mask = torch.cat((mask, torch.zeros((1, 1, 1), dtype=torch.bool)))\n",
    "\n",
    "print(wrong_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test_history.npy', np.array([history.flatten().numpy()], dtype='object'))\n",
    "np.save('test_instruments.npy', np.array([instrument_numbers[instruments[0]]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model to overfit to two instruments' parts in a single song. Tests encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_size = 512\n",
    "heads = 8\n",
    "attention_layers=  6\n",
    "embed_dim = 256\n",
    "\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleTransformer(message_dim, embed_dim, num_instruments, heads, attention_layers, ff_size)\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "    \n",
    "model.eval() # Training with eval just to see if we can overfit without dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('overfit_two_instruments.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = np.load('preprocessed_data/recording318.npy', allow_pickle=True)\n",
    "instruments_np = np.load('preprocessed_data/instruments318.npy', allow_pickle=True)\n",
    "times_np = np.load('preprocessed_data/times318.npy', allow_pickle=True)\n",
    "\n",
    "max_seq_length = 100\n",
    "max_instruments = instruments_np.shape[0]\n",
    "batch_size = 1\n",
    "\n",
    "history = torch.zeros((max_seq_length, max_instruments, batch_size), dtype=torch.long)\n",
    "mask = torch.ones(history.shape, dtype=torch.bool)\n",
    "instruments = torch.tensor([instrument_numbers.index(instruments_np[i]) for i in range(max_instruments)], dtype=torch.long).view(1, -1, 1)\n",
    "times = torch.zeros(history.shape)\n",
    "\n",
    "for inst in range(max_instruments):\n",
    "    history[:max_seq_length, inst, 0] = torch.tensor(recording[inst][:max_seq_length], dtype=torch.long)\n",
    "    mask[:max_seq_length, inst, 0] = False\n",
    "    times[:max_seq_length, inst, 0] = torch.tensor(times_np[inst][:max_seq_length])\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 450\n",
    "train_losses = np.zeros(epochs)\n",
    "num_targets = max_seq_length - 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    \n",
    "    loss = torch.tensor([0], dtype=torch.float32)\n",
    "    for inst in range(max_instruments):\n",
    "        # Index of the instrument we want to generate music for\n",
    "        gen_idx = torch.tensor([inst for i in range(batch_size)], dtype=torch.long)\n",
    "\n",
    "        logits = torch.zeros((num_targets, batch_size, message_dim))\n",
    "\n",
    "        # Move forward in time\n",
    "        for t in range(1, max_seq_length):\n",
    "            # Get current time for each batch element (1x1xB) and mask out messages with a greater time stamp\n",
    "            cur_times = torch.gather(times[t], 0, gen_idx.view(1, -1)).unsqueeze(0)\n",
    "            input_mask = torch.logical_or(mask, times > cur_times.expand(max_seq_length, max_instruments, -1))\n",
    "            for seq in range(batch_size):\n",
    "                input_mask[t:, gen_idx[seq], seq] = True\n",
    "\n",
    "            logits[t - 1] = model(history, input_mask, instruments, gen_idx, t)\n",
    "\n",
    "        logits = logits.view(-1, message_dim)\n",
    "        target_messages = torch.gather(history[1:], 1, gen_idx.view(1, 1, -1).expand(num_targets, -1, -1)).flatten()\n",
    "        output_mask = torch.logical_not(torch.gather(mask[1:], 1, gen_idx.view(1, 1, -1).expand(num_targets, -1, -1)).flatten())\n",
    "        loss = loss + loss_fn(logits[output_mask], target_messages[output_mask])\n",
    "                \n",
    "    loss = loss/max_instruments\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    train_losses[epoch] = loss.data\n",
    "    print('Loss = %f' %(loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'overfit_two_instruments.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb7bc27b710>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAVuUlEQVR4nO3de2xc5ZnH8d/jGc+M7fElsYdcIU4CSwuUW73hEhYVSgulLezSakW1dNtupeyiXuhlW4G66rb9p9vViqWVKtiI0otKiyill0VtKUuBlFtaBwJNSLgkEMiFxCGJc7Hj67N/zNhxbIPH9pw5rz3fj2T5zJnj8eP3j59eP+c955i7CwAQrqq4CwAAvDWCGgACR1ADQOAIagAIHEENAIFLRvGhLS0t3traGsVHA8CstG7dur3unhvvvUiCurW1Ve3t7VF8NADMSma27c3eo/UBAIEjqAEgcAQ1AASOoAaAwBHUABA4ghoAAkdQA0Dgggrq7zz4oh55oSPuMgAgKEUFtZk1mdk9ZrbZzDaZ2QVRFHPbI1v06IsENQCMVOyVid+W9Dt3/7CZpSTVRlFMdaJKfQM8yAAARpowqM2sQdLFkj4uSe7eK6k3imKqE6begcEoPhoAZqxiWh/LJHVI+r6ZPW1mt5tZ3eiDzGyVmbWbWXtHx9TaF9WJKvUT1ABwnGKCOinpXEm3uvs5ko5IunH0Qe6+2t3b3L0tlxv3BlATovUBAGMVE9TbJW1397WF1/coH9wll6T1AQBjTBjU7v66pNfM7NTCrndLei6KYlK0PgBgjGJXfXxG0p2FFR9bJX0iimJofQDAWEUFtbuvl9QWbSn51kcfM2oAOE5QVybmZ9QENQCMFFRQp2h9AMAYQQV1MmGcTASAUYIK6upElXqZUQPAcYIK6hQ9agAYI6igpvUBAGMFFdSsowaAsQILai4hB4DRAgtqLiEHgNGCC2paHwBwvKCCmkvIAWCsoIKa5XkAMFZQQV2dqNKgSwODtD8AYEhQQZ1MmCQxqwaAEYIK6lQiXw5BDQDHBBXU1cNBTesDAIYEFdRDrQ/WUgPAMUEF9dCMmqsTAeCYoIJ6qEfdT+sDAIYFFdSs+gCAsYIKalofADBWUEGdYtUHAIwRVFCnk/lyevoGYq4EAMKRLOYgM3tF0iFJA5L63b0timJqUglJUlcvQQ0AQ4oK6oJL3H1vZJVIqk3lyyGoAeCYoFoftcMz6v6YKwGAcBQb1C7p92a2zsxWjXeAma0ys3Yza+/o6JhSMUOtj2561AAwrNigXunu50p6n6RPmdnFow9w99Xu3ububblcbkrF1NKjBoAxigpqd99Z+L5H0i8krYiimEySoAaA0SYMajOrM7P6oW1J75W0IZJiqkw11Ql106MGgGHFrPqYJ+kXZjZ0/E/c/XdRFVSbSjCjBoARJgxqd98q6awy1CIpf0Kxm6AGgGFBLc+TmFEDwGjBBXVNKqkulucBwLDggrq2OqGuHk4mAsCQ8IKa1gcAHCe4oK5JJbgyEQBGCC6o61JJHaH1AQDDggvqOXUp7e/qlTsPDwAAKcCgbsmm1Dfg6uzui7sUAAhCcEGdq09LkvYe7om5EgAIQ3BB3ZLNB3XHod6YKwGAMAQb1MyoASAvwKBOSSKoAWBIcEE9pzalRJUR1ABQEFxQV1WZWrIp7T5IUAOAFGBQS9LCphrt6uyOuwwACEKYQd1Yo10HjsZdBgAEIcygbspox4Furk4EAAUb1DXq6R/U/i6uTgSAIIN6QWONJGnnAfrUABBkUC9qygf1DoIaAMIM6gVNGUnSLoIaAMIM6ua6lFLJKu3sZOUHAAQZ1GamRU019KgBQIEGtSQtaMwQ1ACgSQS1mSXM7Gkzuy/Kgobkr06k9QEAk5lR3yBpU1SFjLawMaPdB4+qb2CwXL8SAIJUVFCb2WJJ75d0e7TlHLOwqUaDLu0+yKwaQGUrdkZ9i6QvS3rT6a2ZrTKzdjNr7+jomHZhCwtrqWl/AKh0Ewa1mX1A0h53X/dWx7n7andvc/e2XC437cIWFtZSc0IRQKUrZka9UtJVZvaKpLskXWpmP460Ko28jJwZNYDKNmFQu/tN7r7Y3VslXSvpD+5+XdSF1aWTaqqt1qv7uqL+VQAQtGDXUUvS2Sc26fEte7ndKYCKNqmgdveH3f0DURUz2rvfdoK2vdGlLR1HyvUrASA4Qc+oVyxtliRt3NkZcyUAEJ+gg3po5QdL9ABUsqCDuj5TrfpMktudAqhoQQe1lH/Q7Q6W6AGoYOEHdVNGuzqZUQOoXMEH9YKmGp5IDqCiBR/Ub59frwNdfVz4AqBiBR/UFyxvkSQ9+tLemCsBgHgEH9TLc3Wa35DR4y+9EXcpABCL4IPazHThyc16fMteDQ7SpwZQeYIPaklaubxF+7v6tOn1g3GXAgBlNzOC+uR8n5r2B4BKNCOCen5jRstydXpsCycUAVSeGRHUknTRyS1au3WfDh3ti7sUACirGRPUH37nYnX3Degna1+NuxQAKKsZE9RnLm7SRSe36PZHX9bRvoG4ywGAspkxQS1J179ruToO9ejep3bEXQoAlM2MCuoLlzfrrMWNuvWRl7TnIHfUA1AZZlRQm5lufN/b1XGoR9euflKdXZxYBDD7zaiglqQLljfrR/90nl7b36Xr71ynzu7ShPXRvgFd/d3HtG7b/pJ8HgCUyowLaklasXSuvvWhM7X25X264pY1WvNCx7Rvg7px50E989oBfejWx3Xlt/9YokoBYPpmZFBL0jXnLta911+omlRC/3jHn3T5LWv0P49s0e636F1/58EXdebX7h831HtGrCR5bheXqgMIRzLuAqbjrBOb9JvP/o3uWbddP39qu77528361u82q23JXC0/IauNOzvV3Tugb1x9hi5Y3qybH3hBkrTtjS61ttQd91n76XcDCNSEQW1mGUlrJKULx9/j7v8edWHFylQndN35S3Td+Uu0teOwfrl+p361foee3XFAy3NZdXb36brvrdU3rj59+GfWbds/Jqj3Hu4pd+kAUJRiZtQ9ki5198NmVi3pUTP7rbs/GXFtk7Ysl9UX3vNX+vxlp2hg0JVMVOnQ0T595qdP6yu/2DB83H/9/nlddEqL5jVkJEl7Dh3Vj554JaaqAeCtTdij9rzDhZfVha+gbwxtZkom8n9afaZa//33Zw+/99Hzl+j1g0e1es1WSdLgoOuKW/6oLR1H4igVACZU1MlEM0uY2XpJeyQ94O5rxzlmlZm1m1l7R0dHicucnjl1KT1x06W64+Nt+vpVp+uDZy7U3e2vqW9gUPdvfF37jvSO+Zn/+O1mtb+yTxd+88GSLQEEgKkoKqjdfcDdz5a0WNIKMztjnGNWu3ubu7flcrkSlzl9CxprdOnb5qmqynT56fN16Gi/Nuzo1Pcfe0XLRvWrJem2R7bow7c9oZ2dR/XUq6ytBhCfSS3Pc/cDkh6WdEUUxZTLiqVzJUlrX96nV/d16Z1L5mjNly5RXSox7vHpxIxdxQhgFpgwgcwsZ2ZNhe0aSZdJ2hxxXZHK1ae1tKVOT23brzeO9Kg5m9ZJzbVa0FQz7vFBN+QBzHrFrPpYIOmHZpZQPtjvdvf7oi0restzdfrLjk71DbhasilJetOrG7t6ua0qgPhMGNTu/qykc8pQS1m1Ntfp/zbtkSS1ZNOS8o/8Gm/1Rzf3vwYQo4ptvi4ZcQJxKKi/c+05+ofzThpzbHdvf9nqAoDRKjaoW5trh7db6vOtj+ZsWp9Y2TrmWFofAOJUsUH99gUNw9vNdenh7Uz12JUfBDWAOFVsULdk0/r4ha2qzyQ1ty41vL8ulW/bv2NR4/C+boIaQIwqNqgl6WtXna71X32vElU2vG9OXUr3f+5i3XrducP7OJkIIE4VHdSSjgvpIafOr9eCxhqdOq9eEq0PAPGq+KB+M4kq0/2fv1gnza1l1QeAWBHUE6hNJZhRA4gVQT2BmlSCHjWAWBHUE6hNJbS144h+tX6HvvvQS3GXA6ACzehnJpZDlZl2HOjWDXetlyRdd94SNdZWx1sUgIrCjHoCn7rk5ONeP/zCnpgqAVCpCOoJnL+sWe3/dpk+fcnJqjLpL9s74y4JQIUhqIvQkk3rXy8/VS3ZtA73sFQPQHkR1JOQTScJagBlR1BPQjZDUAMoP4J6EupSSR0hqAGUGUE9CdlMUoeOEtQAyougnoRsOqkj3PcDQJkR1JNQl07oMDNqAGVGUE9CNl2tIz3c9wNAeRHUk5BNJ9Q7MKiefsIaQPkQ1JOQTedvjcKsGkA5EdSTUFcI6oc2c78PAOUzYVCb2Ylm9pCZbTKzjWZ2QzkKC9FQUH/xZ8/EXAmASlLMbU77JX3R3Z8ys3pJ68zsAXd/LuLagjPyyeQAUC4TzqjdfZe7P1XYPiRpk6RFURcWohPn1upLl58qSZxQBFA2k+pRm1mrpHMkrR3nvVVm1m5m7R0dHSUqLzz1mfw/IVyhCKBcig5qM8tK+rmkz7n7wdHvu/tqd29z97ZcLlfKGoPSkMk/3eVgd1/MlQCoFEUFtZlVKx/Sd7r7vdGWFDZm1ADKrZhVHybpe5I2ufvN0ZcUtoaawoz6KDNqAOVRzIx6paSPSrrUzNYXvq6MuK5gDc2oD3YzowZQHhMuz3P3RyVZGWqZEYZ61IeYUQMoE65MnCRaHwDKjaCepLpUQlXGyUQA5UNQT5KZqaGmWge6mFEDKA+CegpasmntPdwTdxkAKgRBPQW5bFodhwhqAOVBUE9Brj6tDmbUAMqEoJ6CXD0zagDlQ1BPQa4+ra7eAR3pYeUHgOgR1FOQy6YliVk1gLIgqKcgV58P6j0ENYAyIKinYGFTRpK0q7M75koAVAKCegoWNNZIknYeOBpzJQAqAUE9BXXppJpqq7XzADNqANEjqKdoYWMNQQ2gLAjqKZrfmNGDm/dow47OuEsBMMsR1FP0tvn1kqTbHtkScyUAZjuCeoo+++5TtKiphrvoAYgcQT1FmeqETlvYwF30AESOoJ4G7vkBoBwI6mloyaa1r6tX/QODcZcCYBYjqKchV5+Wu7TvSG/cpQCYxQjqachlU5LEvakBRIqgngZuzgSgHCYMajO7w8z2mNmGchQ0kxy75wdXKAKITjEz6h9IuiLiOmakeQ0ZJatMO/YT1ACiM2FQu/saSfvKUMuMk6gyzW/MMKMGEKmS9ajNbJWZtZtZe0dHR6k+NniLmmq0g6AGEKGSBbW7r3b3Nndvy+VypfrY4C2aU0PrA0CkWPUxTYuaavT6waPq46IXABEhqKdpUVONBl3afZCnvQCIRjHL834q6QlJp5rZdjP7ZPRlzRyL5uSX6NH+ABCV5EQHuPtHylHITLWoqRDUnFAEEBFaH9O0sIkZNYBoEdTTlKlOqCWbZkYNIDIEdQksaMzodU4mAogIQV0C8xrS2n2QGzMBiAZBXQLzGjIszwMQGYK6BOY1ZLTvSK96+gfiLgXALERQl8C8hsJ9qWl/AIgAQV0CJzRkJEl7DtH+AFB6BHUJDF308srerpgrATAbEdQlsDyXVX0mqfZt3LYbQOkR1CWQqDL9detcrX2ZoAZQegR1ibS1ztHWjiPaf6Q37lIAzDIEdYmctbhJkrRhZ2e8hQCYdQjqEjljYaMk6dntBDWA0iKoS6SxtlqtzbV65rUDcZcCYJYhqEtoxdL8CcXBQY+7FACzCEFdQhcsb1Znd5+e23Uw7lIAzCIEdQmtXN4iM+mB53bHXQqAWYSgLqETGjJaubxF9z69Xe60PwCUBkFdYn93ziK9tq9b7dv2x10KgFmCoC6xK86Yr5rqhH7W/lrcpQCYJQjqEqtLJ3XNuYv0y6d3aifPUQRQAgR1BK5/13KZSV+4e736BwbjLgfADEdQR2DxnFp985p36Mmt+/S1/93IumoA01JUUJvZFWb2vJm9ZGY3Rl3UbHDNuYv1zxcv04+ffFWX3fyIfvDYyzxYAMCU2ETLyMwsIekFSe+RtF3SnyV9xN2fe7OfaWtr8/b29lLWOSO5u+57dpduf/Tl4UvLlzTXallLnZblsprXkFZjTXXhK6WaVEKpRJVSyarh79UJUzJRpUSVKWGmVDK/DWB2MbN17t423nvJIn5+haSX3H1r4cPuknS1pDcNauSZmT541kJ98KyF2rizU2te2KsNOzq1peOwHt/yhnr6J9+/TiWrdNLcWhHVQHjm1KZ0979cUPLPLSaoF0kaudZsu6TzRh9kZqskrZKkk046qSTFzSanL2zU6YU77En52faR3gEd6OpVZ3efOrv6dLR/QL39g+odcPX2D6pvYHD4+6C7Bgbzz2XcfZAWChCihkx1JJ9bTFCPN3kb0y9x99WSVkv51sc065r1zEzZdFLZdFKL58RdDYCQFXMycbukE0e8XixpZzTlAABGKyao/yzpFDNbamYpSddK+nW0ZQEAhkzY+nD3fjP7tKT7JSUk3eHuGyOvDAAgqbgetdz9N5J+E3EtAIBxcGUiAASOoAaAwBHUABA4ghoAAjfhvT6m9KFmHZK2TfHHWyTtLWE5Mx3jcQxjcTzG43gzfTyWuHtuvDciCerpMLP2N7sxSSViPI5hLI7HeBxvNo8HrQ8ACBxBDQCBCzGoV8ddQGAYj2MYi+MxHsebteMRXI8aAHC8EGfUAIARCGoACFwwQV2JD9A1szvMbI+ZbRixb66ZPWBmLxa+zxnx3k2F8XnezC6Pp+romNmJZvaQmW0ys41mdkNhf8WNiZllzOxPZvZMYSy+XthfcWMxkpklzOxpM7uv8LoyxsPdY/9S/vapWyQtk5SS9Iyk0+Kuqwx/98WSzpW0YcS+/5R0Y2H7RknfKmyfVhiXtKSlhfFKxP03lHg8Fkg6t7Bdr/xDlU+rxDFR/slK2cJ2taS1ks6vxLEYNS5fkPQTSfcVXlfEeIQyox5+gK6790oaeoDurObuayTtG7X7akk/LGz/UNLfjth/l7v3uPvLkl5SftxmDXff5e5PFbYPSdqk/DM7K25MPO9w4WV14ctVgWMxxMwWS3q/pNtH7K6I8QglqMd7gO6imGqJ2zx33yXlg0vSCYX9FTVGZtYq6RzlZ5IVOSaFf/PXS9oj6QF3r9ixKLhF0pclDY7YVxHjEUpQF/UA3QpXMWNkZllJP5f0OXc/+FaHjrNv1oyJuw+4+9nKP6d0hZmd8RaHz+qxMLMPSNrj7uuK/ZFx9s3Y8QglqHmA7jG7zWyBJBW+7ynsr4gxMrNq5UP6Tne/t7C7osfE3Q9IeljSFarcsVgp6Soze0X51uilZvZjVch4hBLUPED3mF9L+lhh+2OSfjVi/7VmljazpZJOkfSnGOqLjJmZpO9J2uTuN494q+LGxMxyZtZU2K6RdJmkzarAsZAkd7/J3Re7e6vy+fAHd79OlTIecZ/NHHE290rlz/JvkfSVuOsp09/8U0m7JPUpPwP4pKRmSQ9KerHwfe6I479SGJ/nJb0v7vojGI+LlP/39FlJ6wtfV1bimEg6U9LThbHYIOmrhf0VNxbjjM27dGzVR0WMB5eQA0DgQml9AADeBEENAIEjqAEgcAQ1AASOoAaAwBHUABA4ghoAAvf/ZiTdiv50LuAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check if each instrument can reconstruct its part, given the other instrument's part\n",
    "min_time_shift = 0.01\n",
    "time_start = 2*num_notes + num_velocities # Any message greater than this is a time shift message\n",
    "model.eval() # Turns off the dropout for evaluation. Need to do this to get repeatable evaluation outputs\n",
    "\n",
    "for inst in range(max_instruments):\n",
    "    gen_history = history.clone()\n",
    "    gen_mask = mask.clone()\n",
    "    gen_times = times.clone()\n",
    "\n",
    "    gen_idx = torch.tensor([inst], dtype=torch.long)\n",
    "    \n",
    "    seq_length = min(recording[inst].shape[0], max_seq_length)\n",
    "\n",
    "    # Move forward in time\n",
    "    wrong_cnt = 0\n",
    "    for t in range(1, seq_length):\n",
    "        cur_times = torch.gather(gen_times[t], 0, gen_idx.view(1, -1)).unsqueeze(0)\n",
    "        cur_mask = torch.logical_or(gen_mask, gen_times > cur_times.expand(max_seq_length, max_instruments, -1))\n",
    "        for seq in range(batch_size):\n",
    "            cur_mask[t:, gen_idx[seq], seq] = True\n",
    "                \n",
    "        probs = torch.nn.functional.softmax(model(gen_history, cur_mask, instruments, gen_idx, t), dim=1)\n",
    "        gen_history[t, gen_idx, 0] = torch.multinomial(probs, 1).view(1, 1)\n",
    "        if torch.argmax(probs.flatten()) != history[t, gen_idx].flatten():\n",
    "            wrong_cnt += 1\n",
    "            print(torch.topk(probs.flatten(), 10))\n",
    "            print(history[t, gen_idx])\n",
    "            \n",
    "        # If it's a time shift message, update time of next message accordingly\n",
    "        if t < seq_length - 1 and gen_history[t, gen_idx, 0] >= time_start:\n",
    "            gen_times[t + 1, gen_idx, 0] = gen_times[t, gen_idx, 0] + min_time_shift*(gen_history[t, gen_idx, 0] + 1)\n",
    "\n",
    "    print(wrong_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-e02c1fa869e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mcur_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstruments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mgen_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-aee1dd515dc5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, history, mask, instruments, gen_idx, t)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# TODO: do I need the memory_key_padding_mask? I think so, because some of the encoder outputs are from padding values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mdecoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    235\u001b[0m                          \u001b[0mmemory_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                          \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m                          memory_key_padding_mask=memory_key_padding_mask)\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    373\u001b[0m                                    key_padding_mask=memory_key_padding_mask)[0]\n\u001b[1;32m    374\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m         \u001b[0mtgt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         return F.layer_norm(\n\u001b[0;32m--> 151\u001b[0;31m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2047\u001b[0m     \"\"\"\n\u001b[1;32m   2048\u001b[0m     return torch.layer_norm(input, normalized_shape, weight, bias, eps,\n\u001b[0;32m-> 2049\u001b[0;31m                             torch.backends.cudnn.enabled)\n\u001b[0m\u001b[1;32m   2050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Check if the instruments can jointly reconstruct the piece\n",
    "min_time_shift = 0.01\n",
    "time_start = 2*num_notes + num_velocities # Any message greater than this is a time shift message\n",
    "\n",
    "gen_history = history.clone()\n",
    "gen_times = times.clone()\n",
    "gen_times[1:] = float('inf')\n",
    "\n",
    "model.eval() # Turns off the dropout for evaluation. Need to do this to get repeatable evaluation outputs\n",
    "\n",
    "wrong_cnt = 0\n",
    "\n",
    "ts = [1 for inst in range(max_instruments)]\n",
    "\n",
    "# List of instruments who haven't generated 100 messages yet\n",
    "incomplete = list(range(max_instruments))\n",
    "\n",
    "while incomplete:\n",
    "    print(ts)\n",
    "    # Find instrument who's most behind\n",
    "    gen_idx = incomplete[0]\n",
    "    gen_time = gen_times[ts[gen_idx], gen_idx]\n",
    "    for inst in incomplete:\n",
    "        if gen_times[ts[inst], inst] < gen_time:\n",
    "            gen_time = gen_times[ts[inst], inst]\n",
    "            gen_idx = inst\n",
    "            \n",
    "    t = ts[gen_idx]\n",
    "    gen_idx = torch.tensor([gen_idx], dtype=torch.long)\n",
    "    gen_time = torch.tensor(gen_time).view(1, 1, 1)\n",
    "    cur_mask = gen_times > gen_time.expand(max_seq_length, max_instruments, -1)\n",
    "    cur_mask[t:, gen_idx] = True\n",
    "\n",
    "    probs = torch.nn.functional.softmax(model(gen_history, cur_mask, instruments, gen_idx, t), dim=1)\n",
    "    gen_history[t, gen_idx, 0] = torch.multinomial(probs, 1).view(1, 1)\n",
    "\n",
    "    # If it's a time shift message, update time of next message accordingly\n",
    "    if t < max_seq_length - 1 and gen_history[t, gen_idx, 0] >= time_start:\n",
    "        gen_times[t + 1, gen_idx, 0] = gen_times[t, gen_idx, 0] + min_time_shift*(gen_history[t, gen_idx, 0] + 1)\n",
    "        \n",
    "    t += 1\n",
    "    if t >= max_seq_length:\n",
    "        incomplete.remove(gen_idx)\n",
    "    \n",
    "    ts[gen_idx.item()] = t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class MIDIDataset(torch.utils.data.Dataset):\n",
    "    # CONSTRUCTOR: creates a list of recording files and a list\n",
    "    # of instrument files in root_dir. Assumes that the directory\n",
    "    # contains recording0.npy to recordingM.npy,\n",
    "    # as well as instruments0.npy to instrumentsM.npy\n",
    "    # ARGUMENTS\n",
    "    # root_dir: the directory to search\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        files = os.listdir(root_dir)\n",
    "        self.recordings = []\n",
    "        self.instrument_files = []\n",
    "        self.time_files = []\n",
    "        for file in files:\n",
    "            if 'recording' in file:\n",
    "                self.recordings.append(os.path.join(root_dir, file))\n",
    "            elif 'instruments' in file:\n",
    "                self.instrument_files.append(os.path.join(root_dir, file))\n",
    "            elif 'times' in file:\n",
    "                self.time_files.append(os.path.join(root_dir, file))\n",
    "                \n",
    "        assert(len(self.recordings) == len(self.instrument_files))\n",
    "        assert(len(self.recordings) == len(self.time_files))\n",
    "        self.recordings.sort()\n",
    "        self.instrument_files.sort()\n",
    "        self.time_files.sort()\n",
    "        self.transform = transform\n",
    "\n",
    "    # __len__\n",
    "    # RETURN: the number of recording files in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.recordings)\n",
    "\n",
    "    # __getitem__\n",
    "    # ARGUMENTS\n",
    "    # idx: indicates which file to get\n",
    "    # RETURN: an instance with keys 'instruments', 'history', and 'times'\n",
    "    # instance['history'] is a numpy array of message sequences for each instrument\n",
    "    # instance['instruments'] a numpy array of instrument numbers\n",
    "    # instance['times'] is a numpy array of message time sequences for each instrument\n",
    "    def __getitem__(self, idx):\n",
    "        instance = {'history': np.load(self.recordings[idx], allow_pickle=True), \\\n",
    "                    'instruments': np.load(self.instrument_files[idx], allow_pickle=True), \\\n",
    "                    'times': np.load(self.time_files[idx], allow_pickle=True)}\n",
    "        \n",
    "        assert(len(instance['history']) == len(instance['times']))\n",
    "        assert(len(instance['history']) == len(instance['instruments']))\n",
    "        \n",
    "        for i in range(len(instance['history'])):\n",
    "            assert(len(instance['history'][i]) == len(instance['times'][i]))\n",
    "        \n",
    "        if self.transform:\n",
    "            instance = self.transform(instance)\n",
    "            \n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn: takes a list of samples from the dataset and turns them into a batch\n",
    "# ARGUMENTS\n",
    "# batch: a list of dictionaries\n",
    "# RETURN: a sample with keys 'history', 'instruments', 'times', and 'mask'\n",
    "# sample['history']: an LxNxB tensor containing messages\n",
    "# sample['instruments']: a 1xNxB tensor containing instrument numbers\n",
    "# sample['mask']: an LxNxB tensor containing False where a message is\n",
    "# valid, and True where it isn't (accounts for variable length sequences\n",
    "# and zero padding)\n",
    "# sample['times']: an LxNxB tensor containing times of each message\n",
    "def collate_fn(batch):\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    # We size our tensors to accomodate the longest sequence and the largest ensemble\n",
    "    max_instruments = max([len(instance['history']) for instance in batch])\n",
    "    longest_len = max([max([seq.shape[0] for seq in instance['history']]) for instance in batch])\n",
    "\n",
    "    sample = {'history': torch.ones((longest_len, max_instruments, batch_size), dtype=torch.long), \\\n",
    "              'instruments': torch.zeros((1, max_instruments, batch_size), dtype=torch.long), \\\n",
    "              'times': torch.zeros((longest_len, max_instruments, batch_size)), \\\n",
    "              'mask': torch.ones((longest_len, max_instruments, batch_size), dtype=torch.bool)}\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        instrument_idx = [instrument_numbers.index(inst) for inst in batch[b]['instruments']]\n",
    "        sample['instruments'][0, :len(instrument_idx), b] = torch.tensor(instrument_idx, dtype=torch.long)\n",
    "        \n",
    "        for inst_idx in range(len(batch[b]['history'])):\n",
    "            seq_length = len(batch[b]['history'][inst_idx])\n",
    "            sample['history'][:seq_length, inst_idx, b] = torch.tensor(batch[b]['history'][inst_idx], dtype=torch.long)\n",
    "            sample['mask'][:seq_length, inst_idx, b] = False\n",
    "            sample['times'][:seq_length, inst_idx, b] = torch.tensor(batch[b]['times'][inst_idx])\n",
    "            \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_size = 512\n",
    "heads = 8\n",
    "attention_layers=  6\n",
    "embed_dim = 256\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleTransformer(message_dim, embed_dim, len(instrument_numbers), heads, attention_layers, ff_size)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "dataset = MIDIDataset('preprocessed_data')\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "train_losses = np.zeros(epochs)\n",
    "test_losses = np.zeros(epochs) # TODO: train/test split. Can we do this with Dataloader?\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    for b, batch in enumerate(dataloader):\n",
    "        print('Starting iteration %d' %(b))\n",
    "        max_seq_length = batch['history'].shape[0]\n",
    "        num_targets = max_seq_length - 1 # Messages start from t = 0, but we start generating at t = 1\n",
    "        max_instruments = batch['history'].shape[1]\n",
    "        loss = torch.tensor([0])\n",
    "        for inst in range(max_instruments):\n",
    "            # Index of the instrument we want to generate music for (same for each batch element as of now)\n",
    "            gen_idx = torch.tensor([inst for i in range(batch_size)], dtype=torch.long)\n",
    "            \n",
    "            mask = batch['mask']\n",
    "            \n",
    "            logits = torch.zeros((num_targets, batch_size, message_dim))\n",
    "\n",
    "            # Move forward in time\n",
    "            for t in range(1, max_seq_length):\n",
    "                # Get current time for each batch element (1x1xB) and mask out messages with a greater time stamp\n",
    "                times = torch.gather(batch['times'][t], 0, gen_idx.view(1, -1)).unsqueeze(0)\n",
    "                input_mask = torch.logical_or(mask, batch['times'] > times.expand(max_seq_length, max_instruments, -1))\n",
    "                for seq in range(batch_size):\n",
    "                    input_mask[t:, gen_idx[seq], seq] = True\n",
    "                \n",
    "                logits[t - 1] = model(batch['history'], input_mask, batch['instruments'], gen_idx, t)\n",
    "                \n",
    "            logits = logits.view(-1, message_dim)\n",
    "            target_messages = torch.gather(batch['history'][1:], 1, gen_idx.view(1, 1, -1).expand(num_targets, -1, -1)).flatten()\n",
    "            output_mask = torch.gather(mask[1:], 1, gen_idx.view(1, 1, -1).expand(num_targets, -1, -1)).flatten()\n",
    "            loss += loss_fn(logits[output_mask], target_messages[output_mask])\n",
    "        \n",
    "        loss /= max_instruments\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses[epoch] += loss.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # Disable dropout to make results repeatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
