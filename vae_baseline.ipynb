{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we're using a GPU\n",
    "dev = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we're using a CPU\n",
    "dev = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class MIDIDataset(torch.utils.data.Dataset):\n",
    "    # CONSTRUCTOR: creates an array of message chunks. Assumes that the directory\n",
    "    # contains recording0.npy to recordingM.npy\n",
    "    # ARGUMENTS\n",
    "    # root_dir: the directory to search\n",
    "    # chunk_size: we'll chunk the data into chunks of this size (no more, no less)\n",
    "    def __init__(self, root_dir, chunk_size, transform=None):\n",
    "        self.chunks = []\n",
    "        self.masks = []\n",
    "        \n",
    "        ch = 0\n",
    "        for f, file in enumerate(os.listdir(root_dir)):\n",
    "            data = np.load(root_dir + '/' + file)\n",
    "            nchunks = int(np.floor(data.shape[0]/chunk_size))\n",
    "            self.chunks += [torch.zeros(chunk_size, dtype=torch.long) for c in range(nchunks)]\n",
    "            for chunk_start in range(0, data.shape[0], chunk_size):\n",
    "                if chunk_start + chunk_size > data.shape[0]:\n",
    "                    break\n",
    "                self.chunks[ch] = torch.tensor(data[chunk_start:chunk_start + chunk_size])\n",
    "                ch += 1\n",
    "            \n",
    "            if f%100 == 0:\n",
    "                print(f)\n",
    "            \n",
    "        self.transform = transform\n",
    "\n",
    "    # __len__\n",
    "    # RETURN: the number of chunks in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "\n",
    "    # __getitem__\n",
    "    # ARGUMENTS\n",
    "    # idx: indicates which chunk to get\n",
    "    # RETURN: instance, a dictionary with keys 'messages' and 'mask'.\n",
    "    # Both values associated with these keys are length L tensors\n",
    "    def __getitem__(self, idx):  \n",
    "        instance = self.chunks[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            instance = self.transform(instance)\n",
    "            \n",
    "        return instance\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    chunk_size = batch[0].shape[0]\n",
    "    sample = torch.zeros((chunk_size, len(batch)), dtype=torch.long)\n",
    "    for b, instance in enumerate(batch):\n",
    "        sample[:, b] = instance\n",
    "        \n",
    "    return sample.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2414"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = MIDIDataset('train_vae_baseline', chunk_size)\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "659"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = MIDIDataset('test_vae_baseline', chunk_size)\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEEncoder(torch.nn.Module):\n",
    "    def __init__(self, message_dim, hidden_size, latent_size):\n",
    "        super(VAEEncoder, self).__init__()\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, num_layers=2)\n",
    "        self.mean = torch.nn.Linear(hidden_size, latent_size)\n",
    "        self.logvar = torch.nn.Linear(hidden_size, latent_size)\n",
    "\n",
    "    def forward(self, message_embed):\n",
    "        L = message_embed.shape[0]\n",
    "        B = message_embed.shape[1]\n",
    "        output, hidden = self.gru(message_embed)\n",
    "        z_mean = self.mean(output[-1]).view(1, B, -1)\n",
    "        z_logvar = self.logvar(output[-1]).view(1, B, -1)\n",
    "        return z_mean, z_logvar\n",
    "\n",
    "class VAEDecoder(torch.nn.Module):\n",
    "    def __init__(self, latent_size, hidden_size, message_dim, section_size):\n",
    "        super(VAEDecoder, self).__init__()\n",
    "\n",
    "        self.section_size = section_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_size = latent_size\n",
    "        self.conductor_linear = torch.nn.Linear(latent_size, hidden_size)\n",
    "        self.conductor_gru = torch.nn.GRU(1, hidden_size, num_layers=2)\n",
    "        self.gru = torch.nn.GRU(2*hidden_size, hidden_size, num_layers=2)\n",
    "        self.logits = torch.nn.Linear(hidden_size, message_dim)\n",
    "\n",
    "    # Return teacher forced logits\n",
    "    def forward(self, z, forced_input_embed):\n",
    "        L = forced_input_embed.shape[0]\n",
    "        B = forced_input_embed.shape[1]\n",
    "        assert(z.shape == (1, B, self.latent_size))\n",
    "        \n",
    "        nsections = int(np.ceil(L/self.section_size))\n",
    "        \n",
    "        conductor_hidden = torch.tanh(self.conductor_linear(z)).repeat(2, 1, 1)\n",
    "        conductor_input = torch.zeros((1, B, 1)).to(dev) # Null input for conductor\n",
    "        init_gru_input = torch.zeros((1, B, self.hidden_size)).to(dev)\n",
    "        outputs = []\n",
    "        for section in range(nsections):\n",
    "            hid, conductor_hidden = self.conductor_gru(conductor_input, conductor_hidden)\n",
    "            \n",
    "            start = section*section_size\n",
    "            end = min(start + section_size - 1, L)\n",
    "            \n",
    "            # Teacher forcing\n",
    "            gru_inputs = torch.cat((init_gru_input, forced_input_embed[start:end]), dim=0)\n",
    "            init_hidden = conductor_hidden.clone()\n",
    "            gru_inputs = torch.cat((gru_inputs, init_hidden[0].unsqueeze(0).expand(gru_inputs.shape[0], -1, -1)), dim=2)\n",
    "            output, hidden = self.gru(gru_inputs, init_hidden)\n",
    "            outputs.append(output)\n",
    "            \n",
    "        return self.logits(torch.cat(outputs, dim=0))\n",
    "    \n",
    "    def generate(self, z, nsections, embedding):\n",
    "        assert(z.shape == (1, 1, self.latent_size))\n",
    "        \n",
    "        conductor_hidden = torch.tanh(self.conductor_linear(z)).repeat(2, 1, 1)\n",
    "        conductor_input = torch.zeros((1, 1, 1)).to(dev) # Null input for conductor\n",
    "        messages = []\n",
    "        for section in range(nsections):\n",
    "            hid, conductor_hidden = self.conductor_gru(conductor_input, conductor_hidden)\n",
    "            hidden = conductor_hidden.clone()\n",
    "            gru_input = torch.cat((torch.zeros((1, 1, self.hidden_size)).to(dev), conductor_hidden[0].unsqueeze(0)), dim=2)\n",
    "            \n",
    "            for message in range(self.section_size):\n",
    "                output, hidden = self.gru(gru_input, hidden)\n",
    "                probs = torch.nn.functional.softmax(self.logits(output), dim=2).flatten()\n",
    "                messages.append(torch.multinomial(probs, 1))\n",
    "                gru_input = torch.cat((embedding(messages[-1]).view(1, 1, -1), conductor_hidden[0].unsqueeze(0)), dim=2)\n",
    "            \n",
    "        return [message.item() for message in messages]\n",
    "    \n",
    "class BaselineVAE(torch.nn.Module):\n",
    "    def __init__(self, message_dim, hidden_size, latent_size, section_size):\n",
    "        super(BaselineVAE, self).__init__()\n",
    "        \n",
    "        self.message_dim = message_dim\n",
    "        self.embedding = torch.nn.Embedding(message_dim, hidden_size)\n",
    "        self.encoder = VAEEncoder(message_dim, hidden_size, latent_size)\n",
    "        self.decoder = VAEDecoder(latent_size, hidden_size, message_dim, section_size)\n",
    "        \n",
    "    # Compute loss\n",
    "    def forward(self, messages, kld_weight):\n",
    "        L = messages.shape[0]\n",
    "        B = messages.shape[1]\n",
    "        \n",
    "        message_embed = self.embedding(messages[:-1])\n",
    "        z_mean, z_logvar = self.encoder(message_embed)\n",
    "        z_std = torch.exp(0.5*z_logvar)\n",
    "        eps = torch.randn_like(z_mean)\n",
    "        z = z_mean + eps*z_std\n",
    "        logits = self.decoder(z, message_embed)\n",
    "\n",
    "        return torch.nn.functional.cross_entropy(logits.view(-1, self.message_dim), messages.flatten()) - \\\n",
    "               kld_weight*0.5*torch.sum(1 + z_logvar - z_mean*z_mean - torch.exp(z_logvar))\n",
    "    \n",
    "    # Reconstruct messages\n",
    "    def reconstruct(self, messages):\n",
    "        L = messages.shape[0]\n",
    "        assert(messages.shape[1] == 1)\n",
    "        \n",
    "        message_embed = self.embedding(messages[:-1])\n",
    "        z_mean, z_logvar = self.encoder(message_embed)\n",
    "        z_std = torch.exp(0.5*z_logvar)\n",
    "        eps = torch.randn_like(z_mean)\n",
    "        z = z_mean + eps*z_std\n",
    "        \n",
    "        nsections = int(np.ceil(L/self.decoder.section_size))\n",
    "        return self.decoder.generate(z, nsections, self.embedding)\n",
    "    \n",
    "        # Was using teacher forcing before\n",
    "        #logits = self.decoder(z, message_embed).view(-1, self.message_dim)\n",
    "        #return torch.multinomial(torch.nn.functional.softmax(logits, dim=1), 1).flatten()\n",
    "    \n",
    "    # Interpolate between the two sequences and produce a mashup (hopefully lol)\n",
    "    def interpolate(self, messages1, messages2, weight1):\n",
    "        L = messages1.shape[0]\n",
    "        assert(messages.shape[1] == 1 and messages2.shape[1] == 1)\n",
    "        assert(0 <= weight1 <= 1)\n",
    "        \n",
    "        message_embed = self.embedding(messages1[:-1])\n",
    "        z_mean, z_logvar = self.encoder(message_embed)\n",
    "        z_std = torch.exp(0.5*z_logvar)\n",
    "        eps = torch.randn_like(z_mean)\n",
    "        z1 = z_mean + eps*z_std\n",
    "        \n",
    "        message_embed = self.embedding(messages2[:-1])\n",
    "        z_mean, z_logvar = self.encoder(message_embed)\n",
    "        z_std = torch.exp(0.5*z_logvar)\n",
    "        eps = torch.randn_like(z_mean)\n",
    "        z2 = z_mean + eps*z_std\n",
    "        \n",
    "        z = weight1*z1 + weight2*z2\n",
    "        \n",
    "        nsections = int(np.ceil(L/self.decoder.section_size))\n",
    "        return self.decoder.generate(z, nsections, self.embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization parameters:\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Model parameters: \n",
    "num_notes = 128\n",
    "num_time_shifts = 100 \n",
    "message_dim = 2*num_notes + num_time_shifts\n",
    "hidden_size = 1024\n",
    "latent_size = 512\n",
    "section_size = 200\n",
    "\n",
    "# Checkpoint location\n",
    "checkpoint_dir = 'baseline_vae_checkpoints'\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "model = BaselineVAE(message_dim, hidden_size, latent_size, section_size).to(dev)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses = [0 for epoch in range(epochs)]\n",
    "test_losses = [0 for epoch in range(epochs)]\n",
    "iterations = 0\n",
    "nbatches = len(train_dataloader)\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    model.train()\n",
    "    for b, batch in enumerate(train_dataloader):\n",
    "        if b%10 == 0:\n",
    "            print('Starting iteration %d' %(b))\n",
    "            \n",
    "        # KLD cost annealing\n",
    "        kld_weight = 0.2*(1 - np.exp(-2*iterations/nbatches))\n",
    "        \n",
    "        loss = model(batch, kld_weight)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        iterations += 1\n",
    "        \n",
    "    torch.save(model.state_dict(), checkpoint_dir + '/epoch' + str(epoch) + '.pth')\n",
    "        \n",
    "    model.eval()\n",
    "    print('Computing test loss')\n",
    "    for b, batch in enumerate(test_dataloader):\n",
    "        test_losses[epoch] += model(batch, 1).item()\n",
    "        \n",
    "    test_losses[epoch] /= len(test_dataloader)\n",
    "        \n",
    "    print('Computing train loss')\n",
    "    for b, batch in enumerate(train_dataloader):\n",
    "        train_losses[epoch] += model(batch, 1).item()\n",
    "        \n",
    "    train_losses[epoch] /= len(train_dataloader)\n",
    "        \n",
    "    print('Train loss: %f, Test loss: %f' %(train_losses[epoch], test_losses[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(checkpoint_dir + '/train_losses.npy', np.array(train_losses))\n",
    "np.save(checkpoint_dir + '/test_losses.npy', np.array(test_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = np.load(checkpoint_dir + '/train_losses.npy', allow_pickle=True)\n",
    "test_losses = np.load(checkpoint_dir + '/test_losses.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "COLOR = 'white'\n",
    "plt.rcParams['text.color'] = COLOR\n",
    "plt.rcParams['axes.labelcolor'] = COLOR\n",
    "plt.rcParams['xtick.color'] = COLOR\n",
    "plt.rcParams['ytick.color'] = COLOR\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 100 # 200 e.g. is really fine, but slower\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Loss (Nats)')\n",
    "plt.title('Training Loss')\n",
    "plt.savefig(checkpoint_dir + '/vae_baseline_train_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Loss (Nats)')\n",
    "plt.title('Training Loss')\n",
    "plt.savefig(checkpoint_dir + '/vae_baseline_test_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BaselineVAE(message_dim, hidden_size, latent_size, section_size).to(dev)\n",
    "model.load_state_dict(torch.load(checkpoint_dir + '/epoch9.pth', map_location=dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from model\n",
    "def generate_music(model, gen_length=500):\n",
    "    z = torch.randn_like(torch.zeros((1, 1, latent_size)))\n",
    "    nsections = int(np.ceil(gen_length/section_size))\n",
    "    return model.decoder.generate(z, nsections, model.embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_music = generate_music(model, gen_length=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('baseline_vae_midis/sample.npy', np.array(generated_music))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reconstruction\n",
    "original = train_data[50].unsqueeze(1)\n",
    "reconstruction = model.reconstruct(original) # Should sound roughly the same as the original\n",
    "np.save('baseline_vae_midis/original50.npy', original.flatten().detach().numpy())\n",
    "np.save('baseline_vae_midis/reconstruction50.npy', np.array(reconstruction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
