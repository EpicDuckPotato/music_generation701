{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we're using a GPU\n",
    "dev = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we're using a CPU\n",
    "dev = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class MIDIDataset(torch.utils.data.Dataset):\n",
    "    # CONSTRUCTOR: creates an array of message chunks. Assumes that the directory\n",
    "    # contains recording0.npy to recordingM.npy\n",
    "    # ARGUMENTS\n",
    "    # root_dir: the directory to search\n",
    "    # chunk_size: we'll chunk the data into chunks of this size (or less)\n",
    "    def __init__(self, root_dir, chunk_size, transform=None):\n",
    "        self.chunks = []\n",
    "        self.masks = []\n",
    "        \n",
    "        ch = 0\n",
    "        for f, file in enumerate(os.listdir(root_dir)):\n",
    "            data = np.load(root_dir + '/' + file)\n",
    "            nchunks = int(np.floor(data.shape[0]/chunk_size))\n",
    "            self.chunks += [torch.zeros(chunk_size, dtype=torch.long) for c in range(nchunks)]\n",
    "            for chunk_start in range(0, data.shape[0], chunk_size):\n",
    "                if chunk_start + chunk_size > data.shape[0]:\n",
    "                    break\n",
    "                self.chunks[ch] = torch.tensor(data[chunk_start:chunk_start + chunk_size])\n",
    "                ch += 1\n",
    "            \n",
    "            if f%100 == 0:\n",
    "                print(f)\n",
    "            \n",
    "        self.transform = transform\n",
    "\n",
    "    # __len__\n",
    "    # RETURN: the number of chunks in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "\n",
    "    # __getitem__\n",
    "    # ARGUMENTS\n",
    "    # idx: indicates which chunk to get\n",
    "    # RETURN: instance, a dictionary with keys 'messages' and 'mask'.\n",
    "    # Both values associated with these keys are length L tensors\n",
    "    def __getitem__(self, idx):  \n",
    "        instance = self.chunks[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            instance = self.transform(instance)\n",
    "            \n",
    "        return instance\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    chunk_size = batch[0].shape[0]\n",
    "    sample = torch.zeros((chunk_size, len(batch)), dtype=torch.long)\n",
    "    for b, instance in enumerate(batch):\n",
    "        sample[:, b] = instance\n",
    "        \n",
    "    return sample.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2414"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = MIDIDataset('train_vae_baseline', chunk_size)\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "659"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = MIDIDataset('test_vae_baseline', chunk_size)\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEEncoder(torch.nn.Module):\n",
    "    def __init__(self, message_dim, hidden_size, latent_size):\n",
    "        super(VAEEncoder, self).__init__()\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, num_layers=2)\n",
    "        self.mean = torch.nn.Linear(hidden_size, latent_size)\n",
    "        self.logvar = torch.nn.Linear(hidden_size, latent_size)\n",
    "\n",
    "    def forward(self, message_embed):\n",
    "        L = message_embed.shape[0]\n",
    "        B = message_embed.shape[1]\n",
    "        output, hidden = self.gru(message_embed)\n",
    "        z_mean = self.mean(output[-1]).view(1, B, -1)\n",
    "        z_logvar = self.logvar(output[-1]).view(1, B, -1)\n",
    "        return z_mean, z_logvar\n",
    "\n",
    "class VAEDecoder(torch.nn.Module):\n",
    "    def __init__(self, latent_size, hidden_size, message_dim, section_size):\n",
    "        super(VAEDecoder, self).__init__()\n",
    "\n",
    "        self.section_size = section_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_size = latent_size\n",
    "        self.conductor_linear = torch.nn.Linear(latent_size, hidden_size)\n",
    "        self.conductor_gru = torch.nn.GRU(1, hidden_size, num_layers=2)\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, num_layers=2)\n",
    "        self.logits = torch.nn.Linear(hidden_size, message_dim)\n",
    "\n",
    "    # Return teacher forced logits\n",
    "    def forward(self, z, forced_input_embed):\n",
    "        L = forced_input_embed.shape[0]\n",
    "        B = forced_input_embed.shape[1]\n",
    "        assert(z.shape == (1, B, self.latent_size))\n",
    "        \n",
    "        nsections = int(np.ceil(L/self.section_size))\n",
    "        \n",
    "        conductor_hidden = torch.tanh(self.conductor_linear(z)).repeat(2, 1, 1)\n",
    "        conductor_input = torch.zeros((1, B, 1)).to(dev) # Null input for conductor\n",
    "        init_gru_input = torch.zeros((1, B, self.hidden_size)).to(dev)\n",
    "        outputs = []\n",
    "        for section in range(nsections):\n",
    "            hid, conductor_hidden = self.conductor_gru(conductor_input, conductor_hidden)\n",
    "            \n",
    "            start = section*section_size\n",
    "            end = min(start + section_size - 1, L)\n",
    "            \n",
    "            # Teacher forcing\n",
    "            gru_inputs = torch.cat((init_gru_input, forced_input_embed[start:end]), dim=0)\n",
    "            init_hidden = conductor_hidden.clone()\n",
    "            output, hidden = self.gru(gru_inputs, init_hidden)\n",
    "            outputs.append(output)\n",
    "            \n",
    "        return self.logits(torch.cat(outputs, dim=0))\n",
    "    \n",
    "    def generate(self, z, nsections, embedding):\n",
    "        assert(z.shape == (1, 1, self.latent_size))\n",
    "        \n",
    "        conductor_hidden = torch.tanh(self.conductor_linear(z)).repeat(2, 1, 1)\n",
    "        conductor_input = torch.zeros((1, 1, 1)).to(dev) # Null input for conductor\n",
    "        gru_input = torch.zeros((1, 1, self.hidden_size)).to(dev)\n",
    "        messages = []\n",
    "        for section in range(nsections):\n",
    "            hid, conductor_hidden = self.conductor_gru(conductor_input, conductor_hidden)\n",
    "            hidden = conductor_hidden.clone()\n",
    "            \n",
    "            for message in range(self.section_size):\n",
    "                output, hidden = self.gru(gru_input, hidden)\n",
    "                probs = torch.nn.functional.softmax(self.logits(output), dim=2).flatten()\n",
    "                messages.append(torch.multinomial(probs, 1))\n",
    "                gru_input = embedding(messages[-1]).view(1, 1, -1)\n",
    "            \n",
    "        return messages\n",
    "    \n",
    "class BaselineVAE(torch.nn.Module):\n",
    "    def __init__(self, message_dim, hidden_size, latent_size, section_size):\n",
    "        super(BaselineVAE, self).__init__()\n",
    "        \n",
    "        self.message_dim = message_dim\n",
    "        self.embedding = torch.nn.Embedding(message_dim, hidden_size)\n",
    "        self.encoder = VAEEncoder(message_dim, hidden_size, latent_size)\n",
    "        self.decoder = VAEDecoder(latent_size, hidden_size, message_dim, section_size)\n",
    "        self.normal_sampler = torch.distributions.normal.Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "        \n",
    "    # Compute loss\n",
    "    def forward(self, messages):\n",
    "        L = messages.shape[0]\n",
    "        B = messages.shape[1]\n",
    "        \n",
    "        message_embed = self.embedding(messages[:-1])\n",
    "        z_mean, z_logvar = self.encoder(message_embed)\n",
    "        z_std = torch.exp(0.5*z_logvar)\n",
    "        eps = self.normal_sampler.sample(sample_shape=(1, B)).to(dev)\n",
    "        z = z_mean + eps*z_std\n",
    "        logits = self.decoder(z, message_embed)\n",
    "\n",
    "        return torch.nn.functional.cross_entropy(logits.view(-1, self.message_dim), messages.flatten()) - \\\n",
    "               0.5*torch.sum(1 + z_logvar - z_mean*z_mean - torch.exp(z_logvar))\n",
    "    \n",
    "    # Reconstruct messages\n",
    "    def reconstruct(self, messages):\n",
    "        L = messages.shape[0]\n",
    "        assert(messages.shape[1] == 1)\n",
    "        \n",
    "        message_embed = self.embedding(messages[:-1])\n",
    "        z_mean, z_logvar = self.encoder(message_embed)\n",
    "        z_std = torch.exp(0.5*z_logvar)\n",
    "        eps = self.normal_sampler.sample(sample_shape=(1, 1)).to(dev)\n",
    "        z = z_mean + eps*z_std\n",
    "        logits = self.decoder(z, message_embed).view(-1, self.message_dim)\n",
    "        return torch.multinomial(torch.nn.functional.softmax(logits, dim=1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization parameters:\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Model parameters: \n",
    "message_dim = 388\n",
    "hidden_size = 1024\n",
    "latent_size = 512\n",
    "section_size = 200\n",
    "\n",
    "# Checkpoint location\n",
    "checkpoint_dir = 'baseline_vae_checkpoints'\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "model = BaselineVAE(message_dim, hidden_size, latent_size, section_size).to(dev)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Starting iteration 0\n",
      "Starting iteration 10\n",
      "Starting iteration 20\n",
      "Starting iteration 30\n",
      "Starting iteration 40\n",
      "Starting iteration 50\n",
      "Starting iteration 60\n",
      "Starting iteration 70\n",
      "Starting iteration 80\n",
      "Starting iteration 90\n",
      "Starting iteration 100\n",
      "Starting iteration 110\n",
      "Starting iteration 120\n",
      "Starting iteration 130\n",
      "Starting iteration 140\n",
      "Starting iteration 150\n",
      "Starting iteration 160\n",
      "Starting iteration 170\n",
      "Starting iteration 180\n",
      "Starting iteration 190\n",
      "Starting iteration 200\n",
      "Starting iteration 210\n",
      "Starting iteration 220\n",
      "Starting iteration 230\n",
      "Starting iteration 240\n",
      "Starting iteration 250\n",
      "Starting iteration 260\n",
      "Starting iteration 270\n",
      "Starting iteration 280\n",
      "Starting iteration 290\n",
      "Starting iteration 300\n",
      "Starting iteration 310\n",
      "Starting iteration 320\n",
      "Starting iteration 330\n",
      "Starting iteration 340\n",
      "Starting iteration 350\n",
      "Starting iteration 360\n",
      "Starting iteration 370\n",
      "Starting iteration 380\n",
      "Starting iteration 390\n",
      "Starting iteration 400\n",
      "Starting iteration 410\n",
      "Starting iteration 420\n",
      "Starting iteration 430\n",
      "Starting iteration 440\n",
      "Starting iteration 450\n",
      "Starting iteration 460\n",
      "Starting iteration 470\n",
      "Computing test loss\n",
      "Computing train loss\n",
      "Train loss: 1.872301, Test loss: 2.183596\n",
      "Starting epoch 1\n",
      "Starting iteration 0\n",
      "Starting iteration 10\n",
      "Starting iteration 20\n",
      "Starting iteration 30\n",
      "Starting iteration 40\n",
      "Starting iteration 50\n",
      "Starting iteration 60\n",
      "Starting iteration 70\n",
      "Starting iteration 80\n",
      "Starting iteration 90\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-e220434b9e1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = [0 for epoch in range(epochs)]\n",
    "test_losses = [0 for epoch in range(epochs)]\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    model.train()\n",
    "    for b, batch in enumerate(train_dataloader):\n",
    "        if b%10 == 0:\n",
    "            print('Starting iteration %d' %(b))\n",
    "        loss = model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    torch.save(model.state_dict(), checkpoint_dir + '/epoch' + str(epoch) + '.pth')\n",
    "        \n",
    "    model.eval()\n",
    "    print('Computing test loss')\n",
    "    for b, batch in enumerate(test_dataloader):\n",
    "        test_losses[epoch] += model(batch).item()\n",
    "        \n",
    "    test_losses[epoch] /= len(test_dataloader)\n",
    "        \n",
    "    print('Computing train loss')\n",
    "    for b, batch in enumerate(train_dataloader):\n",
    "        train_losses[epoch] += model(batch).item()\n",
    "        \n",
    "    train_losses[epoch] /= len(train_dataloader)\n",
    "        \n",
    "    print('Train loss: %f, Test loss: %f' %(train_losses[epoch], test_losses[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(checkpoint_dir + '/train_losses.npy', np.array(train_losses))\n",
    "np.save(checkpoint_dir + '/test_losses.npy', np.array(test_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = np.load(checkpoint_dir + '/train_losses.npy', allow_pickle=True)\n",
    "test_losses = np.load(checkpoint_dir + '/test_losses.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "COLOR = 'white'\n",
    "plt.rcParams['text.color'] = COLOR\n",
    "plt.rcParams['axes.labelcolor'] = COLOR\n",
    "plt.rcParams['xtick.color'] = COLOR\n",
    "plt.rcParams['ytick.color'] = COLOR\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 100 # 200 e.g. is really fine, but slower\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Loss (Nats)')\n",
    "plt.title('Training Loss')\n",
    "plt.savefig(checkpoint_dir + '/vae_baseline_train_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Loss (Nats)')\n",
    "plt.title('Training Loss')\n",
    "plt.savefig(checkpoint_dir + '/vae_baseline_test_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BaselineVAE(message_dim, hidden_size, latent_size, section_size).to(dev)\n",
    "model.load_state_dict(torch.load('baseline_vae_checkpoints/epoch9.pth', map_location=dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from model\n",
    "def generate_music(model, gen_length=500):\n",
    "    z = model.normal_sampler.sample(sample_shape=(1, 1, latent_size)).squeeze(3)\n",
    "    nsections = int(np.ceil(gen_length/section_size))\n",
    "    return model.decoder.generate(z, nsections, model.embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_music = generate_music(model, gen_length=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('recording_vae.npy', np.array(generated_music))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reconstruction\n",
    "original = train_data[0].unsqueeze(1)\n",
    "reconstruction = model.reconstruct(original).flatten() # Should sound roughly the same as the original\n",
    "np.save('baseline_vae_midis/original10.npy', original.flatten().detach().numpy())\n",
    "np.save('baseline_vae_midis/reconstruction10.npy', reconstruction.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
