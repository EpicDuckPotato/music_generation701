{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_notes = 128\n",
    "num_time_shifts = 100\n",
    "num_velocities = 32\n",
    "message_dim = 2*num_notes + num_velocities + num_time_shifts\n",
    "instrument_numbers = [0, 6, 40, 41, 42, 43, 45, 60, 68, 70, 71, 73]\n",
    "num_instruments = len(instrument_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "Uses GRU per instrument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A recurrent model to generate MIDI messages for an ensemble of instruments\n",
    "# conditioned on the ensemble's message history\n",
    "class LinkedGRU(torch.nn.Module):\n",
    "    # CONSTRUCTOR\n",
    "    # ARGUMENTS\n",
    "    # message_dim: dimension of a MIDI message\n",
    "    # embed_dim: dimension of message embedding\n",
    "    # num_instruments: number of instrument labels\n",
    "    # hidden_size: size of hidden state for LSTM\n",
    "    # heads: number of attentions heads\n",
    "    # recurrent_layers: number of layers in recurrent unit\n",
    "    def __init__(self, message_dim, embed_dim, num_instruments, hidden_size, heads, recurrent_layers):\n",
    "        super(LinkedGRU, self).__init__()\n",
    "        \n",
    "        self.message_dim = message_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rlayers = recurrent_layers\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(message_dim, embed_dim)\n",
    "        \n",
    "        # We add the tanhed instrument embedding to each input message \n",
    "        # (this is the global conditioning idea from DeepJ, which comes from WaveNet)\n",
    "        self.i_embedding = torch.nn.Embedding(num_instruments, embed_dim)\n",
    "        \n",
    "        # Maybe compare with LSTM\n",
    "        self.gru = torch.nn.GRU(input_size=embed_dim + hidden_size, hidden_size=hidden_size, num_layers=recurrent_layers)\n",
    "        \n",
    "        # After each instrument's LSTM runs for a step, we compute attentions across the current hidden states\n",
    "        # from all the instruments, and concatenate the results with the next inputs\n",
    "        self.inst_attention = torch.nn.MultiheadAttention(hidden_size, heads)\n",
    "        \n",
    "        self.logits = torch.nn.Linear(hidden_size, message_dim)\n",
    "        \n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        \n",
    "    \n",
    "    # forward: computes the loss when trying to predict the next MIDI message for each message\n",
    "    # in history\n",
    "    # ARGUMENTS\n",
    "    # history: an LxNxB tensor, where L is the length of the longest history in\n",
    "    # the batch, N is the max number of instruments in the batch, and B is the batch size. As we\n",
    "    # walk along the first dimension, we should see indices of MIDI events for a particular\n",
    "    # instrument\n",
    "    # instruments: an 1xNxB tensor indicating the instrument for each sequence in each batch\n",
    "    # seq_lengths: an NxB tensor containing the sequence length for each instrument in each batch\n",
    "    # num_inst: a length B tensor indicating the number of instruments in each batch element\n",
    "    # RETURN: loss\n",
    "    def forward(self, history, instruments, seq_lengths, num_inst):\n",
    "        L = history.shape[0] # longest length\n",
    "        N = history.shape[1] # max instruments\n",
    "        B = history.shape[2] # batch size\n",
    "        assert(instruments.shape == (1, N, B))\n",
    "        assert(seq_lengths.shape == (N, B))\n",
    "        \n",
    "        inputs = self.embedding(history) + torch.tanh(self.i_embedding(instruments)).expand(L, -1, -1, -1)\n",
    "        \n",
    "        hidden_states = torch.zeros((N, self.rlayers, B, self.hidden_size))\n",
    "        key_padding_mask = torch.ones((B, N), dtype=torch.bool)\n",
    "        for b in range(B):\n",
    "            key_padding_mask[b, :num_inst[b]] = False\n",
    "        \n",
    "        loss = torch.tensor(0)\n",
    "        \n",
    "        # L - 1 because we don't have a target for the last message\n",
    "        for t in range(L - 1):\n",
    "            # Instrument-wise attention\n",
    "            attentions, weights = self.inst_attention(hidden_states[:, -1], \\\n",
    "                                                      hidden_states[:, -1], \\\n",
    "                                                      hidden_states[:, -1], \\\n",
    "                                                      key_padding_mask=key_padding_mask)\n",
    "            \n",
    "            new_hidden_states = hidden_states.clone()\n",
    "            \n",
    "            for inst in range(N):\n",
    "                # Only propagate hidden states for instruments whose sequence hasn't concluded\n",
    "                sel_idx = [b for b in range(B) if t < seq_lengths[inst, b] - 1]\n",
    "                \n",
    "                out, hid = self.gru(torch.cat((inputs[t, inst, sel_idx], attentions[inst, sel_idx]), dim=1).unsqueeze(0), \\\n",
    "                                    hidden_states[inst, :, sel_idx])\n",
    "                \n",
    "                new_hidden_states[inst, :, sel_idx] = hid\n",
    "                \n",
    "                logits = self.logits(new_hidden_states[inst, -1, sel_idx])\n",
    "                loss_batch = self.loss_fn(logits.view(-1, self.message_dim), \\\n",
    "                                          history[t + 1, inst, sel_idx].flatten())/seq_lengths[inst, sel_idx]\n",
    "                \n",
    "                loss = loss + loss_batch.sum()/B\n",
    "                \n",
    "            hidden_states = new_hidden_states\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    # forward_generate: computes the next MIDI messages and hidden states for each instrument\n",
    "    # ARGUMENTS\n",
    "    # messages: NxB tensor containing the previous message for each instrument\n",
    "    # instruments: NxB tensor indicating the instrument for each sequence in each batch\n",
    "    # mask: NxB tensor containing True where we actually want to generate a message and False otherwise\n",
    "    # num_inst: a length B list indicating the number of instruments in each batch element\n",
    "    # hidden_states: NxRxBxH tensor containing previous hidden states for each instrument (if None, we initialize to 0)\n",
    "    # RETURN: loss\n",
    "    def forward_generate(self, messages, instruments, mask, num_inst, hidden_states=None):\n",
    "        N = messages.shape[0]\n",
    "        B = messages.shape[1]\n",
    "        assert(instruments.shape == (N, B))\n",
    "        assert(mask.shape == (N, B))\n",
    "        assert(hidden_states is None or hidden_states.shape == (N, self.rlayers, B, self.hidden_size))\n",
    "        \n",
    "        inputs = self.embedding(messages) + torch.tanh(self.i_embedding(instruments))\n",
    "        \n",
    "        if hidden_states is None:\n",
    "            hidden_states = torch.zeros((N, self.rlayers, B, self.hidden_size))\n",
    "        \n",
    "        key_padding_mask = torch.ones((B, N), dtype=torch.bool)\n",
    "        for b in range(B):\n",
    "            key_padding_mask[b, :num_inst[b]] = False\n",
    "        \n",
    "        # Instrument-wise attention\n",
    "        attentions, weights = self.inst_attention(hidden_states[:, -1], \\\n",
    "                                                  hidden_states[:, -1], \\\n",
    "                                                  hidden_states[:, -1], \\\n",
    "                                                  key_padding_mask=key_padding_mask)\n",
    "        \n",
    "        new_hidden_states = hidden_states.clone()\n",
    "        new_messages = messages.clone()\n",
    "\n",
    "        for inst in range(N):\n",
    "            # Only propagate hidden states for instruments whose sequence hasn't concluded\n",
    "            sel_idx = [b for b in range(B) if mask[inst, b]]\n",
    "\n",
    "            out, hid = self.gru(torch.cat((inputs[inst, sel_idx], attentions[inst, sel_idx]), dim=1).unsqueeze(0), \\\n",
    "                                hidden_states[inst, :, sel_idx])\n",
    "            \n",
    "            new_hidden_states[inst, :, sel_idx] = hid\n",
    "\n",
    "            probs = torch.nn.functional.softmax(self.logits(new_hidden_states[inst, -1, sel_idx]), dim=1)\n",
    "            #new_messages[inst, sel_idx] = torch.multinomial(probs, 1)\n",
    "            new_messages[inst, sel_idx] = torch.argmax(probs, 1)\n",
    "            \n",
    "        return new_messages, new_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "We train with model.eval() to disable dropout, since these tests try to get the model to overfit to a small sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = 4\n",
    "embed_dim = 256\n",
    "hidden_size = 512\n",
    "recurrent_layers = 3\n",
    "\n",
    "grad_clip = 10\n",
    "\n",
    "model = LinkedGRU(message_dim, embed_dim, num_instruments, hidden_size, heads, recurrent_layers)\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "    \n",
    "model.eval() # Training with eval just to see if we can overfit without dropout\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model to overfit to a single instrument's part in a single song (only the first 100 time steps). Tests decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('debug_models/overfit1_gru.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Loss: 14.167624\n",
      "Starting epoch 100\n",
      "Loss: 1.282164\n",
      "Starting epoch 200\n",
      "Loss: 1.124701\n",
      "Starting epoch 300\n",
      "Loss: 1.207896\n",
      "Starting epoch 400\n",
      "Loss: 0.949695\n",
      "Starting epoch 500\n",
      "Loss: 1.398513\n"
     ]
    }
   ],
   "source": [
    "recording = np.load('preprocessed_data/recording318.npy', allow_pickle=True)\n",
    "instruments_np = np.load('preprocessed_data/instruments318.npy', allow_pickle=True)\n",
    "\n",
    "inst = 1\n",
    "\n",
    "max_length = 10\n",
    "\n",
    "history = torch.tensor(recording[inst][:max_length], dtype=torch.long).view(-1, 1, 1)\n",
    "instruments = torch.tensor(instrument_numbers.index(instruments_np[inst]), dtype=torch.long).view(1, 1, 1)\n",
    "seq_lengths = torch.tensor(max_length).view(1, 1)\n",
    "num_inst = [1]\n",
    "\n",
    "max_instruments = history.shape[1]\n",
    "\n",
    "batch_size = 1\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 501\n",
    "train_losses = np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    if epoch%100 == 0:\n",
    "        print('Starting epoch %d' %(epoch))\n",
    "    \n",
    "    loss = model(history, instruments, seq_lengths, num_inst)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses[epoch] = loss.data\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        print('Loss: %f' %(loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'debug_models/overfit1_gru.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fca56072710>]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from model\n",
    "gen_history = history.clone()\n",
    "model.eval() # Turns off the dropout for evaluation. Need to do this to get repeatable evaluation outputs\n",
    "\n",
    "mask = torch.ones((1, 1), dtype=torch.bool)\n",
    "\n",
    "hidden_states = None\n",
    "\n",
    "# Move forward in time\n",
    "wrong_cnt = 0\n",
    "for t in range(1, max_length):\n",
    "    #gen_history[t], hidden_states = model.forward_generate(gen_history[t - 1], instruments.squeeze(0), mask, num_inst, hidden_states)\n",
    "    #if gen_history[t] != history[t]:\n",
    "    #    wrong_cnt += 1\n",
    "    gen_message, hidden_states = model.forward_generate(history[t - 1], instruments.squeeze(0), mask, num_inst, hidden_states)\n",
    "    print(gen_message)\n",
    "    if gen_message != history[t]:\n",
    "        wrong_cnt += 1\n",
    "\n",
    "print(wrong_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model to overfit to a piece with two instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = np.load('preprocessed_data/recording318.npy', allow_pickle=True)\n",
    "instruments_np = np.load('preprocessed_data/instruments318.npy', allow_pickle=True)\n",
    "\n",
    "max_length = 10\n",
    "max_instruments = instruments_np.shape[0]\n",
    "batch_size = 1\n",
    "\n",
    "history = torch.zeros((max_length, max_instruments, 1), dtype=torch.long)\n",
    "for i in range(max_instruments):\n",
    "    history[:max_length, i, 0] = recording[i][:max_length]\n",
    "    \n",
    "instruments = torch.tensor([instrument_numbers.index(i) for i in instruments_np], dtype=torch.long).view(1, max_instruments, 1)\n",
    "seq_lengths = torch.tensor(max_length).view(1, 1)\n",
    "num_inst = [max_instruments]\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 501\n",
    "train_losses = np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    if epoch%100 == 0:\n",
    "        print('Starting epoch %d' %(epoch))\n",
    "    \n",
    "    loss = model(history, instruments, seq_lengths, num_inst)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses[epoch] = loss.data\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        print('Loss: %f' %(loss.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class MIDIDataset(torch.utils.data.Dataset):\n",
    "    # CONSTRUCTOR: creates a list of recording files and a list\n",
    "    # of instrument files in root_dir. Assumes that the directory\n",
    "    # contains recording0.npy to recordingM.npy,\n",
    "    # as well as instruments0.npy to instrumentsM.npy\n",
    "    # ARGUMENTS\n",
    "    # root_dir: the directory to search\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        files = os.listdir(root_dir)\n",
    "        self.recordings = []\n",
    "        self.instrument_files = []\n",
    "        for file in files:\n",
    "            if 'recording' in file:\n",
    "                self.recordings.append(os.path.join(root_dir, file))\n",
    "            elif 'instruments' in file:\n",
    "                self.instrument_files.append(os.path.join(root_dir, file))\n",
    "                \n",
    "        assert(len(self.recordings) == len(self.instrument_files))\n",
    "        self.recordings.sort()\n",
    "        self.instrument_files.sort()\n",
    "        self.transform = transform\n",
    "\n",
    "    # __len__\n",
    "    # RETURN: the number of recording files in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.recordings)\n",
    "\n",
    "    # __getitem__\n",
    "    # ARGUMENTS\n",
    "    # idx: indicates which file to get\n",
    "    # RETURN: an instance with keys 'instruments', 'history'\n",
    "    # instance['history'] is a numpy array of message sequences for each instrument\n",
    "    # instance['instruments'] a numpy array of instrument numbers\n",
    "    def __getitem__(self, idx):\n",
    "        instance = {'history': np.load(self.recordings[idx], allow_pickle=True), \\\n",
    "                    'instruments': np.load(self.instrument_files[idx], allow_pickle=True)}\n",
    "        \n",
    "        assert(len(instance['history']) == len(instance['instruments']))\n",
    "        \n",
    "        if self.transform:\n",
    "            instance = self.transform(instance)\n",
    "            \n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn: takes a list of samples from the dataset and turns them into a batch.\n",
    "# ARGUMENTS\n",
    "# batch: a list of dictionaries\n",
    "# RETURN: a sample with keys 'history', 'instruments', and 'mask'\n",
    "# sample['history']: an LxNxB tensor containing messages\n",
    "# sample['instruments']: a 1xNxB tensor containing instrument numbers\n",
    "# sample['seq_lengths']: an NxB tensor containing the length of each sequence\n",
    "# sample['num_inst']: a length B tensor containing the number of instruments in each batch\n",
    "def collate_fn(batch):\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    # We size our tensors to accomodate the longest sequence and the largest ensemble\n",
    "    max_instruments = max([len(instance['history']) for instance in batch])\n",
    "    max_len = max([max([seq.shape[0] for seq in instance['history']]) for instance in batch])\n",
    "\n",
    "    sample = {'history': torch.ones((max_len, max_instruments, batch_size), dtype=torch.long), \\\n",
    "              'instruments': torch.zeros((1, max_instruments, batch_size), dtype=torch.long), \\\n",
    "              'seq_lengths': torch.zeros((max_instruments, batch_size), dtype=torch.long), \\\n",
    "              'num_inst': torch.zeros(batch_size, dtype=torch.long)}\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        instrument_idx = [instrument_numbers.index(inst) for inst in batch[b]['instruments']]\n",
    "        sample['instruments'][0, :len(instrument_idx), b] = torch.tensor(instrument_idx, dtype=torch.long)\n",
    "        sample['num_inst'][b] = len(instrument_idx)\n",
    "        \n",
    "        for inst_idx in range(len(batch[b]['history'])):\n",
    "            seq_len = len(batch[b]['history'][inst_idx])\n",
    "            sample['history'][:seq_len, inst_idx, b] = torch.tensor(batch[b]['history'][inst_idx], dtype=torch.long)\n",
    "            sample['seq_lengths'][inst_idx, b] = seq_len\n",
    "            \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "heads = 4\n",
    "embed_dim = 256\n",
    "grad_clip = 10\n",
    "rlayers = 3\n",
    "\n",
    "model = LinkedGRU(message_dim, embed_dim, num_instruments, hidden_size, heads, rlayers)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Starting iteration 0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "dataset = MIDIDataset('preprocessed_data')\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "train_losses = np.zeros(epochs)\n",
    "test_losses = np.zeros(epochs) # TODO: train/test split\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    for b, batch in enumerate(dataloader):\n",
    "        print('Starting iteration %d' %(b))\n",
    "        loss = model(batch['history'], batch['instruments'], batch['seq_lengths'], batch['num_inst'])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss.data)\n",
    "    \n",
    "    torch.save(model.state_dict(), 'trained_models/epoch' + str(epoch) + '.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('trained_models/epoch4.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # Disable dropout to make results repeatable\n",
    "\n",
    "time_steps = 5000 # How many time steps do we sample?\n",
    "\n",
    "max_instruments = 3\n",
    "\n",
    "# Piano, violin, viola\n",
    "instruments = torch.tensor([0, 2, 3]).view(1, max_instruments, 1)\n",
    "\n",
    "# Suppose they all start with the same velocity message\n",
    "# TODO: should we have SOS and EOS tokens like in NLP?\n",
    "gen_history = 24*torch.ones((1, max_instruments, 1), dtype=torch.long)\n",
    "mask = torch.zeros((1, 1, 1), dtype=torch.bool)\n",
    "\n",
    "for t in range(1, time_steps):\n",
    "    # Sanity check\n",
    "    if t%100 == 0:\n",
    "        print(t)\n",
    "    next_messages = torch.zeros((1, max_instruments, 1), dtype=torch.long)\n",
    "    for inst in range(max_instruments):\n",
    "        logits = model(gen_history, mask.expand(t, max_instruments, -1), instruments, inst)\n",
    "        probs = torch.nn.functional.softmax(logits[t - 1], dim=1)\n",
    "        next_messages[0, inst, 0] = torch.multinomial(probs, 1)\n",
    "    \n",
    "    gen_history = torch.cat((gen_history, next_messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_recording = np.array([0 for i in range(max_instruments)], dtype='object')\n",
    "for i in range(max_instruments):\n",
    "    gen_recording[i] = gen_history[:, i].flatten().numpy()\n",
    "    \n",
    "gen_instruments = np.array([instrument_numbers[instruments[0, i, 0].item()] for i in range(max_instruments)])\n",
    "np.save('gen_recording.npy', gen_recording)\n",
    "np.save('gen_instruments.npy', gen_instruments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
