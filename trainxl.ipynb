{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from mem_transformer import MemTransformerLM\n",
    "from dgl.nn.pytorch import GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_notes = 128\n",
    "num_time_shifts = 100\n",
    "num_velocities = 32\n",
    "message_dim = 2*num_notes + num_velocities + num_time_shifts\n",
    "instrument_numbers = [0, 6, 40, 41, 42, 43, 45, 60, 68, 70, 71, 73]\n",
    "num_instruments = len(instrument_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "Uses Transformer XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EnsembleTransformer: takes a history of MIDI messages \n",
    "# for instruments in an ensemble and generates distributions for the next message\n",
    "class EnsembleTransformer(torch.nn.Module):\n",
    "    # CONSTRUCTOR\n",
    "    # ARGUMENTS\n",
    "    # message_dim: dimension of a MIDI message\n",
    "    # embed_dim: dimension of message embedding\n",
    "    # num_instruments: number of instrument labels\n",
    "    # heads: number of attention heads\n",
    "    # attention_layers: number of attention layers\n",
    "    # ff_size: size of the feedforward output at the end of the decoder\n",
    "    # chunk_size: we process sequences in chunks of this size\n",
    "    def __init__(self, message_dim, embed_dim, num_instruments, heads, attention_layers, ff_size, chunk_size):\n",
    "        super(EnsembleTransformer, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.chunk_size = chunk_size\n",
    "        \n",
    "        # We add the tanhed instrument embedding to each input message \n",
    "        # (this is the global conditioning idea from DeepJ, which comes from WaveNet)\n",
    "        self.i_embedding = torch.nn.Embedding(num_instruments, embed_dim)\n",
    "        \n",
    "        # A TransformerXL is used to transform a chunk of messages from an instrument, as\n",
    "        # well as a hidden state encoding the history of messages from all instruments,\n",
    "        # into a new set of hidden states and a distribution for the next message\n",
    "        # for the instrument\n",
    "        assert(embed_dim%heads == 0)\n",
    "        \n",
    "        # TODO: There are a bunch of parameters whose purpose I don't understand (e.g. tie_proj, cutoff).\n",
    "        # Should we use them?\n",
    "        \n",
    "        # TODO: should d_model and d_inner be different? Why do they set ext_len to 0 during training?\n",
    "        self.transformer = MemTransformerLM(message_dim, attention_layers, heads, \\\n",
    "                                            embed_dim, embed_dim // heads, embed_dim, 0.1, \\\n",
    "                                            dropatt=0, tie_weight=True, \\\n",
    "                                            d_embed=embed_dim, div_val=1, \\\n",
    "                                            pre_lnorm=True, \\\n",
    "                                            tgt_len=chunk_size, ext_len=0, mem_len=chunk_size, \\\n",
    "                                            attn_type=0)\n",
    "        \n",
    "        # Between chunks, we compute attentions across the current hidden states from all the instruments,\n",
    "        # producing new hidden states for each\n",
    "        # https://docs.dgl.ai/en/0.4.x/_modules/dgl/nn/pytorch/conv/gatconv.html\n",
    "        self.inst_attention = GATConv(embed_dim, embed_dim, heads)\n",
    "    \n",
    "    # forward: generates the next MIDI message\n",
    "    # for each instrument in an ensemble, given the ensemble history\n",
    "    # ARGUMENTS\n",
    "    # history: an LxNxB tensor, where L is the length of the longest history in\n",
    "    # the batch, N is the max number of instruments in the batch, and B is the batch size. As we\n",
    "    # walk along the first dimension, we should see indices of MIDI events for a particular\n",
    "    # instrument\n",
    "    # instruments: an NxB tensor indicating the instrument for each sequence in each batch\n",
    "    # seq_lengths: an NxB tensor containing the sequence length for each instrument in each batch\n",
    "    # mask: an LxNxB tensor containing True where messages actually exist and False in padding\n",
    "    # locations\n",
    "    # RETURN: loss, as well as an NxB tensor containing the predicted next message for each sequence\n",
    "    def forward(self, history, instruments, seq_lengths, mask):\n",
    "        L = history.shape[0] # longest length\n",
    "        N = history.shape[1] # max instruments\n",
    "        B = history.shape[2] # batch size\n",
    "        assert(instruments.shape == (N, B))\n",
    "        assert(seq_lengths.shape == instruments.shape)\n",
    "        assert(mask.shape == history.shape)\n",
    "        \n",
    "        mems = [[None for b in range(B)] for i in range(N)]\n",
    "        next_messages = torch.zeros((N, B), dtype=torch.long)\n",
    "        \n",
    "        # Target sequence is just the original sequence shifted left\n",
    "        tgt = torch.zeros((L, N, B), dtype=torch.long)\n",
    "        tgt[:-1] = history[1:]\n",
    "        \n",
    "        tgt_mask = torch.zeros((L, N, B), dtype=torch.bool)\n",
    "        tgt_mask[:-1] = mask[1:]\n",
    "        \n",
    "        loss = torch.tensor(0)\n",
    "        \n",
    "        for chunk_start in range(0, L, self.chunk_size):\n",
    "            chunk_end = min(i + self.chunk_size, L)\n",
    "            \n",
    "            for inst in range(N):\n",
    "                # Only propagate sequences that aren't finished\n",
    "                sel_idx = [b for b in range(B) if seq_lengths[inst, b] > chunk_start]\n",
    "                output = self.transformer(history[chunk_start:chunk_end, inst, sel_idx], \\\n",
    "                                          tgt[chunk_start:chunk_end, inst, sel_idx], \\\n",
    "                                          mems[inst]\n",
    "                                          inst)\n",
    "                \n",
    "                loss_mask = tgt_mask[chunk_start:chunk_end, inst, sel_idx]\n",
    "                loss = loss + output[0][loss_mask]\n",
    "                \n",
    "                mems[inst][sel_idx] = output[1:]\n",
    "                \n",
    "                # If we've reached the last chunk for an instrument, populate its\n",
    "                # predicted next message\n",
    "                done_idx = [b for b in range(B) if (seq_lengths[inst, b] > chunk_start and seq_lengths[inst, b] <= chunk_end)]\n",
    "                for d in done_idx:\n",
    "                    next_messages[inst, d] = #TODO: get logits from transformer\n",
    "                    \n",
    "            # Update memory for each sequence using self.inst_attention\n",
    "                \n",
    "        # Average loss over all targets\n",
    "        loss /= torch.sum(tgt_mask)\n",
    "\n",
    "        return loss, next_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "We train with model.eval() to disable dropout, since these tests try to get the model to overfit to a small sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model to overfit to a single instrument's part in a single song (only the first 100 time steps). Tests decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_size = 512\n",
    "heads = 8\n",
    "attention_layers=  6\n",
    "embed_dim = 256\n",
    "chunk_size = 100\n",
    "\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleTransformer(message_dim, embed_dim, num_instruments, heads, attention_layers, ff_size, chunk_size)\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "    \n",
    "model.eval() # Training with eval just to see if we can overfit without dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('overfit_single_instrument.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = np.load('preprocessed_data/recording318.npy', allow_pickle=True)\n",
    "instruments_np = np.load('preprocessed_data/instruments318.npy', allow_pickle=True)\n",
    "\n",
    "inst = 1\n",
    "\n",
    "max_seq_length = 800\n",
    "\n",
    "history = torch.tensor(recording[inst][:max_seq_length], dtype=torch.long).view(-1, 1, 1)\n",
    "mask = torch.zeros(history.shape, dtype=torch.bool)\n",
    "instruments = torch.tensor(instrument_numbers.index(instruments_np[inst]), dtype=torch.long).view(1, 1, 1)\n",
    "\n",
    "max_instruments = history.shape[1]\n",
    "\n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 150\n",
    "train_losses = np.zeros(epochs)\n",
    "\n",
    "target_messages = history[1:].flatten()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    # Index of the instrument we want to generate music for (there's only one instrument)\n",
    "    gen_idx = 0\n",
    "    \n",
    "    # Move forward in time\n",
    "    logits = model(history[:-1], mask[:-1], instruments, gen_idx).view(-1, message_dim)\n",
    "                \n",
    "    loss = loss_fn(logits, target_messages)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses[epoch] = loss.data\n",
    "    print('Loss: %f' %(loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'overfit_single_instrument2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9a84e66cd0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Sample from model\n",
    "gen_history = torch.tensor(recording[inst][0], dtype=torch.long).view(-1, 1, 1)\n",
    "mask = torch.zeros((1, 1, 1), dtype=torch.bool)\n",
    "instruments = torch.tensor(instrument_numbers.index(instruments_np[inst]), dtype=torch.long).view(1, 1, 1)\n",
    "model.eval() # Turns off the dropout for evaluation. Need to do this to get repeatable evaluation outputs\n",
    "\n",
    "seq_length = 100\n",
    "max_instruments = history.shape[1]\n",
    "\n",
    "# Index of the instrument we want to generate music for (there's only one instrument)\n",
    "gen_idx = 0\n",
    "\n",
    "# Move forward in time\n",
    "wrong_cnt = 0\n",
    "for t in range(1, seq_length):\n",
    "    logits = model(gen_history, mask, instruments, gen_idx)\n",
    "    probs = torch.nn.functional.softmax(logits[t - 1], dim=1)\n",
    "    gen_history = torch.cat((gen_history, torch.multinomial(probs, 1).view(1, 1, -1)))\n",
    "    #gen_history = torch.cat((gen_history, torch.argmax(probs.flatten()).view(1, 1, 1)))\n",
    "    if torch.argmax(probs.flatten()) != history[t].flatten():\n",
    "        wrong_cnt += 1\n",
    "        print(torch.topk(probs.flatten(), 10))\n",
    "        print(history[t])\n",
    "    \n",
    "    mask = torch.cat((mask, torch.zeros((1, 1, 1), dtype=torch.bool)))\n",
    "\n",
    "print(wrong_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test_history.npy', np.array([history.flatten().numpy()], dtype='object'))\n",
    "np.save('test_instruments.npy', np.array([instrument_numbers[instruments[0]]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model to overfit to two instruments' parts in a single song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_size = 512\n",
    "heads = 8\n",
    "attention_layers=  6\n",
    "embed_dim = 256\n",
    "\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleTransformer(message_dim, embed_dim, num_instruments, heads, attention_layers, ff_size)\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "    \n",
    "model.eval() # Training with eval just to see if we can overfit without dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('overfit_two_instruments.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = np.load('preprocessed_data/recording318.npy', allow_pickle=True)\n",
    "instruments_np = np.load('preprocessed_data/instruments318.npy', allow_pickle=True)\n",
    "\n",
    "max_seq_length = 1000\n",
    "max_instruments = instruments_np.shape[0]\n",
    "batch_size = 1\n",
    "\n",
    "history = torch.zeros((max_seq_length, max_instruments, batch_size), dtype=torch.long)\n",
    "mask = torch.ones(history.shape, dtype=torch.bool)\n",
    "instruments = torch.tensor([instrument_numbers.index(instruments_np[i]) for i in range(max_instruments)], dtype=torch.long).view(1, -1, 1)\n",
    "\n",
    "for inst in range(max_instruments):\n",
    "    history[:max_seq_length, inst, 0] = torch.tensor(recording[inst][:max_seq_length], dtype=torch.long)\n",
    "    mask[:max_seq_length, inst, 0] = False\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 300\n",
    "train_losses = np.zeros(epochs)\n",
    "num_targets = max_seq_length - 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    \n",
    "    loss = torch.tensor([0], dtype=torch.float32)\n",
    "    for inst in range(max_instruments):\n",
    "        logits = model(history[:-1], mask[:-1], instruments, inst)\n",
    "\n",
    "        logits = logits.view(-1, message_dim)\n",
    "        target_messages = history[1:, inst].flatten()\n",
    "        output_mask = torch.logical_not(mask[1:, inst].flatten())\n",
    "        loss = loss + loss_fn(logits[output_mask], target_messages[output_mask])\n",
    "                \n",
    "    loss = loss/max_instruments\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    train_losses[epoch] = loss.data\n",
    "    print('Loss = %f' %(loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'overfit_two_instruments.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9a84317290>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check if each instrument can reconstruct its part, given the other instrument's part\n",
    "model.eval() # Turns off the dropout for evaluation. Need to do this to get repeatable evaluation outputs\n",
    "\n",
    "for inst in range(max_instruments):\n",
    "    gen_history = history.clone()\n",
    "\n",
    "    # Move forward in time\n",
    "    wrong_cnt = 0\n",
    "    for t in range(1, max_seq_length):\n",
    "        input_mask = mask.clone()\n",
    "        input_mask[t:] = True\n",
    "        logits = model(gen_history, input_mask, instruments, inst)\n",
    "        probs = torch.nn.functional.softmax(logits[t - 1], dim=1)\n",
    "        gen_history[t, inst, 0] = torch.multinomial(probs, 1)\n",
    "        if torch.argmax(probs.flatten()) != history[t, inst].flatten():\n",
    "            wrong_cnt += 1\n",
    "            print(torch.topk(probs.flatten(), 10))\n",
    "            print(history[t, inst])\n",
    "\n",
    "    print(wrong_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = np.load('preprocessed_data/recording318.npy', allow_pickle=True)\n",
    "instruments_np = np.load('preprocessed_data/instruments318.npy', allow_pickle=True)\n",
    "\n",
    "max_seq_length = 1000\n",
    "max_instruments = instruments_np.shape[0]\n",
    "batch_size = 1\n",
    "\n",
    "history = torch.zeros((max_seq_length, max_instruments, batch_size), dtype=torch.long)\n",
    "mask = torch.ones(history.shape, dtype=torch.bool)\n",
    "instruments = torch.tensor([instrument_numbers.index(instruments_np[i]) for i in range(max_instruments)], dtype=torch.long).view(1, -1, 1)\n",
    "\n",
    "for inst in range(max_instruments):\n",
    "    history[:max_seq_length, inst, 0] = torch.tensor(recording[inst][:max_seq_length], dtype=torch.long)\n",
    "    mask[:max_seq_length, inst, 0] = False\n",
    "\n",
    "# Check if the instruments can jointly reconstruct the piece\n",
    "gen_history = history.clone()\n",
    "\n",
    "model.eval() # Turns off the dropout for evaluation. Need to do this to get repeatable evaluation outputs\n",
    "\n",
    "wrong_cnt = 0\n",
    "\n",
    "for t in range(1, max_seq_length):\n",
    "    input_mask = mask.clone()\n",
    "    input_mask[t:] = True\n",
    "    for inst in range(max_instruments):\n",
    "        logits = model(gen_history, input_mask, instruments, inst)\n",
    "        probs = torch.nn.functional.softmax(logits[t - 1], dim=1)\n",
    "        gen_history[t, inst, 0] = torch.multinomial(probs, 1)\n",
    "        if torch.argmax(probs.flatten()) != history[t, inst].flatten():\n",
    "            wrong_cnt += 1\n",
    "           \n",
    "    if t%50 == 0:\n",
    "        print(t)\n",
    "\n",
    "print(wrong_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class MIDIDataset(torch.utils.data.Dataset):\n",
    "    # CONSTRUCTOR: creates a list of recording files and a list\n",
    "    # of instrument files in root_dir. Assumes that the directory\n",
    "    # contains recording0.npy to recordingM.npy,\n",
    "    # as well as instruments0.npy to instrumentsM.npy\n",
    "    # ARGUMENTS\n",
    "    # root_dir: the directory to search\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        files = os.listdir(root_dir)\n",
    "        self.recordings = []\n",
    "        self.instrument_files = []\n",
    "        for file in files:\n",
    "            if 'recording' in file:\n",
    "                self.recordings.append(os.path.join(root_dir, file))\n",
    "            elif 'instruments' in file:\n",
    "                self.instrument_files.append(os.path.join(root_dir, file))\n",
    "                \n",
    "        assert(len(self.recordings) == len(self.instrument_files))\n",
    "        self.recordings.sort()\n",
    "        self.instrument_files.sort()\n",
    "        self.transform = transform\n",
    "\n",
    "    # __len__\n",
    "    # RETURN: the number of recording files in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.recordings)\n",
    "\n",
    "    # __getitem__\n",
    "    # ARGUMENTS\n",
    "    # idx: indicates which file to get\n",
    "    # RETURN: an instance with keys 'instruments', 'history'\n",
    "    # instance['history'] is a numpy array of message sequences for each instrument\n",
    "    # instance['instruments'] a numpy array of instrument numbers\n",
    "    def __getitem__(self, idx):\n",
    "        instance = {'history': np.load(self.recordings[idx], allow_pickle=True), \\\n",
    "                    'instruments': np.load(self.instrument_files[idx], allow_pickle=True)}\n",
    "        \n",
    "        assert(len(instance['history']) == len(instance['instruments']))\n",
    "        \n",
    "        if self.transform:\n",
    "            instance = self.transform(instance)\n",
    "            \n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn: takes a list of samples from the dataset and turns them into a batch.\n",
    "# ARGUMENTS\n",
    "# batch: a list of dictionaries\n",
    "# RETURN: a sample with keys 'history', 'instruments', and 'mask'\n",
    "# sample['history']: an LxNxB tensor containing messages\n",
    "# sample['instruments']: a 1xNxB tensor containing instrument numbers\n",
    "# sample['mask']: an LxNxB tensor containing False where a message is\n",
    "# valid, and True where it isn't (accounts for variable length sequences\n",
    "# and zero padding)\n",
    "def collate_fn(batch):\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    # We size our tensors to accomodate the longest sequence and the largest ensemble\n",
    "    max_instruments = max([len(instance['history']) for instance in batch])\n",
    "    max_len = max([max([seq.shape[0] for seq in instance['history']]) for instance in batch])\n",
    "\n",
    "    sample = {'history': torch.ones((max_len, max_instruments, batch_size), dtype=torch.long), \\\n",
    "              'instruments': torch.zeros((1, max_instruments, batch_size), dtype=torch.long), \\\n",
    "              'mask': torch.ones((max_len, max_instruments, batch_size), dtype=torch.bool)}\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        instrument_idx = [instrument_numbers.index(inst) for inst in batch[b]['instruments']]\n",
    "        sample['instruments'][0, :len(instrument_idx), b] = torch.tensor(instrument_idx, dtype=torch.long)\n",
    "        \n",
    "        for inst_idx in range(len(batch[b]['history'])):\n",
    "            seq_len = len(batch[b]['history'][inst_idx])\n",
    "            sample['history'][:seq_len, inst_idx, b] = torch.tensor(batch[b]['history'][inst_idx], dtype=torch.long)\n",
    "            sample['mask'][:seq_len, inst_idx, b] = False\n",
    "            \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_size = 512\n",
    "heads = 8\n",
    "attention_layers=  6\n",
    "embed_dim = 256\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleTransformer(message_dim, embed_dim, len(instrument_numbers), heads, attention_layers, ff_size)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "dataset = MIDIDataset('preprocessed_data')\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "train_losses = np.zeros(epochs)\n",
    "test_losses = np.zeros(epochs) # TODO: train/test split. Can we do this with Dataloader?\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    for b, batch in enumerate(dataloader):\n",
    "        print('Starting iteration %d' %(b))\n",
    "        max_seq_length = batch['history'].shape[0]\n",
    "        num_targets = max_seq_length - 1 # Messages start from t = 0, but we start generating at t = 1\n",
    "        max_instruments = batch['history'].shape[1]\n",
    "        loss = torch.tensor([0])\n",
    "        for inst in range(max_instruments):         \n",
    "            mask = batch['mask']\n",
    "            \n",
    "            logits = model(batch['history'][:-1], mask[:-1], batch['instruments'], inst)\n",
    "            logits = logits.view(-1, message_dim)\n",
    "            target_messages = batch['history'][1:, inst].flatten()\n",
    "            output_mask = torch.logical_not(mask[1:, inst].flatten())\n",
    "            loss = loss + loss_fn(logits[output_mask], target_messages[output_mask])\n",
    "        \n",
    "        loss /= max_instruments\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.save(model.state_dict(), 'trained_models/epoch' + str(epoch) + '.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('trained_models/epoch4.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # Disable dropout to make results repeatable\n",
    "\n",
    "time_steps = 5000 # How many time steps do we sample?\n",
    "\n",
    "max_instruments = 3\n",
    "\n",
    "# Piano, violin, viola\n",
    "instruments = torch.tensor([0, 2, 3]).view(1, max_instruments, 1)\n",
    "\n",
    "# Suppose they all start with the same velocity message\n",
    "# TODO: should we have SOS and EOS tokens like in NLP?\n",
    "gen_history = 24*torch.ones((1, max_instruments, 1), dtype=torch.long)\n",
    "mask = torch.zeros((1, 1, 1), dtype=torch.bool)\n",
    "\n",
    "for t in range(1, time_steps):\n",
    "    # Sanity check\n",
    "    if t%100 == 0:\n",
    "        print(t)\n",
    "    next_messages = torch.zeros((1, max_instruments, 1), dtype=torch.long)\n",
    "    for inst in range(max_instruments):\n",
    "        logits = model(gen_history, mask.expand(t, max_instruments, -1), instruments, inst)\n",
    "        probs = torch.nn.functional.softmax(logits[t - 1], dim=1)\n",
    "        next_messages[0, inst, 0] = torch.multinomial(probs, 1)\n",
    "    \n",
    "    gen_history = torch.cat((gen_history, next_messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_recording = np.array([0 for i in range(max_instruments)], dtype='object')\n",
    "for i in range(max_instruments):\n",
    "    gen_recording[i] = gen_history[:, i].flatten().numpy()\n",
    "    \n",
    "gen_instruments = np.array([instrument_numbers[instruments[0, i, 0].item()] for i in range(max_instruments)])\n",
    "np.save('gen_recording.npy', gen_recording)\n",
    "np.save('gen_instruments.npy', gen_instruments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
