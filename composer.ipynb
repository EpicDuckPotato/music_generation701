{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_notes = 128\n",
    "num_time_shifts = 100\n",
    "num_velocities = 32\n",
    "message_dim = 2*num_notes + num_velocities + num_time_shifts\n",
    "instrument_numbers = [0, 6, 40, 41, 42, 43, 45, 60, 68, 70, 71, 73]\n",
    "num_instruments = len(instrument_numbers)\n",
    "max_channels = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ComposerLSTM definition\n",
    "\n",
    "Predicts the next message given the message history and available instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ComposerLSTM: takes a history of MIDI messages \n",
    "# and generates a distribution for the next message\n",
    "class ComposerLSTM(torch.nn.Module):\n",
    "    # CONSTRUCTOR\n",
    "    # ARGUMENTS\n",
    "    # message_dim: dimension of a MIDI message\n",
    "    # embed_dim: dimension of message embedding\n",
    "    # num_instruments: number of instrument labels\n",
    "    # hidden_size: size of hidden LSTM state\n",
    "    # heads: number of heads used to compute ensemble vector\n",
    "    # recurrent_layers: the number of layers in the lstm\n",
    "    def __init__(self, message_dim, embed_dim, num_instruments, hidden_size, heads, recurrent_layers=3):\n",
    "        super(ComposerLSTM, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # We use multiheaded attention to transform the ensemble into a single vector\n",
    "        self.i_embedding = torch.nn.Embedding(num_instruments, embed_dim)\n",
    "        self.inst_attention = torch.nn.MultiheadAttention(embed_dim, heads)\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(message_dim, embed_dim)\n",
    "        \n",
    "        # A 3-layer LSTM takes the history of messages (concatenated with the ensemble\n",
    "        # vector) and produces a decoding\n",
    "        self.lstm = torch.nn.LSTM(2*embed_dim, hidden_size, num_layers=recurrent_layers)\n",
    "\n",
    "        # The decoding is passed through a linear layer to get the logits for the next message        \n",
    "        self.logits = torch.nn.Linear(hidden_size, message_dim)\n",
    "    \n",
    "    # forward: generates a probability distribution for the next MIDI message\n",
    "    # ARGUMENTS\n",
    "    # history: an LxB tensor, where L is the length of the longest message history in\n",
    "    # the batch, and B is the batch size. This should be END-PADDED\n",
    "    # along dimension 0\n",
    "    # mask: an LxB tensor, containing True in any locations where history contains padding\n",
    "    # instruments: a NxB tensor indicating the instruments in each channel. This should be END-PADDED along dimension 0\n",
    "    # inst_mask: an NxB tensor containing False where an instrument exists and True otherwise\n",
    "    # RETURN: an LxBxD tensor representing the logits for the next message in each batch\n",
    "    def forward(self, history, mask, instruments, inst_mask):\n",
    "        L = history.shape[0] # longest length\n",
    "        B = history.shape[1] # batch size\n",
    "        assert(mask.shape == history.shape)\n",
    "        assert(instruments.shape == inst_mask.shape)\n",
    "        \n",
    "        # NxBxD\n",
    "        inst_embed = torch.tanh(self.i_embedding(instruments))\n",
    "               \n",
    "        # 1xBxD\n",
    "        ensemble_vec, weights = self.inst_attention(torch.ones((1, B, self.embed_dim)), \\\n",
    "                                                    inst_embed, \\\n",
    "                                                    inst_embed, \\\n",
    "                                                    inst_mask.transpose(0, 1))\n",
    "        \n",
    "        # LxBxD\n",
    "        inputs = torch.cat((self.embedding(history), ensemble_vec.expand(L, -1, -1)), dim=2)\n",
    "        \n",
    "        decoding, last_hidden = self.lstm(inputs)\n",
    "        \n",
    "        # LxBxD\n",
    "        return self.logits(decoding)\n",
    "    \n",
    "    # forward: predicts the next MIDI message\n",
    "    # ARGUMENTS\n",
    "    # last_message: a 1x1 tensor containing the last message\n",
    "    # instruments: an Nx1 tensor indicating the instrument number for each channel\n",
    "    # hidden: the last hidden state for the LSTM\n",
    "    # RETURN: a 1x1 tensor, predicting the next message\n",
    "    def forward_generate(self, last_message, instruments, hidden):\n",
    "        assert(last_message.shape == (1, 1))\n",
    "        \n",
    "        # Nx1xD\n",
    "        inst_embed = torch.tanh(self.i_embedding(instruments))\n",
    "               \n",
    "        # 1x1xD\n",
    "        ensemble_vec, weights = self.inst_attention(torch.ones((1, 1, self.embed_dim)), inst_embed, inst_embed)\n",
    "        \n",
    "        # 1x1xD\n",
    "        inputs = torch.cat((self.embedding(last_message), ensemble_vec), dim=2)\n",
    "        \n",
    "        decoding, new_hidden = self.lstm(inputs, hidden)\n",
    "        \n",
    "        # 1x1xD\n",
    "        probs = torch.nn.functional.softmax(self.logits(decoding), dim=2)\n",
    "               \n",
    "        return torch.multinomial(probs.flatten(), 1).view(1, 1), new_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests for ComposerLSTM\n",
    "We train with model.eval() to disable dropout, since these tests try to get the model to overfit to a small sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model to overfit to a single song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "hidden_size = 1024\n",
    "heads = 4\n",
    "\n",
    "grad_clip = 10\n",
    "\n",
    "model = ComposerLSTM(message_dim, embed_dim, num_instruments, hidden_size, heads)\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "    \n",
    "model.eval() # Training with eval just to see if we can overfit without dropout\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('overfit_composer.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recording = np.load('train_unified/recording0.npy', allow_pickle=True)\n",
    "instruments_np = np.load('train_unified/instruments0.npy', allow_pickle=True)\n",
    "\n",
    "nsamples = 200\n",
    "\n",
    "message_history = torch.tensor(recording[:nsamples, 0], dtype=torch.long).view(-1, 1)\n",
    "mask = torch.zeros(message_history.shape, dtype=torch.bool)\n",
    "instruments = torch.tensor([instrument_numbers.index(i) for i in instruments_np], dtype=torch.long).view(-1, 1)\n",
    "inst_mask = torch.zeros(instruments.shape, dtype=torch.bool)\n",
    "\n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 500\n",
    "train_losses = np.zeros(epochs)\n",
    "\n",
    "target_messages = message_history[1:].flatten()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    \n",
    "    message_logits = model(message_history[:-1], mask[:-1], instruments, inst_mask)\n",
    "    \n",
    "    loss = loss_fn(message_logits.view(-1, message_dim), target_messages)\n",
    "                \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses[epoch] = loss.data\n",
    "    print('Loss: %f' %(loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'overfit_composer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3e24440b50>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Sample from model\n",
    "gen_messages = message_history[0].unsqueeze(0)\n",
    "model.eval() # Turns off the dropout for evaluation. Need to do this to get repeatable evaluation outputs\n",
    "\n",
    "# Move forward in time\n",
    "wrong_cnt = 0\n",
    "hidden = (torch.zeros(3, 1, hidden_size), torch.zeros(3, 1, hidden_size))\n",
    "for t in range(0, message_history.shape[0] - 1):\n",
    "    ret, hidden = model.forward_generate(gen_messages[-1].view(1, 1), instruments, hidden)\n",
    "    \n",
    "    gen_messages = torch.cat((gen_messages, ret), dim=0)\n",
    "    \n",
    "    if gen_messages[-1, 0] != message_history[t + 1, 0]:\n",
    "        print('Wrong message at time %d!' %(t))\n",
    "        wrong_cnt += 1\n",
    "\n",
    "print(wrong_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate all messages with channel 0 for now\n",
    "channels = torch.zeros(gen_messages.shape, dtype=torch.long)\n",
    "gen_history = torch.cat((gen_messages, channels), dim=1)\n",
    "np.save('test_history.npy', gen_history.detach().numpy())\n",
    "np.save('test_instruments.npy', np.array([0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class MIDIDataset(torch.utils.data.Dataset):\n",
    "    # CONSTRUCTOR: creates a tensor of message chunks and associated instruments.\n",
    "    # Assumes that the directory contains recording0.npy to recordingM.npy\n",
    "    # as well as instruments0.npy to instrumentsM.npy\n",
    "    # ARGUMENTS\n",
    "    # root_dir: the directory to search\n",
    "    # chunk_size: we'll chunk the data into chunks of this size (or less)\n",
    "    # max_channels: what's the largest number of instruments in any file?\n",
    "    def __init__(self, root_dir, chunk_size, max_channels, transform=None):\n",
    "        files = os.listdir(root_dir)\n",
    "        recording_files = []\n",
    "        instrument_files = []\n",
    "        for file in files:\n",
    "            if 'recording' in file:\n",
    "                recording_files.append(os.path.join(root_dir, file))\n",
    "            elif 'instruments' in file:\n",
    "                instrument_files.append(os.path.join(root_dir, file))\n",
    "                \n",
    "        assert(len(recording_files) == len(instrument_files))\n",
    "        recording_files.sort()\n",
    "        instrument_files.sort()\n",
    "        \n",
    "        self.chunks = []\n",
    "        self.masks = []\n",
    "        self.instruments = []\n",
    "        self.inst_masks = []\n",
    "        \n",
    "        ch = 0\n",
    "        for f in range(len(recording_files)):\n",
    "            recording = np.load(recording_files[f], allow_pickle=True)\n",
    "            inst = [instrument_numbers.index(i) for i in np.load(instrument_files[f], allow_pickle=True)]\n",
    "            \n",
    "            nchunks = int(np.ceil(recording.shape[0]/chunk_size))\n",
    "            self.chunks += [torch.zeros(chunk_size, dtype=torch.long) for c in range(nchunks)]\n",
    "            self.masks += [torch.ones(chunk_size, dtype=torch.bool) for c in range(nchunks)]\n",
    "            self.instruments += [torch.zeros(max_channels, dtype=torch.long) for c in range(nchunks)]\n",
    "            self.inst_masks += [torch.ones(max_channels, dtype=torch.long) for c in range(nchunks)]\n",
    "            for chunk_start in range(0, recording.shape[0], chunk_size):\n",
    "                chunk_end = min(chunk_start + chunk_size, recording.shape[0])\n",
    "                size = chunk_end - chunk_start\n",
    "                self.chunks[ch][:size] = torch.tensor(recording[chunk_start:chunk_end, 0], dtype=torch.long)\n",
    "                self.masks[ch][:size] = False\n",
    "                self.instruments[ch][:len(inst)] = torch.tensor(inst, dtype=torch.long)\n",
    "                self.inst_masks[ch][:len(inst)] = False\n",
    "                ch += 1\n",
    "            \n",
    "        self.transform = transform\n",
    "\n",
    "    # __len__\n",
    "    # RETURN: the number of recording chunks in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "\n",
    "    # __getitem__\n",
    "    # ARGUMENTS\n",
    "    # idx: indicates which chunk(s) to get\n",
    "    # RETURN: instance, a dictionary with keys 'history', 'instruments', 'mask', and 'inst_mask'\n",
    "    # instance['history'] is a length L tensor containing messages\n",
    "    # instance['instruments'] a length N tensor of instrument numbers\n",
    "    # instance['mask'] a length L tensor containing False where messages exist and True otherwise\n",
    "    # instance['inst_mask'] a length N tensor containing False where instruments exist and True otherwise\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        instance = {'history': self.chunks[idx], \\\n",
    "                    'instruments': self.instruments[idx],\n",
    "                    'mask': self.masks[idx],\n",
    "                    'inst_mask': self.inst_masks[idx]}\n",
    "        \n",
    "        if self.transform:\n",
    "            instance = self.transform(instance)\n",
    "            \n",
    "        return instance\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    chunk_size = batch[0]['history'].shape[0]\n",
    "    max_channels = batch[0]['instruments'].shape[0]\n",
    "    sample = {'history': torch.zeros((chunk_size, len(batch)), dtype=torch.long), \\\n",
    "              'instruments': torch.ones((max_channels, len(batch)), dtype=torch.long), \\\n",
    "              'mask': torch.ones((chunk_size, len(batch)), dtype=torch.bool),\n",
    "              'inst_mask': torch.ones((max_channels, len(batch)), dtype=torch.bool)}\n",
    "    \n",
    "    for b, instance in enumerate(batch):\n",
    "        sample['history'][:, b] = instance['history']\n",
    "        sample['instruments'][:, b] = instance['instruments']\n",
    "        sample['mask'][:, b] = instance['mask']\n",
    "        sample['inst_mask'][:, b] = instance['inst_mask']\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_loss: computes the loss for the model over the batch\n",
    "# ARGUMENTS\n",
    "# model: ComposerLSTM model\n",
    "# loss_fn: torch.nn.CrossEntropyLoss object\n",
    "# batch: see collate_fn definition\n",
    "# RETURN: a scalar loss tensor\n",
    "def compute_loss(model, loss_fn, batch):\n",
    "    batch_size = batch['history'].shape[1]\n",
    "    \n",
    "    max_seq_length = batch['history'].shape[0]\n",
    "        \n",
    "    num_targets = max_seq_length - 1 # Messages start from t = 0, but we start generating at t = 1\n",
    "\n",
    "    message_logits = model(batch['history'][:-1], batch['mask'][:-1], batch['instruments'], batch['inst_mask'])\n",
    "\n",
    "    target_mask = torch.logical_not(batch['mask'][1:])\n",
    "\n",
    "    target_messages = batch['history'][1:][target_mask]\n",
    "\n",
    "    message_loss = loss_fn(message_logits[target_mask], target_messages)\n",
    "\n",
    "    return message_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "hidden_size = 1024\n",
    "heads = 4\n",
    "\n",
    "grad_clip = 10\n",
    "\n",
    "model = ComposerLSTM(message_dim, embed_dim, num_instruments, hidden_size, heads)\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "chunk_size = 500\n",
    "\n",
    "train_dataset = MIDIDataset('train_unified', chunk_size, max_channels)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "test_dataset = MIDIDataset('test_unified', chunk_size, max_channels)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 20\n",
    "train_losses = np.zeros(epochs)\n",
    "test_losses = np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    model.train()\n",
    "    for b, batch in enumerate(train_dataloader):\n",
    "        print('Starting iteration %d' %(b))\n",
    "        loss = compute_loss(model, loss_fn, batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    torch.save(model.state_dict(), 'composer_models/epoch' + str(epoch) + '.pth')\n",
    "\n",
    "    print('Computing test loss')\n",
    "    model.eval()\n",
    "    for batch in test_dataloader:\n",
    "        loss = compute_loss(model, loss_fn, batch)\n",
    "        test_losses[epoch] += loss.data\n",
    "        \n",
    "    print('Computing train loss')\n",
    "    for batch in train_dataloader:\n",
    "        loss = compute_loss(model, loss_fn, batch)\n",
    "        train_losses[epoch] += loss.data\n",
    "    \n",
    "    train_losses[epoch] /= len(train_dataloader)\n",
    "    test_losses[epoch] /= len(test_dataloader)\n",
    "    print('Train Loss: %f, Test Loss: %f' %(train_losses[epoch], test_losses[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # Disable dropout to make results repeatable\n",
    "\n",
    "time_steps = 500 # How many time steps do we sample?\n",
    "\n",
    "# Start with a time shift\n",
    "gen_messages = torch.tensor(387).view(1, 1)\n",
    "model.eval() # Turns off the dropout for evaluation. Need to do this to get repeatable evaluation outputs\n",
    "\n",
    "# Piano\n",
    "instruments = torch.zeros((1, 1), dtype=torch.long)\n",
    "instruments[0, 0] = 0\n",
    "\n",
    "# Move forward in time\n",
    "wrong_cnt = 0\n",
    "hidden = (torch.zeros(3, 1, hidden_size), torch.zeros(3, 1, hidden_size))\n",
    "for t in range(time_steps):\n",
    "    ret, hidden = model.forward_generate(gen_messages[-1].view(1, 1), instruments, hidden)\n",
    "    \n",
    "    gen_messages = torch.cat((gen_messages, ret), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate all messages with channel 0 for now\n",
    "channels = torch.zeros(gen_messages.shape, dtype=torch.long)\n",
    "gen_history = torch.cat((gen_messages, channels), dim=1)\n",
    "np.save('test_history.npy', gen_history.detach().numpy())\n",
    "np.save('test_instruments.npy', np.array([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
