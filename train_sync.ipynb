{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_notes = 128\n",
    "action_dim = 3*num_notes\n",
    "instrument_numbers = [0, 6, 40, 41, 42, 43, 45, 60, 68, 70, 71, 73]\n",
    "num_instruments = len(instrument_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Transformer definition\n",
    "Uses absolute position representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://pytorch.org/tutorials/beginner/transformer_tutorial.html.\n",
    "# Only change is the view/expand in forward (accounts for multiple instruments and batches)\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.shape[1], :].view(1, x.shape[1], 1, -1).expand(x.shape[0], -1, x.shape[2], -1)\n",
    "        return self.dropout(x)\n",
    "        \n",
    "\n",
    "# EnsembleTransformer: takes a history of actions \n",
    "# for instruments in an ensemble and generates distributions for the next actions\n",
    "class EnsembleTransformer(torch.nn.Module):\n",
    "    # CONSTRUCTOR\n",
    "    # ARGUMENTS\n",
    "    # action_dim: dimension of a MIDI message\n",
    "    # embed_dim: dimension of message embedding\n",
    "    # num_instruments: number of instrument labels\n",
    "    # heads: number of attention heads\n",
    "    # attention_layers: number of attention layers\n",
    "    # ff_size: size of the feedforward output at the end of the decoder\n",
    "    # chunk_size: long sequences will be processed in chunks of chunk_size\n",
    "    def __init__(self, action_dim, embed_dim, num_instruments, heads, attention_layers, ff_size, chunk_size):\n",
    "        super(EnsembleTransformer, self).__init__()\n",
    "        \n",
    "        self.action_dim = action_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.chunk_size = chunk_size\n",
    "        \n",
    "        # We add the tanhed instrument embedding to each input message \n",
    "        # (this is the global conditioning idea from DeepJ, which comes from WaveNet)\n",
    "        self.i_embedding = torch.nn.Embedding(num_instruments, embed_dim)\n",
    "        \n",
    "        self.position_encoding = PositionalEncoding(embed_dim)\n",
    "        \n",
    "        self.embedding = torch.nn.Linear(action_dim, embed_dim)\n",
    "        \n",
    "        # A decoder is used to transform the history of the instrument we're generating\n",
    "        # music for, then combine this with the other histories to generate the next action\n",
    "        decoder_layer = torch.nn.TransformerDecoderLayer(embed_dim, heads, ff_size)\n",
    "        self.decoder = torch.nn.TransformerDecoder(decoder_layer, attention_layers)\n",
    "        \n",
    "        self.logits = torch.nn.Linear(embed_dim, action_dim)\n",
    "        \n",
    "    \n",
    "    # forward: generates probabilities for the next MIDI actions\n",
    "    # given the action history of the ensemble\n",
    "    # ARGUMENTS\n",
    "    # history: an NxLxBxA tensor, where N is the max number of instruments in the batch,\n",
    "    # L is the length of the longest history in the batch, B is the batch size,\n",
    "    # and A is the number of actions\n",
    "    # mask: an NxLxB tensor, containing True where an action or instrument doesn't exist\n",
    "    # instruments: a NxB tensor indicating the instrument numbers for each batch\n",
    "    # RETURN: an NxLxBxA tensor out. out[:, :, :, :2*num_notes] represents the Bernoulli probabilities\n",
    "    # for note-on and note-off actions. out[:, :, :, 2*num_notes:] represents the velocity of each note\n",
    "    # assuming it gets turned on. Note that to get the actual values you'll need to take a sigmoid\n",
    "    def forward(self, history, mask, instruments):\n",
    "        N = history.shape[0] # max instruments\n",
    "        L = history.shape[1] # longest length\n",
    "        B = history.shape[2] # batch size\n",
    "        A = history.shape[3]\n",
    "        assert(instruments.shape == (N, B))\n",
    "        assert(mask.shape == (N, L, B))\n",
    "        assert(A == self.action_dim)\n",
    "        \n",
    "        inputs = self.embedding(history) + torch.tanh(self.i_embedding(instruments)).unsqueeze(1).expand(-1, L, -1, -1)\n",
    "        \n",
    "        decoding = torch.zeros((N, L, B, embed_dim))\n",
    "        \n",
    "        for start in range(0, L, self.chunk_size):\n",
    "            end = min(start + chunk_size, L)\n",
    "            size = end - start\n",
    "            \n",
    "            for inst in range(N):\n",
    "                memory_idx = [i for i in range(N) if i != inst]\n",
    "                \n",
    "                tgt_key_padding_mask = mask[inst, start:end].transpose(0, 1)\n",
    "\n",
    "                tgt_mask = torch.triu(torch.ones((size, size), dtype=torch.bool))\n",
    "                tgt_mask.fill_diagonal_(False)\n",
    "\n",
    "                if N == 1:\n",
    "                    memory = torch.zeros((1, B, self.embed_dim))\n",
    "                    memory_key_padding_mask = None\n",
    "                    memory_mask = None\n",
    "                else:\n",
    "                    memory = self.position_encoding(inputs[memory_idx, start:end]).view(-1, B, self.embed_dim)\n",
    "                    memory_key_padding_mask = mask[memory_idx, start:end].view(-1, B).transpose(0, 1)\n",
    "                    memory_mask = tgt_mask.repeat(1, N - 1)\n",
    "\n",
    "                decoder_inputs = self.position_encoding(inputs[inst, start:end].unsqueeze(0)).squeeze(0)\n",
    "                decoding[inst, start:end] = self.decoder(decoder_inputs, \\\n",
    "                                                         memory, \\\n",
    "                                                         tgt_mask=tgt_mask, \\\n",
    "                                                         memory_mask=memory_mask, \\\n",
    "                                                         tgt_key_padding_mask=tgt_key_padding_mask, \\\n",
    "                                                         memory_key_padding_mask=memory_key_padding_mask)\n",
    "\n",
    "        return self.logits(decoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests for baseline transformer\n",
    "We train with model.eval() to disable dropout, since these tests try to get the model to overfit to a small sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model to overfit to a single instrument's part in a single song (only the first 100 time steps). Tests decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_size = 512\n",
    "heads = 8\n",
    "attention_layers=  6\n",
    "embed_dim = 256\n",
    "chunk_size = 200\n",
    "\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleTransformer(action_dim, embed_dim, num_instruments, heads, attention_layers, ff_size, chunk_size)\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "    \n",
    "model.eval() # Training with eval just to see if we can overfit without dropout\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('overfit_single_instrument.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Loss: 0.982203\n",
      "Starting epoch 1\n",
      "Loss: 0.935216\n",
      "Starting epoch 2\n",
      "Loss: 0.895947\n",
      "Starting epoch 3\n",
      "Loss: 0.870366\n",
      "Starting epoch 4\n",
      "Loss: 0.849073\n",
      "Starting epoch 5\n",
      "Loss: 0.829473\n",
      "Starting epoch 6\n",
      "Loss: 0.811161\n",
      "Starting epoch 7\n",
      "Loss: 0.794440\n",
      "Starting epoch 8\n",
      "Loss: 0.779591\n",
      "Starting epoch 9\n",
      "Loss: 0.766769\n",
      "Starting epoch 10\n",
      "Loss: 0.755947\n",
      "Starting epoch 11\n",
      "Loss: 0.746941\n",
      "Starting epoch 12\n",
      "Loss: 0.739508\n",
      "Starting epoch 13\n",
      "Loss: 0.733396\n",
      "Starting epoch 14\n",
      "Loss: 0.728375\n",
      "Starting epoch 15\n",
      "Loss: 0.724249\n",
      "Starting epoch 16\n",
      "Loss: 0.720855\n",
      "Starting epoch 17\n",
      "Loss: 0.718058\n",
      "Starting epoch 18\n",
      "Loss: 0.715748\n",
      "Starting epoch 19\n",
      "Loss: 0.713835\n",
      "Starting epoch 20\n",
      "Loss: 0.712247\n",
      "Starting epoch 21\n",
      "Loss: 0.710923\n",
      "Starting epoch 22\n",
      "Loss: 0.709817\n",
      "Starting epoch 23\n",
      "Loss: 0.708887\n",
      "Starting epoch 24\n",
      "Loss: 0.708103\n",
      "Starting epoch 25\n",
      "Loss: 0.707439\n",
      "Starting epoch 26\n",
      "Loss: 0.706874\n",
      "Starting epoch 27\n",
      "Loss: 0.706390\n",
      "Starting epoch 28\n",
      "Loss: 0.705974\n",
      "Starting epoch 29\n",
      "Loss: 0.705614\n",
      "Starting epoch 30\n",
      "Loss: 0.705302\n",
      "Starting epoch 31\n",
      "Loss: 0.705029\n",
      "Starting epoch 32\n",
      "Loss: 0.704790\n",
      "Starting epoch 33\n",
      "Loss: 0.704580\n",
      "Starting epoch 34\n",
      "Loss: 0.704393\n",
      "Starting epoch 35\n",
      "Loss: 0.704228\n",
      "Starting epoch 36\n",
      "Loss: 0.704081\n",
      "Starting epoch 37\n",
      "Loss: 0.703949\n",
      "Starting epoch 38\n",
      "Loss: 0.703830\n",
      "Starting epoch 39\n",
      "Loss: 0.703724\n",
      "Starting epoch 40\n",
      "Loss: 0.703627\n",
      "Starting epoch 41\n",
      "Loss: 0.703539\n",
      "Starting epoch 42\n",
      "Loss: 0.703459\n",
      "Starting epoch 43\n",
      "Loss: 0.703386\n",
      "Starting epoch 44\n",
      "Loss: 0.703320\n",
      "Starting epoch 45\n",
      "Loss: 0.703259\n",
      "Starting epoch 46\n",
      "Loss: 0.703202\n",
      "Starting epoch 47\n",
      "Loss: 0.703150\n",
      "Starting epoch 48\n",
      "Loss: 0.703102\n",
      "Starting epoch 49\n",
      "Loss: 0.703057\n",
      "Starting epoch 50\n",
      "Loss: 0.703016\n",
      "Starting epoch 51\n",
      "Loss: 0.702976\n",
      "Starting epoch 52\n",
      "Loss: 0.702940\n",
      "Starting epoch 53\n",
      "Loss: 0.702907\n",
      "Starting epoch 54\n",
      "Loss: 0.702874\n",
      "Starting epoch 55\n",
      "Loss: 0.702844\n",
      "Starting epoch 56\n",
      "Loss: 0.702816\n",
      "Starting epoch 57\n",
      "Loss: 0.702788\n",
      "Starting epoch 58\n",
      "Loss: 0.702763\n",
      "Starting epoch 59\n",
      "Loss: 0.702740\n",
      "Starting epoch 60\n",
      "Loss: 0.702716\n",
      "Starting epoch 61\n",
      "Loss: 0.702694\n",
      "Starting epoch 62\n",
      "Loss: 0.702673\n",
      "Starting epoch 63\n",
      "Loss: 0.702653\n",
      "Starting epoch 64\n",
      "Loss: 0.702633\n",
      "Starting epoch 65\n",
      "Loss: 0.702616\n",
      "Starting epoch 66\n",
      "Loss: 0.702597\n",
      "Starting epoch 67\n",
      "Loss: 0.702581\n",
      "Starting epoch 68\n",
      "Loss: 0.702564\n",
      "Starting epoch 69\n",
      "Loss: 0.702548\n",
      "Starting epoch 70\n",
      "Loss: 0.702532\n",
      "Starting epoch 71\n",
      "Loss: 0.702518\n",
      "Starting epoch 72\n",
      "Loss: 0.702503\n",
      "Starting epoch 73\n",
      "Loss: 0.702489\n",
      "Starting epoch 74\n",
      "Loss: 0.702475\n",
      "Starting epoch 75\n",
      "Loss: 0.702462\n",
      "Starting epoch 76\n",
      "Loss: 0.702449\n",
      "Starting epoch 77\n",
      "Loss: 0.702437\n",
      "Starting epoch 78\n",
      "Loss: 0.702424\n",
      "Starting epoch 79\n",
      "Loss: 0.702412\n",
      "Starting epoch 80\n",
      "Loss: 0.702401\n",
      "Starting epoch 81\n",
      "Loss: 0.702389\n",
      "Starting epoch 82\n",
      "Loss: 0.702378\n",
      "Starting epoch 83\n",
      "Loss: 0.702367\n",
      "Starting epoch 84\n",
      "Loss: 0.702356\n",
      "Starting epoch 85\n",
      "Loss: 0.702346\n",
      "Starting epoch 86\n",
      "Loss: 0.702336\n",
      "Starting epoch 87\n",
      "Loss: 0.702325\n",
      "Starting epoch 88\n",
      "Loss: 0.702316\n",
      "Starting epoch 89\n",
      "Loss: 0.702306\n",
      "Starting epoch 90\n",
      "Loss: 0.702297\n",
      "Starting epoch 91\n",
      "Loss: 0.702287\n",
      "Starting epoch 92\n",
      "Loss: 0.702278\n",
      "Starting epoch 93\n",
      "Loss: 0.702269\n",
      "Starting epoch 94\n",
      "Loss: 0.702260\n",
      "Starting epoch 95\n",
      "Loss: 0.702252\n",
      "Starting epoch 96\n",
      "Loss: 0.702243\n",
      "Starting epoch 97\n",
      "Loss: 0.702235\n",
      "Starting epoch 98\n",
      "Loss: 0.702226\n",
      "Starting epoch 99\n",
      "Loss: 0.702218\n",
      "Starting epoch 100\n",
      "Loss: 0.702211\n",
      "Starting epoch 101\n",
      "Loss: 0.702202\n",
      "Starting epoch 102\n",
      "Loss: 0.702195\n",
      "Starting epoch 103\n",
      "Loss: 0.702187\n",
      "Starting epoch 104\n",
      "Loss: 0.702180\n",
      "Starting epoch 105\n",
      "Loss: 0.702172\n",
      "Starting epoch 106\n",
      "Loss: 0.702165\n",
      "Starting epoch 107\n",
      "Loss: 0.702157\n",
      "Starting epoch 108\n",
      "Loss: 0.702151\n",
      "Starting epoch 109\n",
      "Loss: 0.702144\n",
      "Starting epoch 110\n",
      "Loss: 0.702138\n",
      "Starting epoch 111\n",
      "Loss: 0.702131\n",
      "Starting epoch 112\n",
      "Loss: 0.702124\n",
      "Starting epoch 113\n",
      "Loss: 0.702117\n",
      "Starting epoch 114\n",
      "Loss: 0.702111\n",
      "Starting epoch 115\n",
      "Loss: 0.702104\n",
      "Starting epoch 116\n",
      "Loss: 0.702098\n",
      "Starting epoch 117\n",
      "Loss: 0.702092\n",
      "Starting epoch 118\n",
      "Loss: 0.702086\n",
      "Starting epoch 119\n",
      "Loss: 0.702080\n",
      "Starting epoch 120\n",
      "Loss: 0.702074\n",
      "Starting epoch 121\n",
      "Loss: 0.702068\n",
      "Starting epoch 122\n",
      "Loss: 0.702062\n",
      "Starting epoch 123\n",
      "Loss: 0.702057\n",
      "Starting epoch 124\n",
      "Loss: 0.702051\n",
      "Starting epoch 125\n",
      "Loss: 0.702045\n",
      "Starting epoch 126\n",
      "Loss: 0.702040\n",
      "Starting epoch 127\n",
      "Loss: 0.702035\n",
      "Starting epoch 128\n",
      "Loss: 0.702029\n",
      "Starting epoch 129\n",
      "Loss: 0.702025\n",
      "Starting epoch 130\n",
      "Loss: 0.702019\n",
      "Starting epoch 131\n",
      "Loss: 0.702014\n",
      "Starting epoch 132\n",
      "Loss: 0.702009\n",
      "Starting epoch 133\n",
      "Loss: 0.702005\n",
      "Starting epoch 134\n",
      "Loss: 0.701999\n",
      "Starting epoch 135\n",
      "Loss: 0.701994\n",
      "Starting epoch 136\n",
      "Loss: 0.701989\n",
      "Starting epoch 137\n",
      "Loss: 0.701985\n",
      "Starting epoch 138\n",
      "Loss: 0.701980\n",
      "Starting epoch 139\n",
      "Loss: 0.701976\n",
      "Starting epoch 140\n",
      "Loss: 0.701972\n",
      "Starting epoch 141\n",
      "Loss: 0.701967\n",
      "Starting epoch 142\n",
      "Loss: 0.701962\n",
      "Starting epoch 143\n",
      "Loss: 0.701958\n",
      "Starting epoch 144\n",
      "Loss: 0.701954\n",
      "Starting epoch 145\n",
      "Loss: 0.701950\n",
      "Starting epoch 146\n",
      "Loss: 0.701946\n",
      "Starting epoch 147\n",
      "Loss: 0.701941\n",
      "Starting epoch 148\n",
      "Loss: 0.701937\n",
      "Starting epoch 149\n",
      "Loss: 0.701933\n",
      "Starting epoch 150\n",
      "Loss: 0.701929\n",
      "Starting epoch 151\n",
      "Loss: 0.701926\n",
      "Starting epoch 152\n",
      "Loss: 0.701921\n",
      "Starting epoch 153\n",
      "Loss: 0.701917\n",
      "Starting epoch 154\n",
      "Loss: 0.701914\n",
      "Starting epoch 155\n",
      "Loss: 0.701910\n",
      "Starting epoch 156\n",
      "Loss: 0.701906\n",
      "Starting epoch 157\n",
      "Loss: 0.701903\n",
      "Starting epoch 158\n",
      "Loss: 0.701899\n",
      "Starting epoch 159\n",
      "Loss: 0.701896\n",
      "Starting epoch 160\n",
      "Loss: 0.701892\n",
      "Starting epoch 161\n",
      "Loss: 0.701889\n",
      "Starting epoch 162\n",
      "Loss: 0.701886\n",
      "Starting epoch 163\n",
      "Loss: 0.701883\n",
      "Starting epoch 164\n",
      "Loss: 0.701879\n",
      "Starting epoch 165\n",
      "Loss: 0.701875\n",
      "Starting epoch 166\n",
      "Loss: 0.701872\n",
      "Starting epoch 167\n",
      "Loss: 0.701868\n",
      "Starting epoch 168\n",
      "Loss: 0.701865\n",
      "Starting epoch 169\n",
      "Loss: 0.701863\n",
      "Starting epoch 170\n",
      "Loss: 0.701860\n",
      "Starting epoch 171\n",
      "Loss: 0.701857\n",
      "Starting epoch 172\n",
      "Loss: 0.701854\n",
      "Starting epoch 173\n",
      "Loss: 0.701851\n",
      "Starting epoch 174\n",
      "Loss: 0.701848\n",
      "Starting epoch 175\n",
      "Loss: 0.701844\n",
      "Starting epoch 176\n",
      "Loss: 0.701841\n",
      "Starting epoch 177\n",
      "Loss: 0.701839\n",
      "Starting epoch 178\n",
      "Loss: 0.701836\n",
      "Starting epoch 179\n",
      "Loss: 0.701833\n",
      "Starting epoch 180\n",
      "Loss: 0.701830\n",
      "Starting epoch 181\n",
      "Loss: 0.701828\n",
      "Starting epoch 182\n",
      "Loss: 0.701825\n",
      "Starting epoch 183\n",
      "Loss: 0.701822\n",
      "Starting epoch 184\n",
      "Loss: 0.701820\n",
      "Starting epoch 185\n",
      "Loss: 0.701817\n",
      "Starting epoch 186\n",
      "Loss: 0.701814\n",
      "Starting epoch 187\n",
      "Loss: 0.701812\n",
      "Starting epoch 188\n",
      "Loss: 0.701809\n",
      "Starting epoch 189\n",
      "Loss: 0.701806\n",
      "Starting epoch 190\n",
      "Loss: 0.701804\n",
      "Starting epoch 191\n",
      "Loss: 0.701802\n",
      "Starting epoch 192\n",
      "Loss: 0.701799\n",
      "Starting epoch 193\n",
      "Loss: 0.701797\n",
      "Starting epoch 194\n",
      "Loss: 0.701795\n",
      "Starting epoch 195\n",
      "Loss: 0.701792\n",
      "Starting epoch 196\n",
      "Loss: 0.701790\n",
      "Starting epoch 197\n",
      "Loss: 0.701788\n",
      "Starting epoch 198\n",
      "Loss: 0.701786\n",
      "Starting epoch 199\n",
      "Loss: 0.701784\n",
      "Starting epoch 200\n",
      "Loss: 0.701781\n",
      "Starting epoch 201\n",
      "Loss: 0.701779\n",
      "Starting epoch 202\n",
      "Loss: 0.701776\n",
      "Starting epoch 203\n",
      "Loss: 0.701774\n",
      "Starting epoch 204\n",
      "Loss: 0.701772\n",
      "Starting epoch 205\n",
      "Loss: 0.701770\n",
      "Starting epoch 206\n",
      "Loss: 0.701768\n",
      "Starting epoch 207\n",
      "Loss: 0.701766\n",
      "Starting epoch 208\n",
      "Loss: 0.701764\n",
      "Starting epoch 209\n",
      "Loss: 0.701762\n",
      "Starting epoch 210\n",
      "Loss: 0.701760\n",
      "Starting epoch 211\n",
      "Loss: 0.701758\n",
      "Starting epoch 212\n",
      "Loss: 0.701756\n",
      "Starting epoch 213\n",
      "Loss: 0.701754\n",
      "Starting epoch 214\n",
      "Loss: 0.701752\n",
      "Starting epoch 215\n",
      "Loss: 0.701750\n",
      "Starting epoch 216\n",
      "Loss: 0.701748\n",
      "Starting epoch 217\n",
      "Loss: 0.701746\n",
      "Starting epoch 218\n",
      "Loss: 0.701744\n",
      "Starting epoch 219\n",
      "Loss: 0.701743\n",
      "Starting epoch 220\n",
      "Loss: 0.701740\n",
      "Starting epoch 221\n",
      "Loss: 0.701739\n",
      "Starting epoch 222\n",
      "Loss: 0.701737\n",
      "Starting epoch 223\n",
      "Loss: 0.701736\n",
      "Starting epoch 224\n",
      "Loss: 0.701734\n",
      "Starting epoch 225\n",
      "Loss: 0.701732\n",
      "Starting epoch 226\n",
      "Loss: 0.701730\n",
      "Starting epoch 227\n",
      "Loss: 0.701729\n",
      "Starting epoch 228\n",
      "Loss: 0.701727\n",
      "Starting epoch 229\n",
      "Loss: 0.701725\n",
      "Starting epoch 230\n",
      "Loss: 0.701724\n",
      "Starting epoch 231\n",
      "Loss: 0.701722\n",
      "Starting epoch 232\n",
      "Loss: 0.701720\n",
      "Starting epoch 233\n",
      "Loss: 0.701719\n",
      "Starting epoch 234\n",
      "Loss: 0.701717\n",
      "Starting epoch 235\n",
      "Loss: 0.701716\n",
      "Starting epoch 236\n",
      "Loss: 0.701714\n",
      "Starting epoch 237\n",
      "Loss: 0.701712\n",
      "Starting epoch 238\n",
      "Loss: 0.701711\n",
      "Starting epoch 239\n",
      "Loss: 0.701709\n",
      "Starting epoch 240\n",
      "Loss: 0.701708\n",
      "Starting epoch 241\n",
      "Loss: 0.701706\n",
      "Starting epoch 242\n",
      "Loss: 0.701705\n",
      "Starting epoch 243\n",
      "Loss: 0.701703\n",
      "Starting epoch 244\n",
      "Loss: 0.701702\n",
      "Starting epoch 245\n",
      "Loss: 0.701701\n",
      "Starting epoch 246\n",
      "Loss: 0.701699\n",
      "Starting epoch 247\n",
      "Loss: 0.701698\n",
      "Starting epoch 248\n",
      "Loss: 0.701697\n",
      "Starting epoch 249\n",
      "Loss: 0.701695\n",
      "Starting epoch 250\n",
      "Loss: 0.701694\n",
      "Starting epoch 251\n",
      "Loss: 0.701692\n",
      "Starting epoch 252\n",
      "Loss: 0.701691\n",
      "Starting epoch 253\n",
      "Loss: 0.701690\n",
      "Starting epoch 254\n",
      "Loss: 0.701688\n",
      "Starting epoch 255\n",
      "Loss: 0.701687\n",
      "Starting epoch 256\n",
      "Loss: 0.701686\n",
      "Starting epoch 257\n",
      "Loss: 0.701684\n",
      "Starting epoch 258\n",
      "Loss: 0.701683\n",
      "Starting epoch 259\n",
      "Loss: 0.701682\n",
      "Starting epoch 260\n",
      "Loss: 0.701680\n",
      "Starting epoch 261\n",
      "Loss: 0.701679\n",
      "Starting epoch 262\n",
      "Loss: 0.701678\n",
      "Starting epoch 263\n",
      "Loss: 0.701677\n",
      "Starting epoch 264\n",
      "Loss: 0.701676\n",
      "Starting epoch 265\n",
      "Loss: 0.701675\n",
      "Starting epoch 266\n",
      "Loss: 0.701674\n",
      "Starting epoch 267\n",
      "Loss: 0.701673\n",
      "Starting epoch 268\n",
      "Loss: 0.701672\n",
      "Starting epoch 269\n",
      "Loss: 0.701670\n",
      "Starting epoch 270\n",
      "Loss: 0.701669\n",
      "Starting epoch 271\n",
      "Loss: 0.701668\n",
      "Starting epoch 272\n",
      "Loss: 0.701667\n",
      "Starting epoch 273\n",
      "Loss: 0.701666\n",
      "Starting epoch 274\n",
      "Loss: 0.701665\n",
      "Starting epoch 275\n",
      "Loss: 0.701663\n",
      "Starting epoch 276\n",
      "Loss: 0.701662\n",
      "Starting epoch 277\n",
      "Loss: 0.701661\n",
      "Starting epoch 278\n",
      "Loss: 0.701660\n",
      "Starting epoch 279\n",
      "Loss: 0.701659\n",
      "Starting epoch 280\n",
      "Loss: 0.701658\n",
      "Starting epoch 281\n",
      "Loss: 0.701657\n",
      "Starting epoch 282\n",
      "Loss: 0.701656\n",
      "Starting epoch 283\n",
      "Loss: 0.701655\n",
      "Starting epoch 284\n",
      "Loss: 0.701654\n",
      "Starting epoch 285\n",
      "Loss: 0.701653\n",
      "Starting epoch 286\n",
      "Loss: 0.701652\n",
      "Starting epoch 287\n",
      "Loss: 0.701651\n",
      "Starting epoch 288\n",
      "Loss: 0.701650\n",
      "Starting epoch 289\n",
      "Loss: 0.701649\n",
      "Starting epoch 290\n",
      "Loss: 0.701648\n",
      "Starting epoch 291\n",
      "Loss: 0.701647\n",
      "Starting epoch 292\n",
      "Loss: 0.701646\n",
      "Starting epoch 293\n",
      "Loss: 0.701645\n",
      "Starting epoch 294\n",
      "Loss: 0.701644\n",
      "Starting epoch 295\n",
      "Loss: 0.701643\n",
      "Starting epoch 296\n",
      "Loss: 0.701643\n",
      "Starting epoch 297\n",
      "Loss: 0.701641\n",
      "Starting epoch 298\n",
      "Loss: 0.701641\n",
      "Starting epoch 299\n",
      "Loss: 0.701640\n",
      "Starting epoch 300\n",
      "Loss: 0.701639\n",
      "Starting epoch 301\n",
      "Loss: 0.701638\n",
      "Starting epoch 302\n",
      "Loss: 0.701637\n",
      "Starting epoch 303\n",
      "Loss: 0.701636\n",
      "Starting epoch 304\n",
      "Loss: 0.701635\n",
      "Starting epoch 305\n",
      "Loss: 0.701634\n",
      "Starting epoch 306\n",
      "Loss: 0.701633\n",
      "Starting epoch 307\n",
      "Loss: 0.701632\n",
      "Starting epoch 308\n",
      "Loss: 0.701631\n",
      "Starting epoch 309\n",
      "Loss: 0.701630\n",
      "Starting epoch 310\n",
      "Loss: 0.701630\n",
      "Starting epoch 311\n",
      "Loss: 0.701629\n",
      "Starting epoch 312\n",
      "Loss: 0.701628\n",
      "Starting epoch 313\n",
      "Loss: 0.701627\n",
      "Starting epoch 314\n",
      "Loss: 0.701626\n",
      "Starting epoch 315\n",
      "Loss: 0.701626\n",
      "Starting epoch 316\n",
      "Loss: 0.701625\n",
      "Starting epoch 317\n",
      "Loss: 0.701624\n",
      "Starting epoch 318\n",
      "Loss: 0.701623\n",
      "Starting epoch 319\n",
      "Loss: 0.701623\n",
      "Starting epoch 320\n",
      "Loss: 0.701622\n",
      "Starting epoch 321\n",
      "Loss: 0.701622\n",
      "Starting epoch 322\n",
      "Loss: 0.701621\n",
      "Starting epoch 323\n",
      "Loss: 0.701620\n",
      "Starting epoch 324\n",
      "Loss: 0.701619\n",
      "Starting epoch 325\n",
      "Loss: 0.701619\n",
      "Starting epoch 326\n",
      "Loss: 0.701618\n",
      "Starting epoch 327\n",
      "Loss: 0.701618\n",
      "Starting epoch 328\n",
      "Loss: 0.701617\n",
      "Starting epoch 329\n",
      "Loss: 0.701616\n",
      "Starting epoch 330\n",
      "Loss: 0.701615\n",
      "Starting epoch 331\n",
      "Loss: 0.701614\n",
      "Starting epoch 332\n",
      "Loss: 0.701614\n",
      "Starting epoch 333\n",
      "Loss: 0.701613\n",
      "Starting epoch 334\n",
      "Loss: 0.701612\n",
      "Starting epoch 335\n",
      "Loss: 0.701612\n",
      "Starting epoch 336\n",
      "Loss: 0.701611\n",
      "Starting epoch 337\n",
      "Loss: 0.701610\n",
      "Starting epoch 338\n",
      "Loss: 0.701610\n",
      "Starting epoch 339\n",
      "Loss: 0.701609\n",
      "Starting epoch 340\n",
      "Loss: 0.701608\n",
      "Starting epoch 341\n",
      "Loss: 0.701607\n",
      "Starting epoch 342\n",
      "Loss: 0.701607\n",
      "Starting epoch 343\n",
      "Loss: 0.701606\n",
      "Starting epoch 344\n",
      "Loss: 0.701605\n",
      "Starting epoch 345\n",
      "Loss: 0.701604\n",
      "Starting epoch 346\n",
      "Loss: 0.701604\n",
      "Starting epoch 347\n",
      "Loss: 0.701603\n",
      "Starting epoch 348\n",
      "Loss: 0.701603\n",
      "Starting epoch 349\n",
      "Loss: 0.701602\n",
      "Starting epoch 350\n",
      "Loss: 0.701602\n",
      "Starting epoch 351\n",
      "Loss: 0.701601\n",
      "Starting epoch 352\n",
      "Loss: 0.701600\n",
      "Starting epoch 353\n",
      "Loss: 0.701600\n",
      "Starting epoch 354\n",
      "Loss: 0.701599\n",
      "Starting epoch 355\n",
      "Loss: 0.701598\n",
      "Starting epoch 356\n",
      "Loss: 0.701598\n",
      "Starting epoch 357\n",
      "Loss: 0.701597\n",
      "Starting epoch 358\n",
      "Loss: 0.701597\n",
      "Starting epoch 359\n",
      "Loss: 0.701596\n",
      "Starting epoch 360\n",
      "Loss: 0.701595\n",
      "Starting epoch 361\n",
      "Loss: 0.701595\n",
      "Starting epoch 362\n",
      "Loss: 0.701595\n",
      "Starting epoch 363\n",
      "Loss: 0.701594\n",
      "Starting epoch 364\n",
      "Loss: 0.701593\n",
      "Starting epoch 365\n",
      "Loss: 0.701593\n",
      "Starting epoch 366\n",
      "Loss: 0.701592\n",
      "Starting epoch 367\n",
      "Loss: 0.701592\n",
      "Starting epoch 368\n",
      "Loss: 0.701591\n",
      "Starting epoch 369\n",
      "Loss: 0.701591\n",
      "Starting epoch 370\n",
      "Loss: 0.701590\n",
      "Starting epoch 371\n",
      "Loss: 0.701590\n",
      "Starting epoch 372\n",
      "Loss: 0.701589\n",
      "Starting epoch 373\n",
      "Loss: 0.701589\n",
      "Starting epoch 374\n",
      "Loss: 0.701588\n",
      "Starting epoch 375\n",
      "Loss: 0.701588\n",
      "Starting epoch 376\n",
      "Loss: 0.701587\n",
      "Starting epoch 377\n",
      "Loss: 0.701587\n",
      "Starting epoch 378\n",
      "Loss: 0.701586\n",
      "Starting epoch 379\n",
      "Loss: 0.701586\n",
      "Starting epoch 380\n",
      "Loss: 0.701585\n",
      "Starting epoch 381\n",
      "Loss: 0.701584\n",
      "Starting epoch 382\n",
      "Loss: 0.701584\n",
      "Starting epoch 383\n",
      "Loss: 0.701584\n",
      "Starting epoch 384\n",
      "Loss: 0.701583\n",
      "Starting epoch 385\n",
      "Loss: 0.701583\n",
      "Starting epoch 386\n",
      "Loss: 0.701582\n",
      "Starting epoch 387\n",
      "Loss: 0.701581\n",
      "Starting epoch 388\n",
      "Loss: 0.701581\n",
      "Starting epoch 389\n",
      "Loss: 0.701581\n",
      "Starting epoch 390\n",
      "Loss: 0.701580\n",
      "Starting epoch 391\n",
      "Loss: 0.701580\n",
      "Starting epoch 392\n",
      "Loss: 0.701579\n",
      "Starting epoch 393\n",
      "Loss: 0.701579\n",
      "Starting epoch 394\n",
      "Loss: 0.701578\n",
      "Starting epoch 395\n",
      "Loss: 0.701578\n",
      "Starting epoch 396\n",
      "Loss: 0.701577\n",
      "Starting epoch 397\n",
      "Loss: 0.701577\n",
      "Starting epoch 398\n",
      "Loss: 0.701576\n",
      "Starting epoch 399\n",
      "Loss: 0.701576\n",
      "Starting epoch 400\n",
      "Loss: 0.701575\n",
      "Starting epoch 401\n",
      "Loss: 0.701575\n",
      "Starting epoch 402\n",
      "Loss: 0.701575\n",
      "Starting epoch 403\n",
      "Loss: 0.701574\n",
      "Starting epoch 404\n",
      "Loss: 0.701574\n",
      "Starting epoch 405\n",
      "Loss: 0.701573\n",
      "Starting epoch 406\n",
      "Loss: 0.701573\n",
      "Starting epoch 407\n",
      "Loss: 0.701572\n",
      "Starting epoch 408\n",
      "Loss: 0.701572\n",
      "Starting epoch 409\n",
      "Loss: 0.701571\n",
      "Starting epoch 410\n",
      "Loss: 0.701571\n",
      "Starting epoch 411\n",
      "Loss: 0.701570\n",
      "Starting epoch 412\n",
      "Loss: 0.701570\n",
      "Starting epoch 413\n",
      "Loss: 0.701569\n",
      "Starting epoch 414\n",
      "Loss: 0.701569\n",
      "Starting epoch 415\n",
      "Loss: 0.701569\n",
      "Starting epoch 416\n",
      "Loss: 0.701568\n",
      "Starting epoch 417\n",
      "Loss: 0.701568\n",
      "Starting epoch 418\n",
      "Loss: 0.701568\n",
      "Starting epoch 419\n",
      "Loss: 0.701567\n",
      "Starting epoch 420\n",
      "Loss: 0.701567\n",
      "Starting epoch 421\n",
      "Loss: 0.701566\n",
      "Starting epoch 422\n",
      "Loss: 0.701566\n",
      "Starting epoch 423\n",
      "Loss: 0.701566\n",
      "Starting epoch 424\n",
      "Loss: 0.701566\n",
      "Starting epoch 425\n",
      "Loss: 0.701565\n",
      "Starting epoch 426\n",
      "Loss: 0.701565\n",
      "Starting epoch 427\n",
      "Loss: 0.701565\n",
      "Starting epoch 428\n",
      "Loss: 0.701564\n",
      "Starting epoch 429\n",
      "Loss: 0.701564\n",
      "Starting epoch 430\n",
      "Loss: 0.701564\n",
      "Starting epoch 431\n",
      "Loss: 0.701563\n",
      "Starting epoch 432\n",
      "Loss: 0.701563\n",
      "Starting epoch 433\n",
      "Loss: 0.701563\n",
      "Starting epoch 434\n",
      "Loss: 0.701562\n",
      "Starting epoch 435\n",
      "Loss: 0.701562\n",
      "Starting epoch 436\n",
      "Loss: 0.701562\n",
      "Starting epoch 437\n",
      "Loss: 0.701561\n",
      "Starting epoch 438\n",
      "Loss: 0.701561\n",
      "Starting epoch 439\n",
      "Loss: 0.701560\n",
      "Starting epoch 440\n",
      "Loss: 0.701560\n",
      "Starting epoch 441\n",
      "Loss: 0.701560\n",
      "Starting epoch 442\n",
      "Loss: 0.701559\n",
      "Starting epoch 443\n",
      "Loss: 0.701559\n",
      "Starting epoch 444\n",
      "Loss: 0.701559\n",
      "Starting epoch 445\n",
      "Loss: 0.701558\n",
      "Starting epoch 446\n",
      "Loss: 0.701558\n",
      "Starting epoch 447\n",
      "Loss: 0.701558\n",
      "Starting epoch 448\n",
      "Loss: 0.701558\n",
      "Starting epoch 449\n",
      "Loss: 0.701557\n",
      "Starting epoch 450\n",
      "Loss: 0.701557\n",
      "Starting epoch 451\n",
      "Loss: 0.701557\n",
      "Starting epoch 452\n",
      "Loss: 0.701556\n",
      "Starting epoch 453\n",
      "Loss: 0.701556\n",
      "Starting epoch 454\n",
      "Loss: 0.701556\n",
      "Starting epoch 455\n",
      "Loss: 0.701556\n",
      "Starting epoch 456\n",
      "Loss: 0.701555\n",
      "Starting epoch 457\n",
      "Loss: 0.701555\n",
      "Starting epoch 458\n",
      "Loss: 0.701554\n",
      "Starting epoch 459\n",
      "Loss: 0.701554\n",
      "Starting epoch 460\n",
      "Loss: 0.701554\n",
      "Starting epoch 461\n",
      "Loss: 0.701554\n",
      "Starting epoch 462\n",
      "Loss: 0.701553\n",
      "Starting epoch 463\n",
      "Loss: 0.701553\n",
      "Starting epoch 464\n",
      "Loss: 0.701553\n",
      "Starting epoch 465\n",
      "Loss: 0.701553\n",
      "Starting epoch 466\n",
      "Loss: 0.701552\n",
      "Starting epoch 467\n",
      "Loss: 0.701552\n",
      "Starting epoch 468\n",
      "Loss: 0.701551\n",
      "Starting epoch 469\n",
      "Loss: 0.701551\n",
      "Starting epoch 470\n",
      "Loss: 0.701551\n",
      "Starting epoch 471\n",
      "Loss: 0.701550\n",
      "Starting epoch 472\n",
      "Loss: 0.701550\n",
      "Starting epoch 473\n",
      "Loss: 0.701550\n",
      "Starting epoch 474\n",
      "Loss: 0.701549\n",
      "Starting epoch 475\n",
      "Loss: 0.701549\n",
      "Starting epoch 476\n",
      "Loss: 0.701549\n",
      "Starting epoch 477\n",
      "Loss: 0.701549\n",
      "Starting epoch 478\n",
      "Loss: 0.701548\n",
      "Starting epoch 479\n",
      "Loss: 0.701548\n",
      "Starting epoch 480\n",
      "Loss: 0.701547\n",
      "Starting epoch 481\n",
      "Loss: 0.701547\n",
      "Starting epoch 482\n",
      "Loss: 0.701547\n",
      "Starting epoch 483\n",
      "Loss: 0.701547\n",
      "Starting epoch 484\n",
      "Loss: 0.701546\n",
      "Starting epoch 485\n",
      "Loss: 0.701546\n",
      "Starting epoch 486\n",
      "Loss: 0.701546\n",
      "Starting epoch 487\n",
      "Loss: 0.701546\n",
      "Starting epoch 488\n",
      "Loss: 0.701546\n",
      "Starting epoch 489\n",
      "Loss: 0.701546\n",
      "Starting epoch 490\n",
      "Loss: 0.701545\n",
      "Starting epoch 491\n",
      "Loss: 0.701545\n",
      "Starting epoch 492\n",
      "Loss: 0.701544\n",
      "Starting epoch 493\n",
      "Loss: 0.701544\n",
      "Starting epoch 494\n",
      "Loss: 0.701544\n",
      "Starting epoch 495\n",
      "Loss: 0.701544\n",
      "Starting epoch 496\n",
      "Loss: 0.701543\n",
      "Starting epoch 497\n",
      "Loss: 0.701543\n",
      "Starting epoch 498\n",
      "Loss: 0.701543\n",
      "Starting epoch 499\n",
      "Loss: 0.701542\n",
      "Starting epoch 500\n",
      "Loss: 0.701542\n",
      "Starting epoch 501\n",
      "Loss: 0.701542\n",
      "Starting epoch 502\n",
      "Loss: 0.701542\n",
      "Starting epoch 503\n",
      "Loss: 0.701542\n",
      "Starting epoch 504\n",
      "Loss: 0.701541\n",
      "Starting epoch 505\n",
      "Loss: 0.701541\n",
      "Starting epoch 506\n",
      "Loss: 0.701541\n",
      "Starting epoch 507\n",
      "Loss: 0.701541\n",
      "Starting epoch 508\n",
      "Loss: 0.701541\n",
      "Starting epoch 509\n",
      "Loss: 0.701540\n",
      "Starting epoch 510\n",
      "Loss: 0.701540\n",
      "Starting epoch 511\n",
      "Loss: 0.701540\n",
      "Starting epoch 512\n",
      "Loss: 0.701540\n",
      "Starting epoch 513\n",
      "Loss: 0.701539\n",
      "Starting epoch 514\n",
      "Loss: 0.701539\n",
      "Starting epoch 515\n",
      "Loss: 0.701539\n",
      "Starting epoch 516\n",
      "Loss: 0.701539\n",
      "Starting epoch 517\n",
      "Loss: 0.701538\n",
      "Starting epoch 518\n",
      "Loss: 0.701538\n",
      "Starting epoch 519\n",
      "Loss: 0.701538\n",
      "Starting epoch 520\n",
      "Loss: 0.701538\n",
      "Starting epoch 521\n",
      "Loss: 0.701538\n",
      "Starting epoch 522\n",
      "Loss: 0.701537\n",
      "Starting epoch 523\n",
      "Loss: 0.701537\n",
      "Starting epoch 524\n",
      "Loss: 0.701537\n",
      "Starting epoch 525\n",
      "Loss: 0.701536\n",
      "Starting epoch 526\n",
      "Loss: 0.701536\n",
      "Starting epoch 527\n",
      "Loss: 0.701536\n",
      "Starting epoch 528\n",
      "Loss: 0.701536\n",
      "Starting epoch 529\n",
      "Loss: 0.701535\n",
      "Starting epoch 530\n",
      "Loss: 0.701535\n",
      "Starting epoch 531\n",
      "Loss: 0.701535\n",
      "Starting epoch 532\n",
      "Loss: 0.701535\n",
      "Starting epoch 533\n",
      "Loss: 0.701535\n",
      "Starting epoch 534\n",
      "Loss: 0.701534\n",
      "Starting epoch 535\n",
      "Loss: 0.701534\n",
      "Starting epoch 536\n",
      "Loss: 0.701534\n",
      "Starting epoch 537\n",
      "Loss: 0.701534\n",
      "Starting epoch 538\n",
      "Loss: 0.701534\n",
      "Starting epoch 539\n",
      "Loss: 0.701533\n",
      "Starting epoch 540\n",
      "Loss: 0.701533\n",
      "Starting epoch 541\n",
      "Loss: 0.701533\n",
      "Starting epoch 542\n",
      "Loss: 0.701533\n",
      "Starting epoch 543\n",
      "Loss: 0.701533\n",
      "Starting epoch 544\n",
      "Loss: 0.701533\n",
      "Starting epoch 545\n",
      "Loss: 0.701532\n",
      "Starting epoch 546\n",
      "Loss: 0.701532\n",
      "Starting epoch 547\n",
      "Loss: 0.701532\n",
      "Starting epoch 548\n",
      "Loss: 0.701532\n",
      "Starting epoch 549\n",
      "Loss: 0.701531\n",
      "Starting epoch 550\n",
      "Loss: 0.701531\n",
      "Starting epoch 551\n",
      "Loss: 0.701531\n",
      "Starting epoch 552\n",
      "Loss: 0.701531\n",
      "Starting epoch 553\n",
      "Loss: 0.701531\n",
      "Starting epoch 554\n",
      "Loss: 0.701531\n",
      "Starting epoch 555\n",
      "Loss: 0.701530\n",
      "Starting epoch 556\n",
      "Loss: 0.701530\n",
      "Starting epoch 557\n",
      "Loss: 0.701530\n",
      "Starting epoch 558\n",
      "Loss: 0.701530\n",
      "Starting epoch 559\n",
      "Loss: 0.701529\n",
      "Starting epoch 560\n",
      "Loss: 0.701529\n",
      "Starting epoch 561\n",
      "Loss: 0.701529\n",
      "Starting epoch 562\n",
      "Loss: 0.701529\n",
      "Starting epoch 563\n",
      "Loss: 0.701529\n",
      "Starting epoch 564\n",
      "Loss: 0.701528\n",
      "Starting epoch 565\n",
      "Loss: 0.701528\n",
      "Starting epoch 566\n",
      "Loss: 0.701528\n",
      "Starting epoch 567\n",
      "Loss: 0.701528\n",
      "Starting epoch 568\n",
      "Loss: 0.701528\n",
      "Starting epoch 569\n",
      "Loss: 0.701528\n",
      "Starting epoch 570\n",
      "Loss: 0.701527\n",
      "Starting epoch 571\n",
      "Loss: 0.701527\n",
      "Starting epoch 572\n",
      "Loss: 0.701527\n",
      "Starting epoch 573\n",
      "Loss: 0.701527\n",
      "Starting epoch 574\n",
      "Loss: 0.701527\n",
      "Starting epoch 575\n",
      "Loss: 0.701527\n",
      "Starting epoch 576\n",
      "Loss: 0.701526\n",
      "Starting epoch 577\n",
      "Loss: 0.701526\n",
      "Starting epoch 578\n",
      "Loss: 0.701526\n",
      "Starting epoch 579\n",
      "Loss: 0.701526\n",
      "Starting epoch 580\n",
      "Loss: 0.701526\n",
      "Starting epoch 581\n",
      "Loss: 0.701525\n",
      "Starting epoch 582\n",
      "Loss: 0.701525\n",
      "Starting epoch 583\n",
      "Loss: 0.701525\n",
      "Starting epoch 584\n",
      "Loss: 0.701525\n",
      "Starting epoch 585\n",
      "Loss: 0.701524\n",
      "Starting epoch 586\n",
      "Loss: 0.701524\n",
      "Starting epoch 587\n",
      "Loss: 0.701524\n",
      "Starting epoch 588\n",
      "Loss: 0.701524\n",
      "Starting epoch 589\n",
      "Loss: 0.701524\n",
      "Starting epoch 590\n",
      "Loss: 0.701524\n",
      "Starting epoch 591\n",
      "Loss: 0.701523\n",
      "Starting epoch 592\n",
      "Loss: 0.701523\n",
      "Starting epoch 593\n",
      "Loss: 0.701523\n",
      "Starting epoch 594\n",
      "Loss: 0.701523\n",
      "Starting epoch 595\n",
      "Loss: 0.701523\n",
      "Starting epoch 596\n",
      "Loss: 0.701523\n",
      "Starting epoch 597\n",
      "Loss: 0.701523\n",
      "Starting epoch 598\n",
      "Loss: 0.701523\n",
      "Starting epoch 599\n",
      "Loss: 0.701522\n",
      "Starting epoch 600\n",
      "Loss: 0.701522\n",
      "Starting epoch 601\n",
      "Loss: 0.701522\n",
      "Starting epoch 602\n",
      "Loss: 0.701522\n",
      "Starting epoch 603\n",
      "Loss: 0.701522\n",
      "Starting epoch 604\n",
      "Loss: 0.701521\n",
      "Starting epoch 605\n",
      "Loss: 0.701521\n",
      "Starting epoch 606\n",
      "Loss: 0.701521\n",
      "Starting epoch 607\n",
      "Loss: 0.701521\n",
      "Starting epoch 608\n",
      "Loss: 0.701521\n",
      "Starting epoch 609\n",
      "Loss: 0.701521\n",
      "Starting epoch 610\n",
      "Loss: 0.701521\n",
      "Starting epoch 611\n",
      "Loss: 0.701520\n",
      "Starting epoch 612\n",
      "Loss: 0.701520\n",
      "Starting epoch 613\n",
      "Loss: 0.701520\n",
      "Starting epoch 614\n",
      "Loss: 0.701520\n",
      "Starting epoch 615\n",
      "Loss: 0.701520\n",
      "Starting epoch 616\n",
      "Loss: 0.701520\n",
      "Starting epoch 617\n",
      "Loss: 0.701519\n",
      "Starting epoch 618\n",
      "Loss: 0.701519\n",
      "Starting epoch 619\n",
      "Loss: 0.701519\n",
      "Starting epoch 620\n",
      "Loss: 0.701519\n",
      "Starting epoch 621\n",
      "Loss: 0.701519\n",
      "Starting epoch 622\n",
      "Loss: 0.701518\n",
      "Starting epoch 623\n",
      "Loss: 0.701518\n",
      "Starting epoch 624\n",
      "Loss: 0.701518\n",
      "Starting epoch 625\n",
      "Loss: 0.701518\n",
      "Starting epoch 626\n",
      "Loss: 0.701518\n",
      "Starting epoch 627\n",
      "Loss: 0.701517\n",
      "Starting epoch 628\n",
      "Loss: 0.701517\n",
      "Starting epoch 629\n",
      "Loss: 0.701517\n",
      "Starting epoch 630\n",
      "Loss: 0.701517\n",
      "Starting epoch 631\n",
      "Loss: 0.701517\n",
      "Starting epoch 632\n",
      "Loss: 0.701516\n",
      "Starting epoch 633\n",
      "Loss: 0.701516\n",
      "Starting epoch 634\n",
      "Loss: 0.701516\n",
      "Starting epoch 635\n",
      "Loss: 0.701516\n",
      "Starting epoch 636\n",
      "Loss: 0.701516\n",
      "Starting epoch 637\n",
      "Loss: 0.701516\n",
      "Starting epoch 638\n",
      "Loss: 0.701516\n",
      "Starting epoch 639\n",
      "Loss: 0.701515\n",
      "Starting epoch 640\n",
      "Loss: 0.701515\n",
      "Starting epoch 641\n",
      "Loss: 0.701515\n",
      "Starting epoch 642\n",
      "Loss: 0.701515\n",
      "Starting epoch 643\n",
      "Loss: 0.701515\n",
      "Starting epoch 644\n",
      "Loss: 0.701515\n",
      "Starting epoch 645\n",
      "Loss: 0.701514\n",
      "Starting epoch 646\n",
      "Loss: 0.701514\n",
      "Starting epoch 647\n",
      "Loss: 0.701514\n",
      "Starting epoch 648\n",
      "Loss: 0.701514\n",
      "Starting epoch 649\n",
      "Loss: 0.701514\n",
      "Starting epoch 650\n",
      "Loss: 0.701514\n",
      "Starting epoch 651\n",
      "Loss: 0.701514\n",
      "Starting epoch 652\n",
      "Loss: 0.701514\n",
      "Starting epoch 653\n",
      "Loss: 0.701514\n",
      "Starting epoch 654\n",
      "Loss: 0.701513\n",
      "Starting epoch 655\n",
      "Loss: 0.701513\n",
      "Starting epoch 656\n",
      "Loss: 0.701513\n",
      "Starting epoch 657\n",
      "Loss: 0.701513\n",
      "Starting epoch 658\n",
      "Loss: 0.701513\n",
      "Starting epoch 659\n",
      "Loss: 0.701513\n",
      "Starting epoch 660\n",
      "Loss: 0.701513\n",
      "Starting epoch 661\n",
      "Loss: 0.701512\n",
      "Starting epoch 662\n",
      "Loss: 0.701512\n",
      "Starting epoch 663\n",
      "Loss: 0.701512\n",
      "Starting epoch 664\n",
      "Loss: 0.701512\n",
      "Starting epoch 665\n",
      "Loss: 0.701512\n",
      "Starting epoch 666\n",
      "Loss: 0.701512\n",
      "Starting epoch 667\n",
      "Loss: 0.701512\n",
      "Starting epoch 668\n",
      "Loss: 0.701512\n",
      "Starting epoch 669\n",
      "Loss: 0.701512\n",
      "Starting epoch 670\n",
      "Loss: 0.701511\n",
      "Starting epoch 671\n",
      "Loss: 0.701511\n",
      "Starting epoch 672\n",
      "Loss: 0.701511\n",
      "Starting epoch 673\n",
      "Loss: 0.701511\n",
      "Starting epoch 674\n",
      "Loss: 0.701511\n",
      "Starting epoch 675\n",
      "Loss: 0.701511\n",
      "Starting epoch 676\n",
      "Loss: 0.701511\n",
      "Starting epoch 677\n",
      "Loss: 0.701511\n",
      "Starting epoch 678\n",
      "Loss: 0.701511\n",
      "Starting epoch 679\n",
      "Loss: 0.701511\n",
      "Starting epoch 680\n",
      "Loss: 0.701510\n",
      "Starting epoch 681\n",
      "Loss: 0.701510\n",
      "Starting epoch 682\n",
      "Loss: 0.701510\n",
      "Starting epoch 683\n",
      "Loss: 0.701510\n",
      "Starting epoch 684\n",
      "Loss: 0.701510\n",
      "Starting epoch 685\n",
      "Loss: 0.701510\n",
      "Starting epoch 686\n",
      "Loss: 0.701510\n",
      "Starting epoch 687\n",
      "Loss: 0.701509\n",
      "Starting epoch 688\n",
      "Loss: 0.701509\n",
      "Starting epoch 689\n",
      "Loss: 0.701509\n",
      "Starting epoch 690\n",
      "Loss: 0.701509\n",
      "Starting epoch 691\n",
      "Loss: 0.701509\n",
      "Starting epoch 692\n",
      "Loss: 0.701509\n",
      "Starting epoch 693\n",
      "Loss: 0.701509\n",
      "Starting epoch 694\n",
      "Loss: 0.701509\n",
      "Starting epoch 695\n",
      "Loss: 0.701509\n",
      "Starting epoch 696\n",
      "Loss: 0.701509\n",
      "Starting epoch 697\n",
      "Loss: 0.701508\n",
      "Starting epoch 698\n",
      "Loss: 0.701508\n",
      "Starting epoch 699\n",
      "Loss: 0.701508\n",
      "Starting epoch 700\n",
      "Loss: 0.701508\n",
      "Starting epoch 701\n",
      "Loss: 0.701508\n",
      "Starting epoch 702\n",
      "Loss: 0.701508\n",
      "Starting epoch 703\n",
      "Loss: 0.701508\n",
      "Starting epoch 704\n",
      "Loss: 0.701508\n",
      "Starting epoch 705\n",
      "Loss: 0.701508\n",
      "Starting epoch 706\n",
      "Loss: 0.701508\n",
      "Starting epoch 707\n",
      "Loss: 0.701507\n",
      "Starting epoch 708\n",
      "Loss: 0.701507\n",
      "Starting epoch 709\n",
      "Loss: 0.701507\n",
      "Starting epoch 710\n",
      "Loss: 0.701507\n",
      "Starting epoch 711\n",
      "Loss: 0.701507\n",
      "Starting epoch 712\n",
      "Loss: 0.701507\n",
      "Starting epoch 713\n",
      "Loss: 0.701507\n",
      "Starting epoch 714\n",
      "Loss: 0.701507\n",
      "Starting epoch 715\n",
      "Loss: 0.701507\n",
      "Starting epoch 716\n",
      "Loss: 0.701507\n",
      "Starting epoch 717\n",
      "Loss: 0.701507\n",
      "Starting epoch 718\n",
      "Loss: 0.701507\n",
      "Starting epoch 719\n",
      "Loss: 0.701507\n",
      "Starting epoch 720\n",
      "Loss: 0.701507\n",
      "Starting epoch 721\n",
      "Loss: 0.701506\n",
      "Starting epoch 722\n",
      "Loss: 0.701506\n",
      "Starting epoch 723\n",
      "Loss: 0.701506\n",
      "Starting epoch 724\n",
      "Loss: 0.701506\n",
      "Starting epoch 725\n",
      "Loss: 0.701506\n",
      "Starting epoch 726\n",
      "Loss: 0.701506\n",
      "Starting epoch 727\n",
      "Loss: 0.701506\n",
      "Starting epoch 728\n",
      "Loss: 0.701506\n",
      "Starting epoch 729\n",
      "Loss: 0.701506\n",
      "Starting epoch 730\n",
      "Loss: 0.701506\n",
      "Starting epoch 731\n",
      "Loss: 0.701506\n",
      "Starting epoch 732\n",
      "Loss: 0.701506\n",
      "Starting epoch 733\n",
      "Loss: 0.701506\n",
      "Starting epoch 734\n",
      "Loss: 0.701506\n",
      "Starting epoch 735\n",
      "Loss: 0.701506\n",
      "Starting epoch 736\n",
      "Loss: 0.701506\n",
      "Starting epoch 737\n",
      "Loss: 0.701505\n",
      "Starting epoch 738\n",
      "Loss: 0.701505\n",
      "Starting epoch 739\n",
      "Loss: 0.701505\n",
      "Starting epoch 740\n",
      "Loss: 0.701505\n",
      "Starting epoch 741\n",
      "Loss: 0.701505\n",
      "Starting epoch 742\n",
      "Loss: 0.701505\n",
      "Starting epoch 743\n",
      "Loss: 0.701505\n",
      "Starting epoch 744\n",
      "Loss: 0.701505\n",
      "Starting epoch 745\n",
      "Loss: 0.701505\n",
      "Starting epoch 746\n",
      "Loss: 0.701505\n",
      "Starting epoch 747\n",
      "Loss: 0.701505\n",
      "Starting epoch 748\n",
      "Loss: 0.701505\n",
      "Starting epoch 749\n",
      "Loss: 0.701505\n",
      "Starting epoch 750\n",
      "Loss: 0.701505\n",
      "Starting epoch 751\n",
      "Loss: 0.701504\n",
      "Starting epoch 752\n",
      "Loss: 0.701504\n",
      "Starting epoch 753\n",
      "Loss: 0.701504\n",
      "Starting epoch 754\n",
      "Loss: 0.701504\n",
      "Starting epoch 755\n",
      "Loss: 0.701504\n",
      "Starting epoch 756\n",
      "Loss: 0.701504\n",
      "Starting epoch 757\n",
      "Loss: 0.701504\n",
      "Starting epoch 758\n",
      "Loss: 0.701504\n",
      "Starting epoch 759\n",
      "Loss: 0.701504\n",
      "Starting epoch 760\n",
      "Loss: 0.701504\n",
      "Starting epoch 761\n",
      "Loss: 0.701504\n",
      "Starting epoch 762\n",
      "Loss: 0.701504\n",
      "Starting epoch 763\n",
      "Loss: 0.701504\n",
      "Starting epoch 764\n",
      "Loss: 0.701504\n",
      "Starting epoch 765\n",
      "Loss: 0.701504\n",
      "Starting epoch 766\n",
      "Loss: 0.701504\n",
      "Starting epoch 767\n",
      "Loss: 0.701504\n",
      "Starting epoch 768\n",
      "Loss: 0.701504\n",
      "Starting epoch 769\n",
      "Loss: 0.701503\n",
      "Starting epoch 770\n",
      "Loss: 0.701503\n",
      "Starting epoch 771\n",
      "Loss: 0.701503\n",
      "Starting epoch 772\n",
      "Loss: 0.701503\n",
      "Starting epoch 773\n",
      "Loss: 0.701503\n",
      "Starting epoch 774\n",
      "Loss: 0.701503\n",
      "Starting epoch 775\n",
      "Loss: 0.701503\n",
      "Starting epoch 776\n",
      "Loss: 0.701503\n",
      "Starting epoch 777\n",
      "Loss: 0.701503\n",
      "Starting epoch 778\n",
      "Loss: 0.701503\n",
      "Starting epoch 779\n",
      "Loss: 0.701503\n",
      "Starting epoch 780\n",
      "Loss: 0.701503\n",
      "Starting epoch 781\n",
      "Loss: 0.701503\n",
      "Starting epoch 782\n",
      "Loss: 0.701503\n",
      "Starting epoch 783\n",
      "Loss: 0.701503\n",
      "Starting epoch 784\n",
      "Loss: 0.701503\n",
      "Starting epoch 785\n",
      "Loss: 0.701503\n",
      "Starting epoch 786\n",
      "Loss: 0.701503\n",
      "Starting epoch 787\n",
      "Loss: 0.701503\n",
      "Starting epoch 788\n",
      "Loss: 0.701503\n",
      "Starting epoch 789\n",
      "Loss: 0.701503\n",
      "Starting epoch 790\n",
      "Loss: 0.701503\n",
      "Starting epoch 791\n",
      "Loss: 0.701503\n",
      "Starting epoch 792\n",
      "Loss: 0.701503\n",
      "Starting epoch 793\n",
      "Loss: 0.701503\n",
      "Starting epoch 794\n",
      "Loss: 0.701503\n",
      "Starting epoch 795\n",
      "Loss: 0.701502\n",
      "Starting epoch 796\n",
      "Loss: 0.701502\n",
      "Starting epoch 797\n",
      "Loss: 0.701502\n",
      "Starting epoch 798\n",
      "Loss: 0.701502\n",
      "Starting epoch 799\n",
      "Loss: 0.701502\n",
      "Starting epoch 800\n",
      "Loss: 0.701502\n",
      "Starting epoch 801\n",
      "Loss: 0.701502\n",
      "Starting epoch 802\n",
      "Loss: 0.701502\n",
      "Starting epoch 803\n",
      "Loss: 0.701502\n",
      "Starting epoch 804\n",
      "Loss: 0.701502\n",
      "Starting epoch 805\n",
      "Loss: 0.701502\n",
      "Starting epoch 806\n",
      "Loss: 0.701502\n",
      "Starting epoch 807\n",
      "Loss: 0.701502\n",
      "Starting epoch 808\n",
      "Loss: 0.701502\n",
      "Starting epoch 809\n",
      "Loss: 0.701502\n",
      "Starting epoch 810\n",
      "Loss: 0.701502\n",
      "Starting epoch 811\n",
      "Loss: 0.701502\n",
      "Starting epoch 812\n",
      "Loss: 0.701502\n",
      "Starting epoch 813\n",
      "Loss: 0.701502\n",
      "Starting epoch 814\n",
      "Loss: 0.701502\n",
      "Starting epoch 815\n",
      "Loss: 0.701502\n",
      "Starting epoch 816\n",
      "Loss: 0.701502\n",
      "Starting epoch 817\n",
      "Loss: 0.701502\n",
      "Starting epoch 818\n",
      "Loss: 0.701501\n",
      "Starting epoch 819\n",
      "Loss: 0.701501\n",
      "Starting epoch 820\n",
      "Loss: 0.701501\n",
      "Starting epoch 821\n",
      "Loss: 0.701501\n",
      "Starting epoch 822\n",
      "Loss: 0.701501\n",
      "Starting epoch 823\n",
      "Loss: 0.701501\n",
      "Starting epoch 824\n",
      "Loss: 0.701501\n",
      "Starting epoch 825\n",
      "Loss: 0.701501\n",
      "Starting epoch 826\n",
      "Loss: 0.701501\n",
      "Starting epoch 827\n",
      "Loss: 0.701501\n",
      "Starting epoch 828\n",
      "Loss: 0.701501\n",
      "Starting epoch 829\n",
      "Loss: 0.701501\n",
      "Starting epoch 830\n",
      "Loss: 0.701501\n",
      "Starting epoch 831\n",
      "Loss: 0.701501\n",
      "Starting epoch 832\n",
      "Loss: 0.701501\n",
      "Starting epoch 833\n",
      "Loss: 0.701501\n",
      "Starting epoch 834\n",
      "Loss: 0.701501\n",
      "Starting epoch 835\n",
      "Loss: 0.701501\n",
      "Starting epoch 836\n",
      "Loss: 0.701501\n",
      "Starting epoch 837\n",
      "Loss: 0.701501\n",
      "Starting epoch 838\n",
      "Loss: 0.701501\n",
      "Starting epoch 839\n",
      "Loss: 0.701501\n",
      "Starting epoch 840\n",
      "Loss: 0.701501\n",
      "Starting epoch 841\n",
      "Loss: 0.701501\n",
      "Starting epoch 842\n",
      "Loss: 0.701501\n",
      "Starting epoch 843\n",
      "Loss: 0.701501\n",
      "Starting epoch 844\n",
      "Loss: 0.701501\n",
      "Starting epoch 845\n",
      "Loss: 0.701501\n",
      "Starting epoch 846\n",
      "Loss: 0.701501\n",
      "Starting epoch 847\n",
      "Loss: 0.701501\n",
      "Starting epoch 848\n",
      "Loss: 0.701501\n",
      "Starting epoch 849\n",
      "Loss: 0.701501\n",
      "Starting epoch 850\n",
      "Loss: 0.701501\n",
      "Starting epoch 851\n",
      "Loss: 0.701500\n",
      "Starting epoch 852\n",
      "Loss: 0.701500\n",
      "Starting epoch 853\n",
      "Loss: 0.701500\n",
      "Starting epoch 854\n",
      "Loss: 0.701500\n",
      "Starting epoch 855\n",
      "Loss: 0.701500\n",
      "Starting epoch 856\n",
      "Loss: 0.701500\n",
      "Starting epoch 857\n",
      "Loss: 0.701500\n",
      "Starting epoch 858\n",
      "Loss: 0.701500\n",
      "Starting epoch 859\n",
      "Loss: 0.701500\n",
      "Starting epoch 860\n",
      "Loss: 0.701500\n",
      "Starting epoch 861\n",
      "Loss: 0.701500\n",
      "Starting epoch 862\n",
      "Loss: 0.701500\n",
      "Starting epoch 863\n",
      "Loss: 0.701500\n",
      "Starting epoch 864\n",
      "Loss: 0.701500\n",
      "Starting epoch 865\n",
      "Loss: 0.701500\n",
      "Starting epoch 866\n",
      "Loss: 0.701500\n",
      "Starting epoch 867\n",
      "Loss: 0.701500\n",
      "Starting epoch 868\n",
      "Loss: 0.701500\n",
      "Starting epoch 869\n",
      "Loss: 0.701500\n",
      "Starting epoch 870\n",
      "Loss: 0.701500\n",
      "Starting epoch 871\n",
      "Loss: 0.701500\n",
      "Starting epoch 872\n",
      "Loss: 0.701500\n",
      "Starting epoch 873\n",
      "Loss: 0.701500\n",
      "Starting epoch 874\n",
      "Loss: 0.701500\n",
      "Starting epoch 875\n",
      "Loss: 0.701500\n",
      "Starting epoch 876\n",
      "Loss: 0.701499\n",
      "Starting epoch 877\n",
      "Loss: 0.701499\n",
      "Starting epoch 878\n",
      "Loss: 0.701499\n",
      "Starting epoch 879\n",
      "Loss: 0.701499\n",
      "Starting epoch 880\n",
      "Loss: 0.701499\n",
      "Starting epoch 881\n",
      "Loss: 0.701499\n",
      "Starting epoch 882\n",
      "Loss: 0.701499\n",
      "Starting epoch 883\n",
      "Loss: 0.701499\n",
      "Starting epoch 884\n",
      "Loss: 0.701499\n",
      "Starting epoch 885\n",
      "Loss: 0.701499\n",
      "Starting epoch 886\n",
      "Loss: 0.701499\n",
      "Starting epoch 887\n",
      "Loss: 0.701499\n",
      "Starting epoch 888\n",
      "Loss: 0.701499\n",
      "Starting epoch 889\n",
      "Loss: 0.701499\n",
      "Starting epoch 890\n",
      "Loss: 0.701499\n",
      "Starting epoch 891\n",
      "Loss: 0.701499\n",
      "Starting epoch 892\n",
      "Loss: 0.701499\n",
      "Starting epoch 893\n",
      "Loss: 0.701499\n",
      "Starting epoch 894\n",
      "Loss: 0.701499\n",
      "Starting epoch 895\n",
      "Loss: 0.701499\n",
      "Starting epoch 896\n",
      "Loss: 0.701499\n",
      "Starting epoch 897\n",
      "Loss: 0.701499\n",
      "Starting epoch 898\n",
      "Loss: 0.701499\n",
      "Starting epoch 899\n",
      "Loss: 0.701499\n",
      "Starting epoch 900\n",
      "Loss: 0.701499\n",
      "Starting epoch 901\n",
      "Loss: 0.701499\n",
      "Starting epoch 902\n",
      "Loss: 0.701499\n",
      "Starting epoch 903\n",
      "Loss: 0.701498\n",
      "Starting epoch 904\n",
      "Loss: 0.701498\n",
      "Starting epoch 905\n",
      "Loss: 0.701498\n",
      "Starting epoch 906\n",
      "Loss: 0.701498\n",
      "Starting epoch 907\n",
      "Loss: 0.701498\n",
      "Starting epoch 908\n",
      "Loss: 0.701498\n",
      "Starting epoch 909\n",
      "Loss: 0.701498\n",
      "Starting epoch 910\n",
      "Loss: 0.701498\n",
      "Starting epoch 911\n",
      "Loss: 0.701498\n",
      "Starting epoch 912\n",
      "Loss: 0.701498\n",
      "Starting epoch 913\n",
      "Loss: 0.701498\n",
      "Starting epoch 914\n",
      "Loss: 0.701498\n",
      "Starting epoch 915\n",
      "Loss: 0.701498\n",
      "Starting epoch 916\n",
      "Loss: 0.701498\n",
      "Starting epoch 917\n",
      "Loss: 0.701498\n",
      "Starting epoch 918\n",
      "Loss: 0.701498\n",
      "Starting epoch 919\n",
      "Loss: 0.701498\n",
      "Starting epoch 920\n",
      "Loss: 0.701498\n",
      "Starting epoch 921\n",
      "Loss: 0.701498\n",
      "Starting epoch 922\n",
      "Loss: 0.701498\n",
      "Starting epoch 923\n",
      "Loss: 0.701498\n",
      "Starting epoch 924\n",
      "Loss: 0.701498\n",
      "Starting epoch 925\n",
      "Loss: 0.701498\n",
      "Starting epoch 926\n",
      "Loss: 0.701498\n",
      "Starting epoch 927\n",
      "Loss: 0.701498\n",
      "Starting epoch 928\n",
      "Loss: 0.701498\n",
      "Starting epoch 929\n",
      "Loss: 0.701498\n",
      "Starting epoch 930\n",
      "Loss: 0.701497\n",
      "Starting epoch 931\n",
      "Loss: 0.701497\n",
      "Starting epoch 932\n",
      "Loss: 0.701497\n",
      "Starting epoch 933\n",
      "Loss: 0.701497\n",
      "Starting epoch 934\n",
      "Loss: 0.701497\n",
      "Starting epoch 935\n",
      "Loss: 0.701497\n",
      "Starting epoch 936\n",
      "Loss: 0.701497\n",
      "Starting epoch 937\n",
      "Loss: 0.701497\n",
      "Starting epoch 938\n",
      "Loss: 0.701497\n",
      "Starting epoch 939\n",
      "Loss: 0.701497\n",
      "Starting epoch 940\n",
      "Loss: 0.701497\n",
      "Starting epoch 941\n",
      "Loss: 0.701497\n",
      "Starting epoch 942\n",
      "Loss: 0.701497\n",
      "Starting epoch 943\n",
      "Loss: 0.701497\n",
      "Starting epoch 944\n",
      "Loss: 0.701497\n",
      "Starting epoch 945\n",
      "Loss: 0.701497\n",
      "Starting epoch 946\n",
      "Loss: 0.701497\n",
      "Starting epoch 947\n",
      "Loss: 0.701497\n",
      "Starting epoch 948\n",
      "Loss: 0.701497\n",
      "Starting epoch 949\n",
      "Loss: 0.701497\n",
      "Starting epoch 950\n",
      "Loss: 0.701497\n",
      "Starting epoch 951\n",
      "Loss: 0.701497\n",
      "Starting epoch 952\n",
      "Loss: 0.701497\n",
      "Starting epoch 953\n",
      "Loss: 0.701497\n",
      "Starting epoch 954\n",
      "Loss: 0.701497\n",
      "Starting epoch 955\n",
      "Loss: 0.701497\n",
      "Starting epoch 956\n",
      "Loss: 0.701497\n",
      "Starting epoch 957\n",
      "Loss: 0.701496\n",
      "Starting epoch 958\n",
      "Loss: 0.701497\n",
      "Starting epoch 959\n",
      "Loss: 0.701496\n",
      "Starting epoch 960\n",
      "Loss: 0.701496\n",
      "Starting epoch 961\n",
      "Loss: 0.701496\n",
      "Starting epoch 962\n",
      "Loss: 0.701496\n",
      "Starting epoch 963\n",
      "Loss: 0.701496\n",
      "Starting epoch 964\n",
      "Loss: 0.701496\n",
      "Starting epoch 965\n",
      "Loss: 0.701496\n",
      "Starting epoch 966\n",
      "Loss: 0.701496\n",
      "Starting epoch 967\n",
      "Loss: 0.701496\n",
      "Starting epoch 968\n",
      "Loss: 0.701496\n",
      "Starting epoch 969\n",
      "Loss: 0.701496\n",
      "Starting epoch 970\n",
      "Loss: 0.701496\n",
      "Starting epoch 971\n",
      "Loss: 0.701496\n",
      "Starting epoch 972\n",
      "Loss: 0.701496\n",
      "Starting epoch 973\n",
      "Loss: 0.701496\n",
      "Starting epoch 974\n",
      "Loss: 0.701496\n",
      "Starting epoch 975\n",
      "Loss: 0.701496\n",
      "Starting epoch 976\n",
      "Loss: 0.701495\n",
      "Starting epoch 977\n",
      "Loss: 0.701495\n",
      "Starting epoch 978\n",
      "Loss: 0.701495\n",
      "Starting epoch 979\n",
      "Loss: 0.701495\n",
      "Starting epoch 980\n",
      "Loss: 0.701495\n",
      "Starting epoch 981\n",
      "Loss: 0.701495\n",
      "Starting epoch 982\n",
      "Loss: 0.701495\n",
      "Starting epoch 983\n",
      "Loss: 0.701495\n",
      "Starting epoch 984\n",
      "Loss: 0.701495\n",
      "Starting epoch 985\n",
      "Loss: 0.701495\n",
      "Starting epoch 986\n",
      "Loss: 0.701495\n",
      "Starting epoch 987\n",
      "Loss: 0.701495\n",
      "Starting epoch 988\n",
      "Loss: 0.701495\n",
      "Starting epoch 989\n",
      "Loss: 0.701495\n",
      "Starting epoch 990\n",
      "Loss: 0.701495\n",
      "Starting epoch 991\n",
      "Loss: 0.701495\n",
      "Starting epoch 992\n",
      "Loss: 0.701495\n",
      "Starting epoch 993\n",
      "Loss: 0.701495\n",
      "Starting epoch 994\n",
      "Loss: 0.701495\n",
      "Starting epoch 995\n",
      "Loss: 0.701495\n",
      "Starting epoch 996\n",
      "Loss: 0.701495\n",
      "Starting epoch 997\n",
      "Loss: 0.701495\n",
      "Starting epoch 998\n",
      "Loss: 0.701495\n",
      "Starting epoch 999\n",
      "Loss: 0.701494\n"
     ]
    }
   ],
   "source": [
    "recording = np.load('preprocessed_data_sync/recording318.npy', allow_pickle=True)\n",
    "instruments_np = np.load('preprocessed_data_sync/instruments318.npy', allow_pickle=True)\n",
    "\n",
    "inst = 1\n",
    "\n",
    "max_seq_length = 200\n",
    "\n",
    "history = torch.tensor(recording[inst, :max_seq_length]).view(1, max_seq_length, 1, -1)\n",
    "max_instruments = history.shape[0]\n",
    "mask = torch.zeros((max_instruments, max_seq_length, 1), dtype=torch.bool)\n",
    "instruments = torch.tensor(instrument_numbers.index(instruments_np[inst]), dtype=torch.long).view(1, 1)\n",
    "\n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "bce = torch.nn.BCEWithLogitsLoss(reduction='none', pos_weight = 2*torch.ones(2*num_notes))\n",
    "mse = torch.nn.MSELoss(reduction='none')\n",
    "epochs = 1000\n",
    "train_losses = np.zeros(epochs)\n",
    "\n",
    "targets = history[:, 1:]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))    \n",
    "    out = model(history[:, :-1], mask[:, :-1], instruments)\n",
    "                \n",
    "    note_losses = bce(out[:, :, :, :2*num_notes], targets[:, :, :, :2*num_notes])\n",
    "    \n",
    "    # Only apply dynamics losses to notes that are being turned on\n",
    "    dynamics_losses = targets[:, :, :, :num_notes]*mse(torch.sigmoid(out[:, :, :, 2*num_notes:]), targets[:, :, :, 2*num_notes:])\n",
    "    \n",
    "    loss_mask = torch.logical_not(mask[:, :-1])\n",
    "    loss = note_losses[loss_mask].mean() + dynamics_losses[loss_mask].mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses[epoch] = loss.data\n",
    "    print('Loss: %f' %(loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'overfit_single_instrument_sync.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa4e435aad0>]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199\n"
     ]
    }
   ],
   "source": [
    "# Sample from model\n",
    "gen_history = torch.tensor(recording[inst, 0]).view(1, 1, 1, -1)\n",
    "mask = torch.zeros((1, 1, 1), dtype=torch.bool)\n",
    "instruments = torch.tensor(instrument_numbers.index(instruments_np[inst]), dtype=torch.long).view(1, 1)\n",
    "model.eval() # Turns off the dropout for evaluation. Need to do this to get repeatable evaluation outputs\n",
    "\n",
    "max_instruments = history.shape[0]\n",
    "\n",
    "# Move forward in time\n",
    "wrong_cnt = 0\n",
    "for t in range(1, max_seq_length):\n",
    "    #out = model(gen_history, mask, instruments)\n",
    "    out = torch.sigmoid(model(history[:, :t], mask, instruments))\n",
    "    \n",
    "    #next_actions = torch.bernoulli(out[0, -1, :, :2*num_notes]).view(1, 1, 1, -1)\n",
    "    next_actions = torch.round(out[0, -1, :, :2*num_notes]).view(1, 1, 1, -1)\n",
    "    next_vels = out[0, -1, :, 2*num_notes:].view(1, 1, 1, -1)\n",
    "    append = torch.cat((next_actions, next_vels), dim=3)\n",
    "    gen_history = torch.cat((gen_history, append), dim=1)\n",
    "\n",
    "    if torch.any(gen_history[:, t, :, :2*num_notes] != history[:, t, :, :2*num_notes]):\n",
    "        wrong_cnt += 1\n",
    "    \n",
    "    mask = torch.cat((mask, torch.zeros((1, 1, 1), dtype=torch.bool)), dim=1)\n",
    "\n",
    "print(wrong_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 0.6816, 0.6632, 0.6835, 0.5829, 0.6464,\n",
      "         0.5615, 0.6069, 0.6866, 0.6296, 0.6247, 0.6693, 0.6123, 0.6955, 0.6195,\n",
      "         0.6392, 0.6315, 0.5678, 0.6499, 0.6882, 0.6227, 0.6938, 0.6426, 0.5869,\n",
      "         0.5691, 0.6281, 0.6814, 0.6469, 0.5832, 0.5971, 0.5493, 0.5964, 0.6005,\n",
      "         0.6122, 0.5808, 0.6121, 0.5733, 0.6204, 0.6332, 0.5801, 0.5871, 0.5814,\n",
      "         0.6008, 0.6411, 0.6438, 0.6072, 0.6291, 0.5700, 0.6587, 0.5640, 0.6299,\n",
      "         0.6299, 0.6130, 0.6299, 0.6433, 0.6299, 0.6121, 0.6299, 0.6299, 0.6711,\n",
      "         0.6299, 0.6099, 0.6299, 0.6299, 0.6526, 0.6299, 0.6393, 0.6280, 0.6360,\n",
      "         0.6644, 0.6313, 0.6242, 0.5461, 0.6371, 0.6379, 0.5799, 0.5858, 0.6449,\n",
      "         0.6610, 0.6029, 0.5965, 0.6266, 0.6001, 0.6215, 0.6546, 0.6175, 0.6415,\n",
      "         0.6320, 0.6918, 0.6279, 0.5958, 0.6479, 0.5830, 0.6279, 0.6243, 0.6491,\n",
      "         0.5734, 0.6784, 0.6438, 0.5481, 0.6401, 0.5939, 0.5608, 0.6627, 0.6111,\n",
      "         0.6232, 0.6037, 0.6005, 0.5865, 0.6899, 0.6365, 0.6979, 0.5563, 0.5707,\n",
      "         0.6095, 0.5940, 0.6261, 0.5988, 0.5570, 0.6398, 0.5638, 0.5746, 0.5636,\n",
      "         0.6025, 0.6511, 0.5916, 0.6042, 0.6025, 0.6781]],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(gen_history[0, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test_history.npy', gen_history.detach().view(max_instruments, max_seq_length, -1).numpy())\n",
    "np.save('test_instruments.npy', np.array([instrument_numbers[instruments[0]]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model to overfit to two instruments' parts in a single song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('overfit_two_instruments.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = np.load('preprocessed_data_sync/recording318.npy', allow_pickle=True)\n",
    "instruments_np = np.load('preprocessed_data_sync/instruments318.npy', allow_pickle=True)\n",
    "\n",
    "max_seq_length = 200\n",
    "\n",
    "history = torch.tensor(recording[:, :max_seq_length]).view(2, max_seq_length, 1, -1)\n",
    "max_instruments = history.shape[0]\n",
    "mask = torch.zeros((max_instruments, max_seq_length, 1), dtype=torch.bool)\n",
    "instruments = torch.tensor([instrument_numbers.index(i) for i in instruments_np], dtype=torch.long).view(max_instruments, 1)\n",
    "\n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "bce = torch.nn.BCELoss(reduction='none')\n",
    "mse = torch.nn.MSELoss(reduction='none')\n",
    "epochs = 300\n",
    "train_losses = np.zeros(epochs)\n",
    "\n",
    "targets = history[:, 1:]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))    \n",
    "    out = model(history[:, :-1], mask[:, :-1], instruments)\n",
    "                \n",
    "    note_losses = bce(out[:, :, :, :2*num_notes], targets[:, :, :, :2*num_notes])\n",
    "    \n",
    "    # Only apply dynamics losses to noted that are being turned on\n",
    "    dynamics_losses = targets[:, :, :, :num_notes]*mse(out[:, :, :, 2*num_notes:], targets[:, :, :, 2*num_notes:])\n",
    "    \n",
    "    loss_mask = torch.logical_not(mask[:, :-1])\n",
    "    loss = note_losses[loss_mask].mean() + dynamics_losses[loss_mask].mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses[epoch] = loss.data\n",
    "    print('Loss: %f' %(loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'overfit_two_instruments.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9a84317290>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check if each instrument can reconstruct its part, given the other instrument's part\n",
    "model.eval() # Turns off the dropout for evaluation. Need to do this to get repeatable evaluation outputs\n",
    "\n",
    "for inst in range(max_instruments):\n",
    "    gen_history = history.clone()\n",
    "\n",
    "    # Move forward in time\n",
    "    wrong_cnt = 0\n",
    "    for t in range(1, max_seq_length):\n",
    "        input_mask = mask.clone()\n",
    "        input_mask[t:] = True\n",
    "        logits = model(gen_history, input_mask, instruments, inst)\n",
    "        probs = torch.nn.functional.softmax(logits[t - 1], dim=1)\n",
    "        gen_history[t, inst, 0] = torch.multinomial(probs, 1)\n",
    "        if torch.argmax(probs.flatten()) != history[t, inst].flatten():\n",
    "            wrong_cnt += 1\n",
    "            print(torch.topk(probs.flatten(), 10))\n",
    "            print(history[t, inst])\n",
    "\n",
    "    print(wrong_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = np.load('preprocessed_data/recording318.npy', allow_pickle=True)\n",
    "instruments_np = np.load('preprocessed_data/instruments318.npy', allow_pickle=True)\n",
    "\n",
    "max_seq_length = 1000\n",
    "max_instruments = instruments_np.shape[0]\n",
    "batch_size = 1\n",
    "\n",
    "history = torch.zeros((max_seq_length, max_instruments, batch_size), dtype=torch.long)\n",
    "mask = torch.ones(history.shape, dtype=torch.bool)\n",
    "instruments = torch.tensor([instrument_numbers.index(instruments_np[i]) for i in range(max_instruments)], dtype=torch.long).view(1, -1, 1)\n",
    "\n",
    "for inst in range(max_instruments):\n",
    "    history[:max_seq_length, inst, 0] = torch.tensor(recording[inst][:max_seq_length], dtype=torch.long)\n",
    "    mask[:max_seq_length, inst, 0] = False\n",
    "\n",
    "# Check if the instruments can jointly reconstruct the piece\n",
    "gen_history = history.clone()\n",
    "\n",
    "model.eval() # Turns off the dropout for evaluation. Need to do this to get repeatable evaluation outputs\n",
    "\n",
    "wrong_cnt = 0\n",
    "\n",
    "for t in range(1, max_seq_length):\n",
    "    input_mask = mask.clone()\n",
    "    input_mask[t:] = True\n",
    "    for inst in range(max_instruments):\n",
    "        logits = model(gen_history, input_mask, instruments, inst)\n",
    "        probs = torch.nn.functional.softmax(logits[t - 1], dim=1)\n",
    "        gen_history[t, inst, 0] = torch.multinomial(probs, 1)\n",
    "        if torch.argmax(probs.flatten()) != history[t, inst].flatten():\n",
    "            wrong_cnt += 1\n",
    "           \n",
    "    if t%50 == 0:\n",
    "        print(t)\n",
    "\n",
    "print(wrong_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class MIDIDataset(torch.utils.data.Dataset):\n",
    "    # CONSTRUCTOR: creates a list of recording files and a list\n",
    "    # of instrument files in root_dir. Assumes that the directory\n",
    "    # contains recording0.npy to recordingM.npy,\n",
    "    # as well as instruments0.npy to instrumentsM.npy\n",
    "    # ARGUMENTS\n",
    "    # root_dir: the directory to search\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        files = os.listdir(root_dir)\n",
    "        self.recordings = []\n",
    "        self.instrument_files = []\n",
    "        for file in files:\n",
    "            if 'recording' in file:\n",
    "                self.recordings.append(os.path.join(root_dir, file))\n",
    "            elif 'instruments' in file:\n",
    "                self.instrument_files.append(os.path.join(root_dir, file))\n",
    "                \n",
    "        assert(len(self.recordings) == len(self.instrument_files))\n",
    "        self.recordings.sort()\n",
    "        self.instrument_files.sort()\n",
    "        self.transform = transform\n",
    "\n",
    "    # __len__\n",
    "    # RETURN: the number of recording files in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.recordings)\n",
    "\n",
    "    # __getitem__\n",
    "    # ARGUMENTS\n",
    "    # idx: indicates which file to get\n",
    "    # RETURN: an instance with keys 'instruments', 'history'\n",
    "    # instance['history'] is a numpy array of message sequences for each instrument\n",
    "    # instance['instruments'] a numpy array of instrument numbers\n",
    "    def __getitem__(self, idx):\n",
    "        instance = {'history': np.load(self.recordings[idx], allow_pickle=True), \\\n",
    "                    'instruments': np.load(self.instrument_files[idx], allow_pickle=True)}\n",
    "        \n",
    "        assert(len(instance['history']) == len(instance['instruments']))\n",
    "        \n",
    "        if self.transform:\n",
    "            instance = self.transform(instance)\n",
    "            \n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn: takes a list of samples from the dataset and turns them into a batch.\n",
    "# ARGUMENTS\n",
    "# batch: a list of dictionaries\n",
    "# RETURN: a sample with keys 'history', 'instruments', and 'mask'\n",
    "# sample['history']: an LxNxB tensor containing messages\n",
    "# sample['instruments']: a 1xNxB tensor containing instrument numbers\n",
    "# sample['mask']: an LxNxB tensor containing False where a message is\n",
    "# valid, and True where it isn't (accounts for variable length sequences\n",
    "# and zero padding)\n",
    "def collate_fn(batch):\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    # We size our tensors to accomodate the longest sequence and the largest ensemble\n",
    "    max_instruments = max([len(instance['history']) for instance in batch])\n",
    "    min_len = min([min([seq.shape[0] for seq in instance['history']]) for instance in batch])\n",
    "\n",
    "    sample = {'history': torch.ones((min_len, max_instruments, batch_size), dtype=torch.long), \\\n",
    "              'instruments': torch.zeros((1, max_instruments, batch_size), dtype=torch.long), \\\n",
    "              'mask': torch.ones((min_len, max_instruments, batch_size), dtype=torch.bool)}\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        instrument_idx = [instrument_numbers.index(inst) for inst in batch[b]['instruments']]\n",
    "        sample['instruments'][0, :len(instrument_idx), b] = torch.tensor(instrument_idx, dtype=torch.long)\n",
    "        \n",
    "        for inst_idx in range(len(batch[b]['history'])):\n",
    "            sample['history'][:min_len, inst_idx, b] = torch.tensor(batch[b]['history'][inst_idx][:min_len], dtype=torch.long)\n",
    "            sample['mask'][:min_len, inst_idx, b] = False\n",
    "            \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_size = 512\n",
    "heads = 8\n",
    "attention_layers=  6\n",
    "embed_dim = 256\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleTransformer(message_dim, embed_dim, len(instrument_numbers), heads, attention_layers, ff_size)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "dataset = MIDIDataset('preprocessed_data')\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "train_losses = np.zeros(epochs)\n",
    "test_losses = np.zeros(epochs) # TODO: train/test split. Can we do this with Dataloader?\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    for b, batch in enumerate(dataloader):\n",
    "        print('Starting iteration %d' %(b))\n",
    "        max_seq_length = batch['history'].shape[0]\n",
    "        num_targets = max_seq_length - 1 # Messages start from t = 0, but we start generating at t = 1\n",
    "        max_instruments = batch['history'].shape[1]\n",
    "        loss = torch.tensor([0])\n",
    "        for inst in range(max_instruments):         \n",
    "            mask = batch['mask']\n",
    "            \n",
    "            logits = model(batch['history'][:-1], mask[:-1], batch['instruments'], inst)\n",
    "            logits = logits.view(-1, message_dim)\n",
    "            target_messages = batch['history'][1:, inst].flatten()\n",
    "            output_mask = torch.logical_not(mask[1:, inst].flatten())\n",
    "            loss = loss + loss_fn(logits[output_mask], target_messages[output_mask])\n",
    "        \n",
    "        loss /= max_instruments\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.save(model.state_dict(), 'trained_models/epoch' + str(epoch) + '.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('trained_models/epoch4.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # Disable dropout to make results repeatable\n",
    "\n",
    "time_steps = 5000 # How many time steps do we sample?\n",
    "\n",
    "max_instruments = 3\n",
    "\n",
    "# Piano, violin, viola\n",
    "instruments = torch.tensor([0, 2, 3]).view(1, max_instruments, 1)\n",
    "\n",
    "# Suppose they all start with the same velocity message\n",
    "# TODO: should we have SOS and EOS tokens like in NLP?\n",
    "gen_history = 24*torch.ones((1, max_instruments, 1), dtype=torch.long)\n",
    "mask = torch.zeros((1, 1, 1), dtype=torch.bool)\n",
    "\n",
    "for t in range(1, time_steps):\n",
    "    # Sanity check\n",
    "    if t%100 == 0:\n",
    "        print(t)\n",
    "    next_messages = torch.zeros((1, max_instruments, 1), dtype=torch.long)\n",
    "    for inst in range(max_instruments):\n",
    "        logits = model(gen_history, mask.expand(t, max_instruments, -1), instruments, inst)\n",
    "        probs = torch.nn.functional.softmax(logits[t - 1], dim=1)\n",
    "        next_messages[0, inst, 0] = torch.multinomial(probs, 1)\n",
    "    \n",
    "    gen_history = torch.cat((gen_history, next_messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_recording = np.array([0 for i in range(max_instruments)], dtype='object')\n",
    "for i in range(max_instruments):\n",
    "    gen_recording[i] = gen_history[:, i].flatten().numpy()\n",
    "    \n",
    "gen_instruments = np.array([instrument_numbers[instruments[0, i, 0].item()] for i in range(max_instruments)])\n",
    "np.save('gen_recording.npy', gen_recording)\n",
    "np.save('gen_instruments.npy', gen_instruments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
