{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_notes = 128\n",
    "action_dim = 3*num_notes\n",
    "instrument_numbers = [0, 6, 40, 41, 42, 43, 45, 60, 68, 70, 71, 73]\n",
    "num_instruments = len(instrument_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Transformer definition\n",
    "Uses absolute position representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://pytorch.org/tutorials/beginner/transformer_tutorial.html.\n",
    "# Only change is the view/expand in forward (accounts for multiple instruments and batches)\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.shape[1], :].view(1, x.shape[1], 1, -1).expand(x.shape[0], -1, x.shape[2], -1)\n",
    "        return self.dropout(x)\n",
    "        \n",
    "\n",
    "# EnsembleTransformer: takes a history of actions \n",
    "# for instruments in an ensemble and generates distributions for the next actions\n",
    "class EnsembleTransformer(torch.nn.Module):\n",
    "    # CONSTRUCTOR\n",
    "    # ARGUMENTS\n",
    "    # action_dim: dimension of a MIDI message\n",
    "    # embed_dim: dimension of message embedding\n",
    "    # num_instruments: number of instrument labels\n",
    "    # heads: number of attention heads\n",
    "    # attention_layers: number of attention layers\n",
    "    # ff_size: size of the feedforward output at the end of the decoder\n",
    "    # chunk_size: long sequences will be processed in chunks of chunk_size\n",
    "    def __init__(self, action_dim, embed_dim, num_instruments, heads, attention_layers, ff_size, chunk_size):\n",
    "        super(EnsembleTransformer, self).__init__()\n",
    "        \n",
    "        self.action_dim = action_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.chunk_size = chunk_size\n",
    "        \n",
    "        # We add the tanhed instrument embedding to each input message \n",
    "        # (this is the global conditioning idea from DeepJ, which comes from WaveNet)\n",
    "        self.i_embedding = torch.nn.Embedding(num_instruments, embed_dim)\n",
    "        \n",
    "        self.position_encoding = PositionalEncoding(embed_dim)\n",
    "        \n",
    "        self.embedding = torch.nn.Linear(action_dim, embed_dim)\n",
    "        \n",
    "        # A decoder is used to transform the history of the instrument we're generating\n",
    "        # music for, then combine this with the other histories to generate the next action\n",
    "        decoder_layer = torch.nn.TransformerDecoderLayer(embed_dim, heads, ff_size)\n",
    "        self.decoder = torch.nn.TransformerDecoder(decoder_layer, attention_layers)\n",
    "        \n",
    "        self.logits = torch.nn.Linear(embed_dim, action_dim)\n",
    "        \n",
    "    \n",
    "    # forward: generates probabilities for the next MIDI actions\n",
    "    # given the action history of the ensemble\n",
    "    # ARGUMENTS\n",
    "    # history: an NxLxBxA tensor, where N is the max number of instruments in the batch,\n",
    "    # L is the length of the longest history in the batch, B is the batch size,\n",
    "    # and A is the number of actions\n",
    "    # mask: an NxLxB tensor, containing True where an action or instrument doesn't exist\n",
    "    # instruments: a NxB tensor indicating the instrument numbers for each batch\n",
    "    # RETURN: an NxLxBxA tensor out. out[:, :, :, :2*num_notes] represents the Bernoulli probabilities\n",
    "    # for note-on and note-off actions. out[:, :, :, 2*num_notes:] represents the velocity of each note\n",
    "    # assuming it gets turned on. Note that to get the actual values you'll need to take a sigmoid\n",
    "    def forward(self, history, mask, instruments):\n",
    "        N = history.shape[0] # max instruments\n",
    "        L = history.shape[1] # longest length\n",
    "        B = history.shape[2] # batch size\n",
    "        A = history.shape[3]\n",
    "        assert(instruments.shape == (N, B))\n",
    "        assert(mask.shape == (N, L, B))\n",
    "        assert(A == self.action_dim)\n",
    "        \n",
    "        inputs = self.embedding(history) + torch.tanh(self.i_embedding(instruments)).unsqueeze(1).expand(-1, L, -1, -1)\n",
    "        \n",
    "        decoding = torch.zeros((N, L, B, embed_dim))\n",
    "        \n",
    "        for start in range(0, L, self.chunk_size):\n",
    "            end = min(start + chunk_size, L)\n",
    "            size = end - start\n",
    "            \n",
    "            chunk = self.position_encoding(inputs[:, start:end])\n",
    "            \n",
    "            for inst in range(N):\n",
    "                memory_idx = [i for i in range(N) if i != inst]\n",
    "                \n",
    "                tgt_key_padding_mask = mask[inst, start:end].transpose(0, 1)\n",
    "\n",
    "                tgt_mask = torch.triu(torch.ones((size, size), dtype=torch.bool))\n",
    "                tgt_mask.fill_diagonal_(False)\n",
    "\n",
    "                if N == 1:\n",
    "                    memory = torch.zeros((1, B, self.embed_dim))\n",
    "                    memory_key_padding_mask = None\n",
    "                    memory_mask = None\n",
    "                else:\n",
    "                    memory = chunk[memory_idx].view(-1, B, self.embed_dim)\n",
    "                    memory_key_padding_mask = mask[memory_idx, start:end].view(-1, B).transpose(0, 1)\n",
    "                    memory_mask = tgt_mask.repeat(1, N - 1)\n",
    "\n",
    "                decoding[inst, start:end] = self.decoder(chunk[inst], \\\n",
    "                                                         memory, \\\n",
    "                                                         tgt_mask=tgt_mask, \\\n",
    "                                                         memory_mask=memory_mask, \\\n",
    "                                                         tgt_key_padding_mask=tgt_key_padding_mask, \\\n",
    "                                                         memory_key_padding_mask=memory_key_padding_mask)\n",
    "\n",
    "        return self.logits(decoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests for baseline transformer\n",
    "We train with model.eval() to disable dropout, since these tests try to get the model to overfit to a small sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model to overfit to a single instrument's part in a single song (only the first 100 time steps). Tests decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_size = 512\n",
    "heads = 8\n",
    "attention_layers=  6\n",
    "embed_dim = 256\n",
    "chunk_size = 200\n",
    "\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleTransformer(action_dim, embed_dim, num_instruments, heads, attention_layers, ff_size, chunk_size)\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "    \n",
    "model.eval() # Training with eval just to see if we can overfit without dropout\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('overfit_single_instrument.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recording = np.load('preprocessed_data_sync/recording318.npy', allow_pickle=True)\n",
    "instruments_np = np.load('preprocessed_data_sync/instruments318.npy', allow_pickle=True)\n",
    "\n",
    "inst = 1\n",
    "\n",
    "max_seq_length = 200\n",
    "\n",
    "history = torch.tensor(recording[inst, :max_seq_length]).view(1, max_seq_length, 1, -1)\n",
    "max_instruments = history.shape[0]\n",
    "mask = torch.zeros((max_instruments, max_seq_length, 1), dtype=torch.bool)\n",
    "instruments = torch.tensor(instrument_numbers.index(instruments_np[inst]), dtype=torch.long).view(1, 1)\n",
    "\n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "bce = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "mse = torch.nn.MSELoss(reduction='none')\n",
    "epochs = 1000\n",
    "train_losses = np.zeros(epochs)\n",
    "\n",
    "targets = history[:, 1:]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))    \n",
    "    out = model(history[:, :-1], mask[:, :-1], instruments)\n",
    "                \n",
    "    note_losses = bce(out[:, :, :, :2*num_notes], targets[:, :, :, :2*num_notes])\n",
    "    \n",
    "    # Only apply dynamics losses to notes that are being turned on\n",
    "    dynamics_losses = targets[:, :, :, :num_notes]*mse(torch.sigmoid(out[:, :, :, 2*num_notes:]), targets[:, :, :, 2*num_notes:])\n",
    "    \n",
    "    loss_mask = torch.logical_not(mask[:, :-1])\n",
    "    loss = note_losses[loss_mask].mean() + dynamics_losses[loss_mask].mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses[epoch] = loss.data\n",
    "    print('Loss: %f' %(loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'overfit_single_instrument_sync.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa4e435aad0>]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from model\n",
    "gen_history = torch.tensor(recording[inst, 0]).view(1, 1, 1, -1)\n",
    "mask = torch.zeros((1, 1, 1), dtype=torch.bool)\n",
    "instruments = torch.tensor(instrument_numbers.index(instruments_np[inst]), dtype=torch.long).view(1, 1)\n",
    "model.eval() # Turns off the dropout for evaluation. Need to do this to get repeatable evaluation outputs\n",
    "\n",
    "max_instruments = history.shape[0]\n",
    "\n",
    "# Move forward in time\n",
    "wrong_cnt = 0\n",
    "for t in range(1, max_seq_length):\n",
    "    #out = model(gen_history, mask, instruments)\n",
    "    out = torch.sigmoid(model(history[:, :t], mask, instruments))\n",
    "    \n",
    "    #next_actions = torch.bernoulli(out[0, -1, :, :2*num_notes]).view(1, 1, 1, -1)\n",
    "    next_actions = torch.round(out[0, -1, :, :2*num_notes]).view(1, 1, 1, -1)\n",
    "    next_vels = out[0, -1, :, 2*num_notes:].view(1, 1, 1, -1)\n",
    "    append = torch.cat((next_actions, next_vels), dim=3)\n",
    "    gen_history = torch.cat((gen_history, append), dim=1)\n",
    "    \n",
    "    if torch.any(gen_history[:, t, :, :2*num_notes] != history[:, t, :, :2*num_notes]):\n",
    "        wrong_cnt += 1\n",
    "    \n",
    "    mask = torch.cat((mask, torch.zeros((1, 1, 1), dtype=torch.bool)), dim=1)\n",
    "\n",
    "print(wrong_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test_history.npy', gen_history.detach().view(max_instruments, max_seq_length, -1).numpy())\n",
    "np.save('test_instruments.npy', np.array([instrument_numbers[instruments[0]]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class MIDIDataset(torch.utils.data.Dataset):\n",
    "    # CONSTRUCTOR: creates a list of recording files and a list\n",
    "    # of instrument files in root_dir. Assumes that the directory\n",
    "    # contains recording0.npy to recordingM.npy,\n",
    "    # as well as instruments0.npy to instrumentsM.npy\n",
    "    # ARGUMENTS\n",
    "    # root_dir: the directory to search\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        files = os.listdir(root_dir)\n",
    "        self.recordings = []\n",
    "        self.instrument_files = []\n",
    "        for file in files:\n",
    "            if 'recording' in file:\n",
    "                self.recordings.append(os.path.join(root_dir, file))\n",
    "            elif 'instruments' in file:\n",
    "                self.instrument_files.append(os.path.join(root_dir, file))\n",
    "                \n",
    "        assert(len(self.recordings) == len(self.instrument_files))\n",
    "        self.recordings.sort()\n",
    "        self.instrument_files.sort()\n",
    "        self.transform = transform\n",
    "\n",
    "    # __len__\n",
    "    # RETURN: the number of recording files in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.recordings)\n",
    "\n",
    "    # __getitem__\n",
    "    # ARGUMENTS\n",
    "    # idx: indicates which file to get\n",
    "    # RETURN: an instance with keys 'instruments', 'history'\n",
    "    # instance['history'] is a numpy array of MIDI actions\n",
    "    # instance['instruments'] a numpy array of instrument numbers\n",
    "    def __getitem__(self, idx):\n",
    "        instance = {'history': np.load(self.recordings[idx], allow_pickle=True), \\\n",
    "                    'instruments': np.load(self.instrument_files[idx], allow_pickle=True)}\n",
    "        \n",
    "        assert(len(instance['history']) == len(instance['instruments']))\n",
    "        \n",
    "        if self.transform:\n",
    "            instance = self.transform(instance)\n",
    "            \n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn: takes a list of samples from the dataset and turns them into a batch.\n",
    "# ARGUMENTS\n",
    "# batch: a list of dictionaries\n",
    "# RETURN: a sample with keys 'history', 'instruments', and 'mask'\n",
    "# sample['history']: an NxLxBxD tensor containing actions\n",
    "# sample['instruments']: an NxB tensor containing instrument numbers\n",
    "# sample['mask']: an NxLxB tensor containing False where a message is\n",
    "# valid, and True where it isn't (accounts for variable length sequences\n",
    "# and zero padding)\n",
    "def collate_fn(batch):\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    # We size our tensors to accomodate the longest sequence and the largest ensemble\n",
    "    max_instruments = max([instance['history'].shape[0] for instance in batch])\n",
    "    max_len = max([instance['history'].shape[1] for instance in batch])\n",
    "\n",
    "    sample = {'history': torch.ones((max_instruments, max_len, batch_size, action_dim)), \\\n",
    "              'instruments': torch.zeros((max_instruments, batch_size), dtype=torch.long), \\\n",
    "              'mask': torch.ones((max_instruments, max_len, batch_size), dtype=torch.bool)}\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        instrument_idx = [instrument_numbers.index(inst) for inst in batch[b]['instruments']]\n",
    "        sample['instruments'][:len(instrument_idx), b] = torch.tensor(instrument_idx, dtype=torch.long)\n",
    "        seq_length = batch[b]['history'].shape[1]\n",
    "        n_inst = batch[b]['history'].shape[0]\n",
    "        \n",
    "        sample['history'][:n_inst, :seq_length, b] = torch.tensor(batch[b]['history'])\n",
    "        sample['mask'][:n_inst, :seq_length, b] = False\n",
    "            \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_size = 512\n",
    "heads = 8\n",
    "attention_layers=  6\n",
    "embed_dim = 256\n",
    "chunk_size = 200\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleTransformer(action_dim, embed_dim, len(instrument_numbers), heads, attention_layers, ff_size, chunk_size)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "dataset = MIDIDataset('preprocessed_data_sync')\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "bce = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "mse = torch.nn.MSELoss(reduction='none')\n",
    "epochs = 10\n",
    "train_losses = np.zeros(epochs)\n",
    "test_losses = np.zeros(epochs) # TODO: train/test split. Can we do this with Dataloader?\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    n_iter = 0\n",
    "    for batch in dataloader:\n",
    "        # Chunk during training as well to make sure we don't hang\n",
    "        for start in range(0, batch['history'].shape[1] - 1, chunk_size):  \n",
    "            if n_iter%100 == 0:\n",
    "                print('Starting iteration %d' %(n_iter))\n",
    "            \n",
    "            end = min(start + chunk_size, batch['history'].shape[1] - 1)\n",
    "            \n",
    "            size = end - start\n",
    "            \n",
    "            out = model(batch['history'][:, start:end], batch['mask'][:, start:end], batch['instruments'])\n",
    "\n",
    "            targets = batch['history'][:, start + 1:end + 1]\n",
    "\n",
    "            note_losses = bce(out[:, :, :, :2*num_notes], targets[:, :, :, :2*num_notes])\n",
    "\n",
    "            # Only apply dynamics losses to notes that are being turned on\n",
    "            dynamics_losses = targets[:, :, :, :num_notes]*mse(torch.sigmoid(out[:, :, :, 2*num_notes:]), targets[:, :, :, 2*num_notes:])\n",
    "\n",
    "            loss_mask = torch.logical_not(batch['mask'][:, start:end])\n",
    "            loss = note_losses[loss_mask].mean() + dynamics_losses[loss_mask].mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.save(model.state_dict(), 'trained_models/epoch' + str(epoch) + '.pth')\n",
    "\n",
    "            train_losses[epoch] += loss.data\n",
    "            n_iter += 1\n",
    "        \n",
    "    train_losses[epoch] /= n_iter\n",
    "    print('Loss: %f' %(loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'it6100.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('trained_models/epoch4.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = np.load('preprocessed_data_sync/recording0.npy', allow_pickle=True)\n",
    "instruments_np = np.load('preprocessed_data_sync/instruments0.npy', allow_pickle=True)\n",
    "\n",
    "history = torch.tensor(recording)\n",
    "max_instruments = history.shape[0]\n",
    "instruments = torch.tensor([instrument_numbers.index(i) for i in instruments_np], dtype=torch.long).view(max_instruments, 1)\n",
    "\n",
    "model.eval() # Disable dropout to make results repeatable\n",
    "\n",
    "time_steps = 200 # How many time steps do we sample?\n",
    "\n",
    "# TODO: should we have SOS and EOS tokens like in NLP?\n",
    "gen_history = history[:, 0].view(max_instruments, 1, 1, -1)\n",
    "mask = torch.zeros((max_instruments, 1, 1), dtype=torch.bool)\n",
    "\n",
    "for t in range(1, max_seq_length):\n",
    "    out = torch.sigmoid(model(gen_history, mask, instruments))\n",
    "    \n",
    "    next_actions = torch.bernoulli(out[:, -1, :, :2*num_notes]).unsqueeze(1)\n",
    "    next_vels = out[:, -1, :, 2*num_notes:].unsqueeze(1)\n",
    "    append = torch.cat((next_actions, next_vels), dim=3)\n",
    "    gen_history = torch.cat((gen_history, append), dim=1)\n",
    "    mask = torch.cat((mask, torch.zeros((1, 1, 1), dtype=torch.bool)), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('gen_history.npy', gen_history.detach().view(max_instruments, max_seq_length, -1).numpy())\n",
    "np.save('gen_instruments.npy', np.array([instrument_numbers[instruments[0]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
