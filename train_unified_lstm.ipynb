{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_notes = 128\n",
    "num_time_shifts = 100\n",
    "num_velocities = 32\n",
    "message_dim = 2*num_notes + num_velocities + num_time_shifts\n",
    "instrument_numbers = [0, 6, 40, 41, 42, 43, 45, 60, 68, 70, 71, 73]\n",
    "num_instruments = len(instrument_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline LSTM definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://pytorch.org/tutorials/beginner/transformer_tutorial.html.\n",
    "# Only change is the view/expand in forward (accounts for batches)\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.shape[0], :].unsqueeze(1).expand(-1, x.shape[1], -1)\n",
    "        return self.dropout(x)\n",
    "        \n",
    "\n",
    "# EnsembleLSTM: takes a history of MIDI messages \n",
    "# for instruments in an ensemble and generates a distribution for the next message,\n",
    "# as well as the instrument who should issue the message\n",
    "class EnsembleLSTM(torch.nn.Module):\n",
    "    # CONSTRUCTOR\n",
    "    # ARGUMENTS\n",
    "    # message_dim: dimension of a MIDI message\n",
    "    # embed_dim: dimension of message embedding\n",
    "    # num_instruments: number of instrument labels\n",
    "    # hidden_size: size of hidden LSTM state\n",
    "    def __init__(self, message_dim, embed_dim, num_instruments, hidden_size):\n",
    "        super(EnsembleLSTM, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.lstm_input_dim = 2*embed_dim\n",
    "        \n",
    "        # We concatenate the tanhed instrument embedding to each input message.\n",
    "        # We have num_instruments + 1 embeddings to account for the dummy instrument\n",
    "        # associated with time shift events\n",
    "        # We could potentially reduce the size of the instrument embedding, but\n",
    "        # for now let's keep it the same size as the message embedding\n",
    "        self.i_embedding = torch.nn.Embedding(num_instruments + 1, embed_dim)\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(message_dim, embed_dim)\n",
    "        \n",
    "        # A 3-layer LSTM takes the history of messages (concatenated with their\n",
    "        # associated instrument encoding) and produces a decoding\n",
    "        self.lstm = torch.nn.LSTM(self.lstm_input_dim, hidden_size, num_layers=3)\n",
    "\n",
    "        # The decoding is passed through a linear layer to get the logits for the next message        \n",
    "        self.message_logits = torch.nn.Linear(hidden_size, message_dim)\n",
    "        \n",
    "        # The decoding is passed through a different linear layer to get a query for\n",
    "        # instrument-wise attention, which predicts which channel should generate\n",
    "        # this message\n",
    "        self.inst_query = torch.nn.Linear(hidden_size, embed_dim)\n",
    "        \n",
    "        # Indicates which channel is associated with each instrument\n",
    "        self.position_encoding = PositionalEncoding(embed_dim)\n",
    "    \n",
    "    # forward: generates a probability distribution for the next MIDI message\n",
    "    # and the channel that issues the message, given a message history for the instrument ensemble\n",
    "    # ARGUMENTS\n",
    "    # history: an LxBx2 tensor, where L is the length of the longest message history in\n",
    "    # the batch, and B is the batch size. The first index along dimension 2 stores the\n",
    "    # message number. The second stores the channel number. This should be END-PADDED\n",
    "    # along dimension 0\n",
    "    # mask: an LxB tensor, containing True in any locations where history contains\n",
    "    # padding\n",
    "    # instruments: a CxB tensor indicating the instrument number for each channel, where\n",
    "    # C is the maximum number of channels in the batch. This should be END-PADDED along dimension 0.\n",
    "    # The the last valid channel index should contain num_instruments (indicating the \"time-shift instrument\")\n",
    "    # RETURN: two tensors. The first is LxBxD, representing the distribution for the next message at each time\n",
    "    # step (need to take the softmax to get actual probabilities). The second is LxBxC, representing the\n",
    "    # distribution for the next channel at each time step (need to take the softmax to get actual probabilities)\n",
    "    def forward(self, history, mask, instruments):\n",
    "        L = history.shape[0] # longest length\n",
    "        B = history.shape[1] # batch size\n",
    "        C = instruments.shape[0]\n",
    "        assert(mask.shape == (L, B))\n",
    "        \n",
    "        # CxBxD\n",
    "        inst_embed = torch.tanh(self.i_embedding(instruments))\n",
    "        \n",
    "        # LxBxD, instrument embedding associated with each message\n",
    "        inst_tags = torch.gather(inst_embed, 0, history[:, :, 1].unsqueeze(2).expand(-1, -1, self.embed_dim))\n",
    "        \n",
    "        # LxBxD\n",
    "        message_embed = self.embedding(history[:, :, 0])\n",
    "        \n",
    "        inputs = torch.cat((message_embed, inst_tags), dim=2)\n",
    "        \n",
    "        decoding, last_hidden = self.lstm(inputs)\n",
    "        \n",
    "        # LxBxD\n",
    "        message_dist = self.message_logits(decoding)\n",
    "        \n",
    "        # CxLxBxD\n",
    "        inst_queries = self.inst_query(decoding).unsqueeze(0).expand(C, -1, -1, -1)\n",
    "        \n",
    "        prods = inst_queries*self.position_encoding(inst_embed).unsqueeze(1).expand(-1, L, -1, -1)/self.embed_dim\n",
    "        \n",
    "        # Dot-product attention (LxBxC)\n",
    "        channel_dist = torch.sum(prods, dim=3).permute(1, 2, 0)\n",
    "        \n",
    "        return message_dist, channel_dist\n",
    "    \n",
    "    # forward: generates a probability distribution for the next MIDI message\n",
    "    # and the channel that should issue it, given the previous message and channel,\n",
    "    # as well as the LSTM hidden state\n",
    "    # ARGUMENTS\n",
    "    # last_token: a 1x1x2 tensor. The first index along dimension 2 stores the\n",
    "    # message number. The second stores the channel number\n",
    "    # instruments: a Cx1 tensor indicating the instrument number for each channel, where C is the number of channels\n",
    "    # The the last channel index should contain num_instruments (indicating the \"time-shift instrument\")\n",
    "    # hidden: the last hidden state for the LSTM\n",
    "    # RETURN: a 1x1x2 tensor, predicting the next message and the channel that should issue it, as well as the new\n",
    "    def forward_generate(self, last_token, instruments, hidden):\n",
    "        assert(last_token.shape == (1, 1, 2))\n",
    "        C = instruments.shape[0]\n",
    "        \n",
    "        # Cx1xD\n",
    "        inst_embed = torch.tanh(self.i_embedding(instruments))\n",
    "        \n",
    "        # 1x1xD, instrument embedding associated with each message\n",
    "        inst_tags = torch.gather(inst_embed, 0, last_token[:, :, 1].unsqueeze(2).expand(-1, -1, self.embed_dim))\n",
    "        \n",
    "        # 1x1xD\n",
    "        message_embed = self.embedding(last_token[:, :, 0])\n",
    "        \n",
    "        inputs = torch.cat((message_embed, inst_tags), dim=2)\n",
    "        \n",
    "        decoding, new_hidden = self.lstm(inputs, hidden)\n",
    "        \n",
    "        # 1x1xD\n",
    "        message_dist = self.message_logits(decoding)\n",
    "        \n",
    "        # Cx1x1xD\n",
    "        inst_queries = self.inst_query(decoding).unsqueeze(0).expand(C, -1, -1, -1)\n",
    "        \n",
    "        prods = inst_queries*self.position_encoding(inst_embed).unsqueeze(1)/self.embed_dim\n",
    "        \n",
    "        # Dot-product attention (1x1xC)\n",
    "        channel_dist = torch.sum(prods, dim=3).permute(1, 2, 0)\n",
    "        \n",
    "        #message = torch.multinomial(torch.softmax(message_dist.flatten(), dim=0), 1)\n",
    "        #channel = torch.multinomial(torch.softmax(channel_dist.flatten(), dim=0), 1)\n",
    "        \n",
    "        message = torch.argmax(torch.softmax(message_dist.flatten(), dim=0))\n",
    "        channel = torch.argmax(torch.softmax(channel_dist.flatten(), dim=0))\n",
    "        \n",
    "        ret = torch.cat((message.view(1, 1, 1), channel.view(1, 1, 1)), dim=2)\n",
    "        \n",
    "        return ret, new_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests for baseline LSTM\n",
    "We train with model.eval() to disable dropout, since these tests try to get the model to overfit to a small sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model to overfit to a single song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "hidden_size = 1024\n",
    "\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleLSTM(message_dim, embed_dim, num_instruments, hidden_size)\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "    \n",
    "model.eval() # Training with eval just to see if we can overfit without dropout\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('overfit_single_song.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Loss: 7.567419\n",
      "Starting epoch 1\n",
      "Loss: 7.284444\n",
      "Starting epoch 2\n",
      "Loss: 5.026549\n",
      "Starting epoch 3\n",
      "Loss: 4.377564\n",
      "Starting epoch 4\n",
      "Loss: 4.178999\n",
      "Starting epoch 5\n",
      "Loss: 4.053557\n",
      "Starting epoch 6\n",
      "Loss: 4.051679\n",
      "Starting epoch 7\n",
      "Loss: 4.050501\n",
      "Starting epoch 8\n",
      "Loss: 4.027046\n",
      "Starting epoch 9\n",
      "Loss: 4.015304\n",
      "Starting epoch 10\n",
      "Loss: 4.010924\n",
      "Starting epoch 11\n",
      "Loss: 4.001538\n",
      "Starting epoch 12\n",
      "Loss: 3.993667\n",
      "Starting epoch 13\n",
      "Loss: 3.989302\n",
      "Starting epoch 14\n",
      "Loss: 3.988303\n",
      "Starting epoch 15\n",
      "Loss: 3.987134\n",
      "Starting epoch 16\n",
      "Loss: 3.986350\n",
      "Starting epoch 17\n",
      "Loss: 3.986148\n",
      "Starting epoch 18\n",
      "Loss: 3.976891\n",
      "Starting epoch 19\n",
      "Loss: 3.971772\n",
      "Starting epoch 20\n",
      "Loss: 3.964532\n",
      "Starting epoch 21\n",
      "Loss: 3.962311\n",
      "Starting epoch 22\n",
      "Loss: 3.958121\n",
      "Starting epoch 23\n",
      "Loss: 3.954402\n",
      "Starting epoch 24\n",
      "Loss: 3.952146\n",
      "Starting epoch 25\n",
      "Loss: 3.949122\n",
      "Starting epoch 26\n",
      "Loss: 3.943463\n",
      "Starting epoch 27\n",
      "Loss: 3.936802\n",
      "Starting epoch 28\n",
      "Loss: 3.930243\n",
      "Starting epoch 29\n",
      "Loss: 3.922114\n",
      "Starting epoch 30\n",
      "Loss: 3.913517\n",
      "Starting epoch 31\n",
      "Loss: 3.906054\n",
      "Starting epoch 32\n",
      "Loss: 3.894599\n",
      "Starting epoch 33\n",
      "Loss: 3.880554\n",
      "Starting epoch 34\n",
      "Loss: 3.863465\n",
      "Starting epoch 35\n",
      "Loss: 3.844538\n",
      "Starting epoch 36\n",
      "Loss: 3.822307\n",
      "Starting epoch 37\n",
      "Loss: 3.795623\n",
      "Starting epoch 38\n",
      "Loss: 3.760786\n",
      "Starting epoch 39\n",
      "Loss: 3.718619\n",
      "Starting epoch 40\n",
      "Loss: 3.681127\n",
      "Starting epoch 41\n",
      "Loss: 3.647157\n",
      "Starting epoch 42\n",
      "Loss: 3.598785\n",
      "Starting epoch 43\n",
      "Loss: 3.538578\n",
      "Starting epoch 44\n",
      "Loss: 3.482194\n",
      "Starting epoch 45\n",
      "Loss: 3.429688\n",
      "Starting epoch 46\n",
      "Loss: 3.366828\n",
      "Starting epoch 47\n",
      "Loss: 3.305066\n",
      "Starting epoch 48\n",
      "Loss: 3.246994\n",
      "Starting epoch 49\n",
      "Loss: 3.182752\n",
      "Starting epoch 50\n",
      "Loss: 3.126019\n",
      "Starting epoch 51\n",
      "Loss: 3.063936\n",
      "Starting epoch 52\n",
      "Loss: 3.006214\n",
      "Starting epoch 53\n",
      "Loss: 2.948681\n",
      "Starting epoch 54\n",
      "Loss: 2.892239\n",
      "Starting epoch 55\n",
      "Loss: 2.839777\n",
      "Starting epoch 56\n",
      "Loss: 2.782951\n",
      "Starting epoch 57\n",
      "Loss: 2.732931\n",
      "Starting epoch 58\n",
      "Loss: 2.683068\n",
      "Starting epoch 59\n",
      "Loss: 2.629631\n",
      "Starting epoch 60\n",
      "Loss: 2.580695\n",
      "Starting epoch 61\n",
      "Loss: 2.534047\n",
      "Starting epoch 62\n",
      "Loss: 2.493135\n",
      "Starting epoch 63\n",
      "Loss: 2.463448\n",
      "Starting epoch 64\n",
      "Loss: 2.414692\n",
      "Starting epoch 65\n",
      "Loss: 2.358129\n",
      "Starting epoch 66\n",
      "Loss: 2.322595\n",
      "Starting epoch 67\n",
      "Loss: 2.267346\n",
      "Starting epoch 68\n",
      "Loss: 2.232921\n",
      "Starting epoch 69\n",
      "Loss: 2.184100\n",
      "Starting epoch 70\n",
      "Loss: 2.141943\n",
      "Starting epoch 71\n",
      "Loss: 2.101691\n",
      "Starting epoch 72\n",
      "Loss: 2.052498\n",
      "Starting epoch 73\n",
      "Loss: 2.008987\n",
      "Starting epoch 74\n",
      "Loss: 1.962577\n",
      "Starting epoch 75\n",
      "Loss: 1.925144\n",
      "Starting epoch 76\n",
      "Loss: 1.880702\n",
      "Starting epoch 77\n",
      "Loss: 1.846642\n",
      "Starting epoch 78\n",
      "Loss: 1.808559\n",
      "Starting epoch 79\n",
      "Loss: 1.770936\n",
      "Starting epoch 80\n",
      "Loss: 1.734275\n",
      "Starting epoch 81\n",
      "Loss: 1.702734\n",
      "Starting epoch 82\n",
      "Loss: 1.671914\n",
      "Starting epoch 83\n",
      "Loss: 1.641657\n",
      "Starting epoch 84\n",
      "Loss: 1.611567\n",
      "Starting epoch 85\n",
      "Loss: 1.588902\n",
      "Starting epoch 86\n",
      "Loss: 1.557508\n",
      "Starting epoch 87\n",
      "Loss: 1.534441\n",
      "Starting epoch 88\n",
      "Loss: 1.509870\n",
      "Starting epoch 89\n",
      "Loss: 1.486754\n",
      "Starting epoch 90\n",
      "Loss: 1.458845\n",
      "Starting epoch 91\n",
      "Loss: 1.434604\n",
      "Starting epoch 92\n",
      "Loss: 1.411169\n",
      "Starting epoch 93\n",
      "Loss: 1.387412\n",
      "Starting epoch 94\n",
      "Loss: 1.360047\n",
      "Starting epoch 95\n",
      "Loss: 1.335619\n",
      "Starting epoch 96\n",
      "Loss: 1.310359\n",
      "Starting epoch 97\n",
      "Loss: 1.286524\n",
      "Starting epoch 98\n",
      "Loss: 1.262901\n",
      "Starting epoch 99\n",
      "Loss: 1.241840\n",
      "Starting epoch 100\n",
      "Loss: 1.227762\n",
      "Starting epoch 101\n",
      "Loss: 1.229901\n",
      "Starting epoch 102\n",
      "Loss: 1.188424\n",
      "Starting epoch 103\n",
      "Loss: 1.158119\n",
      "Starting epoch 104\n",
      "Loss: 1.148274\n",
      "Starting epoch 105\n",
      "Loss: 1.125868\n",
      "Starting epoch 106\n",
      "Loss: 1.101883\n",
      "Starting epoch 107\n",
      "Loss: 1.084822\n",
      "Starting epoch 108\n",
      "Loss: 1.070193\n",
      "Starting epoch 109\n",
      "Loss: 1.053849\n",
      "Starting epoch 110\n",
      "Loss: 1.033426\n",
      "Starting epoch 111\n",
      "Loss: 1.016871\n",
      "Starting epoch 112\n",
      "Loss: 1.004490\n",
      "Starting epoch 113\n",
      "Loss: 0.989986\n",
      "Starting epoch 114\n",
      "Loss: 0.973743\n",
      "Starting epoch 115\n",
      "Loss: 0.955021\n",
      "Starting epoch 116\n",
      "Loss: 0.939554\n",
      "Starting epoch 117\n",
      "Loss: 0.927438\n",
      "Starting epoch 118\n",
      "Loss: 0.918855\n",
      "Starting epoch 119\n",
      "Loss: 0.913273\n",
      "Starting epoch 120\n",
      "Loss: 0.895243\n",
      "Starting epoch 121\n",
      "Loss: 0.869852\n",
      "Starting epoch 122\n",
      "Loss: 0.858616\n",
      "Starting epoch 123\n",
      "Loss: 0.850927\n",
      "Starting epoch 124\n",
      "Loss: 0.834305\n",
      "Starting epoch 125\n",
      "Loss: 0.808523\n",
      "Starting epoch 126\n",
      "Loss: 0.788831\n",
      "Starting epoch 127\n",
      "Loss: 0.777147\n",
      "Starting epoch 128\n",
      "Loss: 0.819422\n",
      "Starting epoch 129\n",
      "Loss: 0.940409\n",
      "Starting epoch 130\n",
      "Loss: 0.852195\n",
      "Starting epoch 131\n",
      "Loss: 0.784779\n",
      "Starting epoch 132\n",
      "Loss: 0.798394\n",
      "Starting epoch 133\n",
      "Loss: 0.764772\n",
      "Starting epoch 134\n",
      "Loss: 0.734757\n",
      "Starting epoch 135\n",
      "Loss: 0.745977\n",
      "Starting epoch 136\n",
      "Loss: 0.699196\n",
      "Starting epoch 137\n",
      "Loss: 0.697696\n",
      "Starting epoch 138\n",
      "Loss: 0.679104\n",
      "Starting epoch 139\n",
      "Loss: 0.662151\n",
      "Starting epoch 140\n",
      "Loss: 0.651102\n",
      "Starting epoch 141\n",
      "Loss: 0.644945\n",
      "Starting epoch 142\n",
      "Loss: 0.627175\n",
      "Starting epoch 143\n",
      "Loss: 0.621234\n",
      "Starting epoch 144\n",
      "Loss: 0.609312\n",
      "Starting epoch 145\n",
      "Loss: 0.599671\n",
      "Starting epoch 146\n",
      "Loss: 0.591606\n",
      "Starting epoch 147\n",
      "Loss: 0.585648\n",
      "Starting epoch 148\n",
      "Loss: 0.574683\n",
      "Starting epoch 149\n",
      "Loss: 0.569030\n",
      "Starting epoch 150\n",
      "Loss: 0.561770\n",
      "Starting epoch 151\n",
      "Loss: 0.555372\n",
      "Starting epoch 152\n",
      "Loss: 0.547844\n",
      "Starting epoch 153\n",
      "Loss: 0.541365\n",
      "Starting epoch 154\n",
      "Loss: 0.535406\n",
      "Starting epoch 155\n",
      "Loss: 0.529797\n",
      "Starting epoch 156\n",
      "Loss: 0.522528\n",
      "Starting epoch 157\n",
      "Loss: 0.516651\n",
      "Starting epoch 158\n",
      "Loss: 0.510943\n",
      "Starting epoch 159\n",
      "Loss: 0.505468\n",
      "Starting epoch 160\n",
      "Loss: 0.499732\n",
      "Starting epoch 161\n",
      "Loss: 0.494277\n",
      "Starting epoch 162\n",
      "Loss: 0.488591\n",
      "Starting epoch 163\n",
      "Loss: 0.483859\n",
      "Starting epoch 164\n",
      "Loss: 0.478186\n",
      "Starting epoch 165\n",
      "Loss: 0.472248\n",
      "Starting epoch 166\n",
      "Loss: 0.468196\n",
      "Starting epoch 167\n",
      "Loss: 0.465389\n",
      "Starting epoch 168\n",
      "Loss: 0.459589\n",
      "Starting epoch 169\n",
      "Loss: 0.454147\n",
      "Starting epoch 170\n",
      "Loss: 0.448609\n",
      "Starting epoch 171\n",
      "Loss: 0.444479\n",
      "Starting epoch 172\n",
      "Loss: 0.438751\n",
      "Starting epoch 173\n",
      "Loss: 0.433964\n",
      "Starting epoch 174\n",
      "Loss: 0.429615\n",
      "Starting epoch 175\n",
      "Loss: 0.425148\n",
      "Starting epoch 176\n",
      "Loss: 0.420459\n",
      "Starting epoch 177\n",
      "Loss: 0.416170\n",
      "Starting epoch 178\n",
      "Loss: 0.411729\n",
      "Starting epoch 179\n",
      "Loss: 0.406966\n",
      "Starting epoch 180\n",
      "Loss: 0.402989\n",
      "Starting epoch 181\n",
      "Loss: 0.399036\n",
      "Starting epoch 182\n",
      "Loss: 0.395203\n",
      "Starting epoch 183\n",
      "Loss: 0.391237\n",
      "Starting epoch 184\n",
      "Loss: 0.387473\n",
      "Starting epoch 185\n",
      "Loss: 0.383558\n",
      "Starting epoch 186\n",
      "Loss: 0.379837\n",
      "Starting epoch 187\n",
      "Loss: 0.375999\n",
      "Starting epoch 188\n",
      "Loss: 0.372413\n",
      "Starting epoch 189\n",
      "Loss: 0.368690\n",
      "Starting epoch 190\n",
      "Loss: 0.364991\n",
      "Starting epoch 191\n",
      "Loss: 0.361339\n",
      "Starting epoch 192\n",
      "Loss: 0.357698\n",
      "Starting epoch 193\n",
      "Loss: 0.353906\n",
      "Starting epoch 194\n",
      "Loss: 0.350065\n",
      "Starting epoch 195\n",
      "Loss: 0.346277\n",
      "Starting epoch 196\n",
      "Loss: 0.342505\n",
      "Starting epoch 197\n",
      "Loss: 0.338743\n",
      "Starting epoch 198\n",
      "Loss: 0.335106\n",
      "Starting epoch 199\n",
      "Loss: 0.331437\n",
      "Starting epoch 200\n",
      "Loss: 0.328247\n",
      "Starting epoch 201\n",
      "Loss: 0.324343\n",
      "Starting epoch 202\n",
      "Loss: 0.320419\n",
      "Starting epoch 203\n",
      "Loss: 0.316563\n",
      "Starting epoch 204\n",
      "Loss: 0.313039\n",
      "Starting epoch 205\n",
      "Loss: 0.309041\n",
      "Starting epoch 206\n",
      "Loss: 0.305553\n",
      "Starting epoch 207\n",
      "Loss: 0.301748\n",
      "Starting epoch 208\n",
      "Loss: 0.298239\n",
      "Starting epoch 209\n",
      "Loss: 0.294705\n",
      "Starting epoch 210\n",
      "Loss: 0.291034\n",
      "Starting epoch 211\n",
      "Loss: 0.287657\n",
      "Starting epoch 212\n",
      "Loss: 0.284197\n",
      "Starting epoch 213\n",
      "Loss: 0.280658\n",
      "Starting epoch 214\n",
      "Loss: 0.277132\n",
      "Starting epoch 215\n",
      "Loss: 0.273689\n",
      "Starting epoch 216\n",
      "Loss: 0.270098\n",
      "Starting epoch 217\n",
      "Loss: 0.266535\n",
      "Starting epoch 218\n",
      "Loss: 0.263070\n",
      "Starting epoch 219\n",
      "Loss: 0.259481\n",
      "Starting epoch 220\n",
      "Loss: 0.255771\n",
      "Starting epoch 221\n",
      "Loss: 0.251837\n",
      "Starting epoch 222\n",
      "Loss: 0.248335\n",
      "Starting epoch 223\n",
      "Loss: 0.244654\n",
      "Starting epoch 224\n",
      "Loss: 0.241065\n",
      "Starting epoch 225\n",
      "Loss: 0.237710\n",
      "Starting epoch 226\n",
      "Loss: 0.234433\n",
      "Starting epoch 227\n",
      "Loss: 0.231450\n",
      "Starting epoch 228\n",
      "Loss: 0.228414\n",
      "Starting epoch 229\n",
      "Loss: 0.225237\n",
      "Starting epoch 230\n",
      "Loss: 0.222547\n",
      "Starting epoch 231\n",
      "Loss: 0.219703\n",
      "Starting epoch 232\n",
      "Loss: 0.216873\n",
      "Starting epoch 233\n",
      "Loss: 0.214226\n",
      "Starting epoch 234\n",
      "Loss: 0.211564\n",
      "Starting epoch 235\n",
      "Loss: 0.208831\n",
      "Starting epoch 236\n",
      "Loss: 0.205901\n",
      "Starting epoch 237\n",
      "Loss: 0.203138\n",
      "Starting epoch 238\n",
      "Loss: 0.201959\n",
      "Starting epoch 239\n",
      "Loss: 0.199688\n",
      "Starting epoch 240\n",
      "Loss: 0.206724\n",
      "Starting epoch 241\n",
      "Loss: 0.231214\n",
      "Starting epoch 242\n",
      "Loss: 0.419045\n",
      "Starting epoch 243\n",
      "Loss: 0.240334\n",
      "Starting epoch 244\n",
      "Loss: 0.473473\n",
      "Starting epoch 245\n",
      "Loss: 0.248711\n",
      "Starting epoch 246\n",
      "Loss: 0.751672\n",
      "Starting epoch 247\n",
      "Loss: 0.278845\n",
      "Starting epoch 248\n",
      "Loss: 0.927974\n",
      "Starting epoch 249\n",
      "Loss: 0.331875\n",
      "Starting epoch 250\n",
      "Loss: 0.294086\n",
      "Starting epoch 251\n",
      "Loss: 0.458060\n",
      "Starting epoch 252\n",
      "Loss: 0.358652\n",
      "Starting epoch 253\n",
      "Loss: 0.264396\n",
      "Starting epoch 254\n",
      "Loss: 0.272393\n",
      "Starting epoch 255\n",
      "Loss: 0.280218\n",
      "Starting epoch 256\n",
      "Loss: 0.280362\n",
      "Starting epoch 257\n",
      "Loss: 0.257655\n",
      "Starting epoch 258\n",
      "Loss: 0.238889\n",
      "Starting epoch 259\n",
      "Loss: 0.229181\n",
      "Starting epoch 260\n",
      "Loss: 0.222934\n",
      "Starting epoch 261\n",
      "Loss: 0.218658\n",
      "Starting epoch 262\n",
      "Loss: 0.216344\n",
      "Starting epoch 263\n",
      "Loss: 0.214175\n",
      "Starting epoch 264\n",
      "Loss: 0.210655\n",
      "Starting epoch 265\n",
      "Loss: 0.206476\n",
      "Starting epoch 266\n",
      "Loss: 0.202050\n",
      "Starting epoch 267\n",
      "Loss: 0.197600\n",
      "Starting epoch 268\n",
      "Loss: 0.193767\n",
      "Starting epoch 269\n",
      "Loss: 0.190895\n",
      "Starting epoch 270\n",
      "Loss: 0.189205\n",
      "Starting epoch 271\n",
      "Loss: 0.187617\n",
      "Starting epoch 272\n",
      "Loss: 0.184554\n",
      "Starting epoch 273\n",
      "Loss: 0.181869\n",
      "Starting epoch 274\n",
      "Loss: 0.179413\n",
      "Starting epoch 275\n",
      "Loss: 0.177604\n",
      "Starting epoch 276\n",
      "Loss: 0.176252\n",
      "Starting epoch 277\n",
      "Loss: 0.175110\n",
      "Starting epoch 278\n",
      "Loss: 0.173983\n",
      "Starting epoch 279\n",
      "Loss: 0.172806\n",
      "Starting epoch 280\n",
      "Loss: 0.171474\n",
      "Starting epoch 281\n",
      "Loss: 0.170060\n",
      "Starting epoch 282\n",
      "Loss: 0.168724\n",
      "Starting epoch 283\n",
      "Loss: 0.167371\n",
      "Starting epoch 284\n",
      "Loss: 0.166103\n",
      "Starting epoch 285\n",
      "Loss: 0.164954\n",
      "Starting epoch 286\n",
      "Loss: 0.163890\n",
      "Starting epoch 287\n",
      "Loss: 0.162942\n",
      "Starting epoch 288\n",
      "Loss: 0.162044\n",
      "Starting epoch 289\n",
      "Loss: 0.161132\n",
      "Starting epoch 290\n",
      "Loss: 0.160261\n",
      "Starting epoch 291\n",
      "Loss: 0.159429\n",
      "Starting epoch 292\n",
      "Loss: 0.158632\n",
      "Starting epoch 293\n",
      "Loss: 0.157885\n",
      "Starting epoch 294\n",
      "Loss: 0.157170\n",
      "Starting epoch 295\n",
      "Loss: 0.156492\n",
      "Starting epoch 296\n",
      "Loss: 0.155824\n",
      "Starting epoch 297\n",
      "Loss: 0.155137\n",
      "Starting epoch 298\n",
      "Loss: 0.154516\n",
      "Starting epoch 299\n",
      "Loss: 0.153881\n",
      "Starting epoch 300\n",
      "Loss: 0.153254\n",
      "Starting epoch 301\n",
      "Loss: 0.152648\n",
      "Starting epoch 302\n",
      "Loss: 0.152065\n",
      "Starting epoch 303\n",
      "Loss: 0.151503\n",
      "Starting epoch 304\n",
      "Loss: 0.150963\n",
      "Starting epoch 305\n",
      "Loss: 0.150441\n",
      "Starting epoch 306\n",
      "Loss: 0.149926\n",
      "Starting epoch 307\n",
      "Loss: 0.149389\n",
      "Starting epoch 308\n",
      "Loss: 0.148810\n",
      "Starting epoch 309\n",
      "Loss: 0.148253\n",
      "Starting epoch 310\n",
      "Loss: 0.147705\n",
      "Starting epoch 311\n",
      "Loss: 0.147130\n",
      "Starting epoch 312\n",
      "Loss: 0.146897\n",
      "Starting epoch 313\n",
      "Loss: 0.146470\n",
      "Starting epoch 314\n",
      "Loss: 0.145806\n",
      "Starting epoch 315\n",
      "Loss: 0.145576\n",
      "Starting epoch 316\n",
      "Loss: 0.145107\n",
      "Starting epoch 317\n",
      "Loss: 0.144694\n",
      "Starting epoch 318\n",
      "Loss: 0.144263\n",
      "Starting epoch 319\n",
      "Loss: 0.143842\n",
      "Starting epoch 320\n",
      "Loss: 0.143422\n",
      "Starting epoch 321\n",
      "Loss: 0.142915\n",
      "Starting epoch 322\n",
      "Loss: 0.142547\n",
      "Starting epoch 323\n",
      "Loss: 0.141871\n",
      "Starting epoch 324\n",
      "Loss: 0.141446\n",
      "Starting epoch 325\n",
      "Loss: 0.140842\n",
      "Starting epoch 326\n",
      "Loss: 0.140371\n",
      "Starting epoch 327\n",
      "Loss: 0.139755\n",
      "Starting epoch 328\n",
      "Loss: 0.139160\n",
      "Starting epoch 329\n",
      "Loss: 0.138499\n",
      "Starting epoch 330\n",
      "Loss: 0.137728\n",
      "Starting epoch 331\n",
      "Loss: 0.136865\n",
      "Starting epoch 332\n",
      "Loss: 0.135858\n",
      "Starting epoch 333\n",
      "Loss: 0.134944\n",
      "Starting epoch 334\n",
      "Loss: 0.133776\n",
      "Starting epoch 335\n",
      "Loss: 0.132318\n",
      "Starting epoch 336\n",
      "Loss: 0.130849\n",
      "Starting epoch 337\n",
      "Loss: 0.129666\n",
      "Starting epoch 338\n",
      "Loss: 0.128858\n",
      "Starting epoch 339\n",
      "Loss: 0.127814\n",
      "Starting epoch 340\n",
      "Loss: 0.127467\n",
      "Starting epoch 341\n",
      "Loss: 0.124250\n",
      "Starting epoch 342\n",
      "Loss: 0.121749\n",
      "Starting epoch 343\n",
      "Loss: 0.120789\n",
      "Starting epoch 344\n",
      "Loss: 0.120739\n",
      "Starting epoch 345\n",
      "Loss: 0.120063\n",
      "Starting epoch 346\n",
      "Loss: 0.117694\n",
      "Starting epoch 347\n",
      "Loss: 0.116870\n",
      "Starting epoch 348\n",
      "Loss: 0.115467\n",
      "Starting epoch 349\n",
      "Loss: 0.114588\n",
      "Starting epoch 350\n",
      "Loss: 0.114511\n",
      "Starting epoch 351\n",
      "Loss: 0.112831\n",
      "Starting epoch 352\n",
      "Loss: 0.112704\n",
      "Starting epoch 353\n",
      "Loss: 0.111942\n",
      "Starting epoch 354\n",
      "Loss: 0.111388\n",
      "Starting epoch 355\n",
      "Loss: 0.110736\n",
      "Starting epoch 356\n",
      "Loss: 0.110167\n",
      "Starting epoch 357\n",
      "Loss: 0.109816\n",
      "Starting epoch 358\n",
      "Loss: 0.109278\n",
      "Starting epoch 359\n",
      "Loss: 0.108774\n",
      "Starting epoch 360\n",
      "Loss: 0.108306\n",
      "Starting epoch 361\n",
      "Loss: 0.107889\n",
      "Starting epoch 362\n",
      "Loss: 0.107541\n",
      "Starting epoch 363\n",
      "Loss: 0.107160\n",
      "Starting epoch 364\n",
      "Loss: 0.106741\n",
      "Starting epoch 365\n",
      "Loss: 0.106393\n",
      "Starting epoch 366\n",
      "Loss: 0.106065\n",
      "Starting epoch 367\n",
      "Loss: 0.105735\n",
      "Starting epoch 368\n",
      "Loss: 0.105441\n",
      "Starting epoch 369\n",
      "Loss: 0.105138\n",
      "Starting epoch 370\n",
      "Loss: 0.104803\n",
      "Starting epoch 371\n",
      "Loss: 0.104498\n",
      "Starting epoch 372\n",
      "Loss: 0.104225\n",
      "Starting epoch 373\n",
      "Loss: 0.103948\n",
      "Starting epoch 374\n",
      "Loss: 0.103661\n",
      "Starting epoch 375\n",
      "Loss: 0.103387\n",
      "Starting epoch 376\n",
      "Loss: 0.103132\n",
      "Starting epoch 377\n",
      "Loss: 0.102879\n",
      "Starting epoch 378\n",
      "Loss: 0.102627\n",
      "Starting epoch 379\n",
      "Loss: 0.102384\n",
      "Starting epoch 380\n",
      "Loss: 0.102148\n",
      "Starting epoch 381\n",
      "Loss: 0.101908\n",
      "Starting epoch 382\n",
      "Loss: 0.101672\n",
      "Starting epoch 383\n",
      "Loss: 0.101443\n",
      "Starting epoch 384\n",
      "Loss: 0.101223\n",
      "Starting epoch 385\n",
      "Loss: 0.101009\n",
      "Starting epoch 386\n",
      "Loss: 0.100797\n",
      "Starting epoch 387\n",
      "Loss: 0.100590\n",
      "Starting epoch 388\n",
      "Loss: 0.100390\n",
      "Starting epoch 389\n",
      "Loss: 0.100192\n",
      "Starting epoch 390\n",
      "Loss: 0.099995\n",
      "Starting epoch 391\n",
      "Loss: 0.099801\n",
      "Starting epoch 392\n",
      "Loss: 0.099613\n",
      "Starting epoch 393\n",
      "Loss: 0.099429\n",
      "Starting epoch 394\n",
      "Loss: 0.099242\n",
      "Starting epoch 395\n",
      "Loss: 0.099057\n",
      "Starting epoch 396\n",
      "Loss: 0.098875\n",
      "Starting epoch 397\n",
      "Loss: 0.098688\n",
      "Starting epoch 398\n",
      "Loss: 0.098503\n",
      "Starting epoch 399\n",
      "Loss: 0.098323\n",
      "Starting epoch 400\n",
      "Loss: 0.098157\n",
      "Starting epoch 401\n",
      "Loss: 0.097997\n",
      "Starting epoch 402\n",
      "Loss: 0.097834\n",
      "Starting epoch 403\n",
      "Loss: 0.097664\n",
      "Starting epoch 404\n",
      "Loss: 0.097494\n",
      "Starting epoch 405\n",
      "Loss: 0.097329\n",
      "Starting epoch 406\n",
      "Loss: 0.097167\n",
      "Starting epoch 407\n",
      "Loss: 0.097008\n",
      "Starting epoch 408\n",
      "Loss: 0.096850\n",
      "Starting epoch 409\n",
      "Loss: 0.096695\n",
      "Starting epoch 410\n",
      "Loss: 0.096542\n",
      "Starting epoch 411\n",
      "Loss: 0.096387\n",
      "Starting epoch 412\n",
      "Loss: 0.096234\n",
      "Starting epoch 413\n",
      "Loss: 0.096081\n",
      "Starting epoch 414\n",
      "Loss: 0.095930\n",
      "Starting epoch 415\n",
      "Loss: 0.095781\n",
      "Starting epoch 416\n",
      "Loss: 0.095634\n",
      "Starting epoch 417\n",
      "Loss: 0.095488\n",
      "Starting epoch 418\n",
      "Loss: 0.095341\n",
      "Starting epoch 419\n",
      "Loss: 0.095191\n",
      "Starting epoch 420\n",
      "Loss: 0.095041\n",
      "Starting epoch 421\n",
      "Loss: 0.094885\n",
      "Starting epoch 422\n",
      "Loss: 0.094735\n",
      "Starting epoch 423\n",
      "Loss: 0.094581\n",
      "Starting epoch 424\n",
      "Loss: 0.094436\n",
      "Starting epoch 425\n",
      "Loss: 0.094292\n",
      "Starting epoch 426\n",
      "Loss: 0.094149\n",
      "Starting epoch 427\n",
      "Loss: 0.094006\n",
      "Starting epoch 428\n",
      "Loss: 0.093865\n",
      "Starting epoch 429\n",
      "Loss: 0.093722\n",
      "Starting epoch 430\n",
      "Loss: 0.093581\n",
      "Starting epoch 431\n",
      "Loss: 0.093440\n",
      "Starting epoch 432\n",
      "Loss: 0.093301\n",
      "Starting epoch 433\n",
      "Loss: 0.093163\n",
      "Starting epoch 434\n",
      "Loss: 0.093023\n",
      "Starting epoch 435\n",
      "Loss: 0.092885\n",
      "Starting epoch 436\n",
      "Loss: 0.092746\n",
      "Starting epoch 437\n",
      "Loss: 0.092607\n",
      "Starting epoch 438\n",
      "Loss: 0.092468\n",
      "Starting epoch 439\n",
      "Loss: 0.092330\n",
      "Starting epoch 440\n",
      "Loss: 0.092192\n",
      "Starting epoch 441\n",
      "Loss: 0.092053\n",
      "Starting epoch 442\n",
      "Loss: 0.091915\n",
      "Starting epoch 443\n",
      "Loss: 0.091778\n",
      "Starting epoch 444\n",
      "Loss: 0.091640\n",
      "Starting epoch 445\n",
      "Loss: 0.091503\n",
      "Starting epoch 446\n",
      "Loss: 0.091364\n",
      "Starting epoch 447\n",
      "Loss: 0.091224\n",
      "Starting epoch 448\n",
      "Loss: 0.091084\n",
      "Starting epoch 449\n",
      "Loss: 0.090945\n",
      "Starting epoch 450\n",
      "Loss: 0.090803\n",
      "Starting epoch 451\n",
      "Loss: 0.090662\n",
      "Starting epoch 452\n",
      "Loss: 0.090521\n",
      "Starting epoch 453\n",
      "Loss: 0.090379\n",
      "Starting epoch 454\n",
      "Loss: 0.090239\n",
      "Starting epoch 455\n",
      "Loss: 0.090098\n",
      "Starting epoch 456\n",
      "Loss: 0.089955\n",
      "Starting epoch 457\n",
      "Loss: 0.089812\n",
      "Starting epoch 458\n",
      "Loss: 0.089669\n",
      "Starting epoch 459\n",
      "Loss: 0.089525\n",
      "Starting epoch 460\n",
      "Loss: 0.089381\n",
      "Starting epoch 461\n",
      "Loss: 0.089234\n",
      "Starting epoch 462\n",
      "Loss: 0.089088\n",
      "Starting epoch 463\n",
      "Loss: 0.088936\n",
      "Starting epoch 464\n",
      "Loss: 0.088783\n",
      "Starting epoch 465\n",
      "Loss: 0.088622\n",
      "Starting epoch 466\n",
      "Loss: 0.088461\n",
      "Starting epoch 467\n",
      "Loss: 0.088300\n",
      "Starting epoch 468\n",
      "Loss: 0.088151\n",
      "Starting epoch 469\n",
      "Loss: 0.088010\n",
      "Starting epoch 470\n",
      "Loss: 0.087867\n",
      "Starting epoch 471\n",
      "Loss: 0.087714\n",
      "Starting epoch 472\n",
      "Loss: 0.087554\n",
      "Starting epoch 473\n",
      "Loss: 0.087398\n",
      "Starting epoch 474\n",
      "Loss: 0.087244\n",
      "Starting epoch 475\n",
      "Loss: 0.087098\n",
      "Starting epoch 476\n",
      "Loss: 0.086952\n",
      "Starting epoch 477\n",
      "Loss: 0.086809\n",
      "Starting epoch 478\n",
      "Loss: 0.086667\n",
      "Starting epoch 479\n",
      "Loss: 0.086558\n",
      "Starting epoch 480\n",
      "Loss: 0.086402\n",
      "Starting epoch 481\n",
      "Loss: 0.086236\n",
      "Starting epoch 482\n",
      "Loss: 0.086059\n",
      "Starting epoch 483\n",
      "Loss: 0.085907\n",
      "Starting epoch 484\n",
      "Loss: 0.085750\n",
      "Starting epoch 485\n",
      "Loss: 0.085568\n",
      "Starting epoch 486\n",
      "Loss: 0.085390\n",
      "Starting epoch 487\n",
      "Loss: 0.085227\n",
      "Starting epoch 488\n",
      "Loss: 0.085064\n",
      "Starting epoch 489\n",
      "Loss: 0.084898\n",
      "Starting epoch 490\n",
      "Loss: 0.084728\n",
      "Starting epoch 491\n",
      "Loss: 0.084555\n",
      "Starting epoch 492\n",
      "Loss: 0.084393\n",
      "Starting epoch 493\n",
      "Loss: 0.084217\n",
      "Starting epoch 494\n",
      "Loss: 0.084051\n",
      "Starting epoch 495\n",
      "Loss: 0.083888\n",
      "Starting epoch 496\n",
      "Loss: 0.083725\n",
      "Starting epoch 497\n",
      "Loss: 0.083558\n",
      "Starting epoch 498\n",
      "Loss: 0.083383\n",
      "Starting epoch 499\n",
      "Loss: 0.083205\n"
     ]
    }
   ],
   "source": [
    "recording = np.load('preprocessed_data_unified/recording318.npy', allow_pickle=True)\n",
    "instruments_np = np.load('preprocessed_data_unified/instruments318.npy', allow_pickle=True)\n",
    "\n",
    "nsamples = 100\n",
    "\n",
    "history = torch.tensor(recording[:nsamples], dtype=torch.long).view(-1, 1, 2)\n",
    "mask = torch.zeros((history.shape[0], history.shape[1]), dtype=torch.bool)\n",
    "instruments = torch.tensor([instrument_numbers.index(i) for i in instruments_np] + [num_instruments], dtype=torch.long).view(-1, 1)\n",
    "\n",
    "num_channels = instruments.shape[0]\n",
    "\n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 500\n",
    "train_losses = np.zeros(epochs)\n",
    "\n",
    "target_messages = history[1:, :, 0].flatten()\n",
    "target_channels = history[1:, :, 1].flatten()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    \n",
    "    message_logits, channel_logits = model(history[:-1], mask[:-1], instruments)\n",
    "    \n",
    "    loss = loss_fn(message_logits.view(-1, message_dim), target_messages) + \\\n",
    "           loss_fn(channel_logits.view(-1, num_channels), target_channels)\n",
    "                \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses[epoch] = loss.data\n",
    "    print('Loss: %f' %(loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'overfit_single_song.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f124ca69bd0>]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong instrument at time 90!\n",
      "Wrong instrument at time 91!\n",
      "Wrong instrument at time 96!\n",
      "Wrong instrument at time 98!\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Sample from model\n",
    "gen_history = history[0].unsqueeze(0)\n",
    "mask = torch.zeros((1, 1), dtype=torch.bool)\n",
    "model.eval() # Turns off the dropout for evaluation. Need to do this to get repeatable evaluation outputs\n",
    "\n",
    "# Move forward in time\n",
    "wrong_cnt = 0\n",
    "hidden = (torch.zeros(3, 1, hidden_size), torch.zeros(3, 1, hidden_size))\n",
    "for t in range(0, history.shape[0] - 1):\n",
    "    ret, hidden = model.forward_generate(gen_history[-1].view(1, 1, 2), instruments, hidden)\n",
    "    \n",
    "    gen_history = torch.cat((gen_history, ret), dim=0)\n",
    "    \n",
    "    if gen_history[-1, 0, 0] != history[t + 1, 0, 0]:\n",
    "        print('Wrong message at time %d!' %(t))\n",
    "        wrong_cnt += 1\n",
    "        \n",
    "    if gen_history[-1, 0, 1] != history[t + 1, 0, 1]:\n",
    "        print('Wrong instrument at time %d!' %(t))\n",
    "        wrong_cnt += 1\n",
    "    \n",
    "    mask = torch.cat((mask, torch.zeros((1, 1), dtype=torch.bool)), dim=0)\n",
    "\n",
    "print(wrong_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gen_history[:, 0, 0].flatten())\n",
    "print(history[:, 0, 0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test_history.npy', gen_history.squeeze(1).detach().numpy())\n",
    "np.save('test_instruments.npy', [instrument_numbers[i] for i in instruments[:-1, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class MIDIDataset(torch.utils.data.Dataset):\n",
    "    # CONSTRUCTOR: creates a list of recording files and a list\n",
    "    # of instrument files in root_dir. Assumes that the directory\n",
    "    # contains recording0.npy to recordingM.npy,\n",
    "    # as well as instruments0.npy to instrumentsM.npy\n",
    "    # ARGUMENTS\n",
    "    # root_dir: the directory to search\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        files = os.listdir(root_dir)\n",
    "        self.recordings = []\n",
    "        self.instrument_files = []\n",
    "        for file in files:\n",
    "            if 'recording' in file:\n",
    "                self.recordings.append(os.path.join(root_dir, file))\n",
    "            elif 'instruments' in file:\n",
    "                self.instrument_files.append(os.path.join(root_dir, file))\n",
    "                \n",
    "        assert(len(self.recordings) == len(self.instrument_files))\n",
    "        self.recordings.sort()\n",
    "        self.instrument_files.sort()\n",
    "        self.transform = transform\n",
    "\n",
    "    # __len__\n",
    "    # RETURN: the number of recording files in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.recordings)\n",
    "\n",
    "    # __getitem__\n",
    "    # ARGUMENTS\n",
    "    # idx: indicates which file to get\n",
    "    # RETURN: an instance with keys 'instruments', 'history'\n",
    "    # instance['history'] is an Lx2 numpy array containing messages and associated channels\n",
    "    # instance['instruments'] a numpy array of instrument numbers\n",
    "    def __getitem__(self, idx):\n",
    "        instance = {'history': np.load(self.recordings[idx], allow_pickle=True), \\\n",
    "                    'instruments': np.load(self.instrument_files[idx], allow_pickle=True)}\n",
    "        \n",
    "        if self.transform:\n",
    "            instance = self.transform(instance)\n",
    "            \n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn: takes a list of samples from the dataset and turns them into a batch.\n",
    "# ARGUMENTS\n",
    "# batch: a list of dictionaries\n",
    "# RETURN: a sample with keys 'history', 'instruments', and 'mask'\n",
    "# sample['history']: an LxBx2 tensor containing messages\n",
    "# sample['instruments']: a CxB tensor containing instrument numbers for each channel\n",
    "# sample['mask']: an LxB tensor containing False where a message is\n",
    "# valid, and True where it isn't (accounts for variable length sequences\n",
    "# and zero padding)\n",
    "# sample['nchan']: a length B tensor containing the number of channels for each batch\n",
    "# element (including the dummy time-shift channel)\n",
    "def collate_fn(batch):\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    # We size our tensors to accomodate the longest sequence and the largest number of channels\n",
    "    # (including the dummy time shift channel)\n",
    "    max_channels = max([instance['instruments'].shape[0] for instance in batch]) + 1\n",
    "    longest_len = max([instance['history'].shape[0] for instance in batch])\n",
    "\n",
    "    sample = {'history': torch.zeros((longest_len, batch_size, 2), dtype=torch.long), \\\n",
    "              'instruments': torch.zeros((max_channels, batch_size), dtype=torch.long), \\\n",
    "              'mask': torch.ones((longest_len, batch_size), dtype=torch.bool), \\\n",
    "              'nchan': torch.zeros(batch_size, dtype=torch.long)}\n",
    "\n",
    "    for b, instance in enumerate(batch):\n",
    "        instrument_idx = [instrument_numbers.index(i) for i in instance['instruments']]\n",
    "        \n",
    "        sample['instruments'][:len(instrument_idx), b] = torch.tensor(instrument_idx, dtype=torch.long)\n",
    "        # Add dummy channel\n",
    "        sample['instruments'][len(instrument_idx), b] = num_instruments\n",
    "        \n",
    "        # +1 to account for udmmy channel\n",
    "        sample['nchan'][b] = len(instrument_idx) + 1\n",
    "        \n",
    "        seq_length = instance['history'].shape[0]\n",
    "        sample['history'][:seq_length, b] = torch.tensor(instance['history'], dtype=torch.long)\n",
    "        sample['mask'][:seq_length, b] = False\n",
    "            \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "hidden_size = 1024\n",
    "\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleLSTM(message_dim, embed_dim, num_instruments, hidden_size)\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "chunk_size = 200\n",
    "\n",
    "dataset = MIDIDataset('preprocessed_data_unified')\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "epochs = 20\n",
    "train_losses = np.zeros(epochs)\n",
    "test_losses = np.zeros(epochs) # TODO: train/test split\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    n_iter = 0\n",
    "    for b, batch in enumerate(dataloader):\n",
    "        max_seq_length = batch['history'].shape[0]\n",
    "        \n",
    "        num_targets = max_seq_length - 1 # Messages start from t = 0, but we start generating at t = 1\n",
    "            \n",
    "        print('Starting iteration %d' %(b))\n",
    "        \n",
    "        message_logits, channel_logits = model(batch['history'][:-1], batch['mask'][:-1], batch['instruments'])\n",
    "        \n",
    "        target_mask = torch.logical_not(batch['mask'][1:])\n",
    "        \n",
    "        num_valid_targets = target_mask.sum()\n",
    "        \n",
    "        target_messages = batch['history'][1:, :, 0][target_mask]\n",
    "    \n",
    "        message_loss = loss_fn(message_logits[target_mask], target_messages)/num_valid_targets\n",
    "        \n",
    "        channel_losses = torch.zeros(batch_size)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            channel_logits_i = channel_logits[:, :, :batch['nchan'][i]][target_mask]\n",
    "            target_channels = batch['history'][1:, i, 1][target_mask]\n",
    "            channel_losses[i] = loss_fn(channel_logits_i, target_channels)/num_valid_targets\n",
    "            \n",
    "        loss = message_loss + channel_losses\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses[epoch] = loss.data\n",
    "    \n",
    "    torch.save(model.state_dict(), 'trained_models_12_3/epoch' + str(epoch) + '.pth')\n",
    "    train_losses[epoch] /= n_iter\n",
    "    print('Loss: %f' %(train_losses[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('trained_models_12_2/epoch19.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # Disable dropout to make results repeatable\n",
    "\n",
    "time_steps = 200 # How many time steps do we sample?\n",
    "\n",
    "# Start with a time shift\n",
    "gen_history = torch.zeros((1, 1, 2), dtype=torch.long)\n",
    "gen_history[0, 0, 0] = 387\n",
    "\n",
    "# Violin\n",
    "instruments = torch.zeros((2, 1), dtype=torch.long)\n",
    "instruments[0, 0] = 2\n",
    "instruments[1, 0] = num_instruments\n",
    "    \n",
    "hidden = (torch.zeros(3, 1, hidden_size), torch.zeros(3, 1, hidden_size))\n",
    "for t in range(0, time_steps):\n",
    "    ret, hidden = model.forward_generate(gen_history[-1].view(1, 1, 2), instruments, hidden)\n",
    "    gen_history = torch.cat((gen_history, ret), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('gen_history.npy', gen_history.squeeze(1).detach().numpy())\n",
    "np.save('gen_instruments.npy', [instrument_numbers[i] for i in instruments[:-1, 0]])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
