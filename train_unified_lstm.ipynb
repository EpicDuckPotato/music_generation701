{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_notes = 128\n",
    "num_time_shifts = 100\n",
    "num_velocities = 32\n",
    "message_dim = 2*num_notes + num_velocities + num_time_shifts\n",
    "instrument_numbers = [0, 6, 40, 41, 42, 43, 45, 60, 68, 70, 71, 73]\n",
    "num_instruments = len(instrument_numbers)\n",
    "max_channels = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble LSTM definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://pytorch.org/tutorials/beginner/transformer_tutorial.html.\n",
    "# Only change is the view/expand in forward (accounts for batches)\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.shape[0], :].unsqueeze(1).expand(-1, x.shape[1], -1)\n",
    "        return self.dropout(x)\n",
    "        \n",
    "\n",
    "# EnsembleLSTM: takes a history of MIDI messages \n",
    "# for instruments in an ensemble and generates a distribution for the next message,\n",
    "# as well as the instrument who should issue the message\n",
    "class EnsembleLSTM(torch.nn.Module):\n",
    "    # CONSTRUCTOR\n",
    "    # ARGUMENTS\n",
    "    # message_dim: dimension of a MIDI message\n",
    "    # embed_dim: dimension of message embedding\n",
    "    # num_instruments: number of instrument labels\n",
    "    # hidden_size: size of hidden LSTM state\n",
    "    # heads: number of attention heads\n",
    "    # recurrent_layers: the number of layers in the lstm\n",
    "    def __init__(self, message_dim, embed_dim, num_instruments, hidden_size, heads, recurrent_layers=3):\n",
    "        super(EnsembleLSTM, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # We add the tanhed instrument embedding to each input message\n",
    "        self.i_embedding = torch.nn.Embedding(num_instruments, embed_dim)\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(message_dim, embed_dim)\n",
    "        \n",
    "        # A 3-layer LSTM takes the history of messages (concatenated with their\n",
    "        # associated instrument encoding) and produces a decoding\n",
    "        self.lstm = torch.nn.LSTM(embed_dim, hidden_size, num_layers=recurrent_layers)\n",
    "\n",
    "        # The decoding is passed through a linear layer to get the logits for the next message        \n",
    "        self.message_logits = torch.nn.Linear(hidden_size, message_dim)\n",
    "        \n",
    "        # The decoding becomes a query for attention across the instruments, which is used to\n",
    "        # predict the next instrument\n",
    "        self.inst_query = torch.nn.Linear(hidden_size, embed_dim)\n",
    "        self.inst_attention = torch.nn.MultiheadAttention(embed_dim, heads)\n",
    "        \n",
    "        # Indicates which channel is associated with each instrument\n",
    "        self.position_encoding = PositionalEncoding(embed_dim)\n",
    "    \n",
    "    # forward: generates a probability distribution for the next MIDI message\n",
    "    # and the channel that issues the message, given a message history for the instrument ensemble\n",
    "    # ARGUMENTS\n",
    "    # history: an LxBx2 tensor, where L is the length of the longest message history in\n",
    "    # the batch, and B is the batch size. The first index along dimension 2 stores the\n",
    "    # message number. The second stores the channel number. This should be END-PADDED\n",
    "    # along dimension 0. All time shifts should be associated with channel -1.\n",
    "    # mask: an LxB tensor, containing True in any locations where history contains\n",
    "    # padding\n",
    "    # instruments: a CxB tensor indicating the instrument number for each channel, where\n",
    "    # N is the maximum number of channels in the batch. This should be END-PADDED along dimension 0\n",
    "    # inst_mask: contains False where an instrument exists and True elsewhere\n",
    "    # RETURN: two tensors. The first is LxBxD, representing the distribution for the next message at each time\n",
    "    # step (need to take the softmax to get actual probabilities). The second is LxBxC, representing the\n",
    "    # distribution for the next channel at each time step (need to take the softmax to get actual probabilities)\n",
    "    def forward(self, history, mask, instruments, inst_mask):\n",
    "        L = history.shape[0] # longest length\n",
    "        B = history.shape[1] # batch size\n",
    "        N = instruments.shape[0]\n",
    "        assert(mask.shape == (L, B))\n",
    "        \n",
    "        # CxBxD\n",
    "        inst_embed = self.position_encoding(torch.tanh(self.i_embedding(instruments)))\n",
    "        \n",
    "        # Which messages are time shifts?\n",
    "        time_shift_mask = history[:, :, 1] < 0\n",
    "        \n",
    "        # LxBxD, instrument embedding associated with each message\n",
    "        inst_sel = history[:, :, 1].unsqueeze(2).expand(-1, -1, self.embed_dim).clone()\n",
    "        inst_sel[time_shift_mask] = 0\n",
    "        \n",
    "        inst_tags = torch.gather(inst_embed, 0, inst_sel)\n",
    "        inst_tags[time_shift_mask] = 0\n",
    "        \n",
    "        # LxBxD\n",
    "        inputs = self.embedding(history[:, :, 0]) + inst_tags\n",
    "        \n",
    "        decoding, last_hidden = self.lstm(inputs)\n",
    "        \n",
    "        # LxBxD\n",
    "        message_dist = self.message_logits(decoding)\n",
    "        \n",
    "        # channel_dist (BxLxN) contains the attention weights for each instrument.\n",
    "        # We have L queries (the elements of decoding). Our keys and values\n",
    "        # are the instrument embeddings\n",
    "        att_out, channel_dist = self.inst_attention(self.inst_query(decoding), \\\n",
    "                                                    inst_embed, inst_embed,\n",
    "                                                    key_padding_mask = inst_mask.transpose(0, 1))\n",
    "        \n",
    "        return message_dist, channel_dist.transpose(0, 1)\n",
    "    \n",
    "    # forward: generates a probability distribution for the next MIDI message\n",
    "    # and the channel that should issue it, given the previous message and channel,\n",
    "    # as well as the LSTM hidden state\n",
    "    # ARGUMENTS\n",
    "    # last_token: a 1x1x2 tensor. The first index along dimension 2 stores the\n",
    "    # message number. The second stores the channel number\n",
    "    # instruments: a Cx1 tensor indicating the instrument number for each channel, where C is the number of channels\n",
    "    # The the last channel index should contain num_instruments (indicating the \"time-shift instrument\")\n",
    "    # hidden: the last hidden state for the LSTM\n",
    "    # RETURN: a 1x1x2 tensor, predicting the next message and the channel that should issue it, as well as the new\n",
    "    def forward_generate(self, last_token, instruments, hidden):\n",
    "        assert(last_token.shape == (1, 1, 2))\n",
    "        C = instruments.shape[0]\n",
    "        \n",
    "        # Cx1xD\n",
    "        inst_embed = self.position_encoding(torch.tanh(self.i_embedding(instruments)))\n",
    "        \n",
    "        time_shift_mask = last_token[:, :, 1] < 0\n",
    "        inst_sel = last_token[:, :, 1].unsqueeze(2).expand(-1, -1, self.embed_dim).clone()\n",
    "        inst_sel[time_shift_mask] = 0\n",
    "        \n",
    "        # 1x1xD, instrument embedding associated with each message\n",
    "        inst_tags = torch.gather(inst_embed, 0, inst_sel)\n",
    "        inst_tags[time_shift_mask] = 0\n",
    "        \n",
    "        # 1x1xD\n",
    "        inputs = self.embedding(last_token[:, :, 0]) + inst_tags\n",
    "        \n",
    "        decoding, new_hidden = self.lstm(inputs, hidden)\n",
    "        \n",
    "        # 1x1xD\n",
    "        message_dist = self.message_logits(decoding)\n",
    "        \n",
    "        # channel_dist (1x1xC) contains the attention weights for each instrument.\n",
    "        # We have 1 query (the decoding). Our keys and values\n",
    "        # are the instrument embeddings\n",
    "        att_out, channel_dist = self.inst_attention(self.inst_query(decoding), \\\n",
    "                                                    inst_embed, inst_embed)\n",
    "        \n",
    "        message = torch.multinomial(torch.softmax(message_dist.flatten(), dim=0), 1)\n",
    "        channel = torch.multinomial(channel_dist.flatten(), 1)\n",
    "        \n",
    "        #message = torch.argmax(torch.softmax(message_dist.flatten(), dim=0))\n",
    "        #channel = torch.argmax(channel_dist.flatten())\n",
    "        \n",
    "        ret = torch.cat((message.view(1, 1, 1), channel.view(1, 1, 1)), dim=2)\n",
    "        \n",
    "        return ret, new_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests for EnsembleLSTM\n",
    "We train with model.eval() to disable dropout, since these tests try to get the model to overfit to a small sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model to overfit to a single song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "hidden_size = 1024\n",
    "heads = 4\n",
    "\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleLSTM(message_dim, embed_dim, num_instruments, hidden_size, heads)\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "    \n",
    "model.eval() # Training with eval just to see if we can overfit without dropout\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('overfit_single_song2.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recording = np.load('train_unified/recording0.npy', allow_pickle=True)\n",
    "instruments_np = np.load('train_unified/instruments0.npy', allow_pickle=True)\n",
    "\n",
    "nsamples = 100\n",
    "\n",
    "history = torch.tensor(recording[:nsamples], dtype=torch.long).view(-1, 1, 2)\n",
    "mask = torch.zeros((history.shape[0], history.shape[1]), dtype=torch.bool)\n",
    "instruments = torch.tensor([instrument_numbers.index(i) for i in instruments_np], dtype=torch.long).view(-1, 1)\n",
    "inst_mask = torch.zeros(instruments.shape, dtype=torch.bool)\n",
    "\n",
    "num_channels = instruments.shape[0]\n",
    "\n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "message_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "channel_loss_fn = torch.nn.NLLLoss(ignore_index=-1)\n",
    "epochs = 500\n",
    "train_losses = np.zeros(epochs)\n",
    "\n",
    "target_messages = history[1:, :, 0].flatten()\n",
    "target_channels = history[1:, :, 1].flatten()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    \n",
    "    message_logits, channel_probs = model(history[:-1], mask[:-1], instruments, inst_mask)\n",
    "    channel_log_probs = torch.log(channel_probs + 1e-10)\n",
    "    \n",
    "    loss = message_loss_fn(message_logits.view(-1, message_dim), target_messages) + \\\n",
    "           channel_loss_fn(channel_log_probs.view(-1, instruments.shape[0]), target_channels)\n",
    "                \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses[epoch] = loss.data\n",
    "    print('Loss: %f' %(loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'overfit_single_song2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sample from model\n",
    "gen_history = history[0].unsqueeze(0)\n",
    "mask = torch.zeros((1, 1), dtype=torch.bool)\n",
    "model.eval() # Turns off the dropout for evaluation. Need to do this to get repeatable evaluation outputs\n",
    "\n",
    "# Move forward in time\n",
    "wrong_cnt = 0\n",
    "hidden = (torch.zeros(3, 1, hidden_size), torch.zeros(3, 1, hidden_size))\n",
    "for t in range(0, history.shape[0] - 1):\n",
    "    ret, hidden = model.forward_generate(gen_history[-1].view(1, 1, 2), instruments, hidden)\n",
    "    \n",
    "    gen_history = torch.cat((gen_history, ret), dim=0)\n",
    "    \n",
    "    if gen_history[-1, 0, 0] != history[t + 1, 0, 0]:\n",
    "        print('Wrong message at time %d!' %(t))\n",
    "        wrong_cnt += 1\n",
    "        \n",
    "    if gen_history[-1, 0, 1] != history[t + 1, 0, 1]:\n",
    "        print('Wrong instrument at time %d!' %(t))\n",
    "        wrong_cnt += 1\n",
    "    \n",
    "    mask = torch.cat((mask, torch.zeros((1, 1), dtype=torch.bool)), dim=0)\n",
    "\n",
    "print(wrong_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test_history.npy', gen_history.squeeze(1).detach().numpy())\n",
    "np.save('test_instruments.npy', [instrument_numbers[i] for i in instruments[:-1, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class MIDIDataset(torch.utils.data.Dataset):\n",
    "    # CONSTRUCTOR: creates a tensor of message chunks and associated instruments.\n",
    "    # Assumes that the directory contains recording0.npy to recordingM.npy\n",
    "    # as well as instruments0.npy to instrumentsM.npy\n",
    "    # ARGUMENTS\n",
    "    # root_dir: the directory to search\n",
    "    # chunk_size: we'll chunk the data into chunks of this size (or less)\n",
    "    # max_channels: what's the largest number of instruments in any file?\n",
    "    def __init__(self, root_dir, chunk_size, max_channels, transform=None):\n",
    "        files = os.listdir(root_dir)\n",
    "        recording_files = []\n",
    "        instrument_files = []\n",
    "        for file in files:\n",
    "            if 'recording' in file:\n",
    "                recording_files.append(os.path.join(root_dir, file))\n",
    "            elif 'instruments' in file:\n",
    "                instrument_files.append(os.path.join(root_dir, file))\n",
    "                \n",
    "        assert(len(recording_files) == len(instrument_files))\n",
    "        recording_files.sort()\n",
    "        instrument_files.sort()\n",
    "        \n",
    "        self.chunks = []\n",
    "        self.masks = []\n",
    "        self.instruments = []\n",
    "        self.inst_masks = []\n",
    "        \n",
    "        ch = 0\n",
    "        for f in range(len(recording_files)):\n",
    "            recording = np.load(recording_files[f], allow_pickle=True)\n",
    "            inst = [instrument_numbers.index(i) for i in np.load(instrument_files[f], allow_pickle=True)]\n",
    "            \n",
    "            nchunks = int(np.ceil(recording.shape[0]/chunk_size))\n",
    "            self.chunks += [torch.zeros((chunk_size, 2), dtype=torch.long) for c in range(nchunks)]\n",
    "            self.masks += [torch.ones(chunk_size, dtype=torch.bool) for c in range(nchunks)]\n",
    "            self.instruments += [torch.zeros(max_channels, dtype=torch.long) for c in range(nchunks)]\n",
    "            self.inst_masks += [torch.ones(max_channels, dtype=torch.long) for c in range(nchunks)]\n",
    "            for chunk_start in range(0, recording.shape[0], chunk_size):\n",
    "                chunk_end = min(chunk_start + chunk_size, recording.shape[0])\n",
    "                size = chunk_end - chunk_start\n",
    "                self.chunks[ch][:size] = torch.tensor(recording[chunk_start:chunk_end], dtype=torch.long)\n",
    "                self.masks[ch][:size] = False\n",
    "                self.instruments[ch][:len(inst)] = torch.tensor(inst, dtype=torch.long)\n",
    "                self.inst_masks[ch][:len(inst)] = False\n",
    "                ch += 1\n",
    "            \n",
    "        self.transform = transform\n",
    "\n",
    "    # __len__\n",
    "    # RETURN: the number of recording chunks in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "\n",
    "    # __getitem__\n",
    "    # ARGUMENTS\n",
    "    # idx: indicates which chunk(s) to get\n",
    "    # RETURN: instance, a dictionary with keys 'history' and 'instruments'\n",
    "    # instance['history'] is an Lx2 tensor containing messages and associated channels\n",
    "    # instance['instruments'] a length N tensor of instrument numbers\n",
    "    # instance['mask'] a length L tensor containing False where messages exist and True otherwise\n",
    "    # instance['inst_mask'] a length N tensor containing False where instruments exist and True otherwise\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        instance = {'history': self.chunks[idx], \\\n",
    "                    'instruments': self.instruments[idx],\n",
    "                    'mask': self.masks[idx],\n",
    "                    'inst_mask': self.inst_masks[idx]}\n",
    "        \n",
    "        if self.transform:\n",
    "            instance = self.transform(instance)\n",
    "            \n",
    "        return instance\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    chunk_size = batch[0]['history'].shape[0]\n",
    "    max_channels = batch[0]['instruments'].shape[0]\n",
    "    sample = {'history': torch.zeros((chunk_size, len(batch), 2), dtype=torch.long), \\\n",
    "              'instruments': torch.ones((max_channels, len(batch)), dtype=torch.long), \\\n",
    "              'mask': torch.ones((chunk_size, len(batch)), dtype=torch.bool),\n",
    "              'inst_mask': torch.ones((max_channels, len(batch)), dtype=torch.bool)}\n",
    "    \n",
    "    for b, instance in enumerate(batch):\n",
    "        sample['history'][:, b] = instance['history']\n",
    "        sample['instruments'][:, b] = instance['instruments']\n",
    "        sample['mask'][:, b] = instance['mask']\n",
    "        sample['inst_mask'][:, b] = instance['inst_mask']\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_loss: computes the loss for the model over the batch\n",
    "# ARGUMENTS\n",
    "# model: EnsembleLSTM model\n",
    "# message_loss_fn: torch.nn.CrossEntropyLoss object\n",
    "# channel_loss_fn: torch.nn.NLLLoss object\n",
    "# batch: see collate_fn definition\n",
    "# RETURN: a scalar loss tensor\n",
    "def compute_loss(model, message_loss_fn, channel_loss_fn, batch):  \n",
    "    max_seq_length = batch['history'].shape[0]\n",
    "\n",
    "    message_logits, channel_dist = model(batch['history'][:-1], batch['mask'][:-1], batch['instruments'], batch['inst_mask'])\n",
    "    log_channel_dist = torch.log(channel_dist + 1e-10)\n",
    "\n",
    "    target_mask = torch.logical_not(batch['mask'][1:])\n",
    "\n",
    "    message_loss = message_loss_fn(message_logits[target_mask], batch['history'][1:, :, 0][target_mask])\n",
    "    channel_loss = channel_loss_fn(log_channel_dist[target_mask], batch['history'][1:, :, 1][target_mask])\n",
    "\n",
    "    return message_loss + channel_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "hidden_size = 1024\n",
    "heads = 4\n",
    "\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleLSTM(message_dim, embed_dim, num_instruments, hidden_size, heads)\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "chunk_size = 500\n",
    "\n",
    "train_dataset = MIDIDataset('train_unified', chunk_size, max_channels)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "test_dataset = MIDIDataset('test_unified', chunk_size, max_channels)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "message_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "channel_loss_fn = torch.nn.NLLLoss(ignore_index=-1)\n",
    "epochs = 20\n",
    "train_losses = np.zeros(epochs)\n",
    "test_losses = np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    model.train()\n",
    "    for b, batch in enumerate(train_dataloader):\n",
    "        print('Starting iteration %d' %(b))\n",
    "        loss = compute_loss(model, message_loss_fn, channel_loss_fn, batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    torch.save(model.state_dict(), 'unified_transformer_models/epoch' + str(epoch) + '.pth')\n",
    "\n",
    "    print('Computing test loss')\n",
    "    model.eval()\n",
    "    for batch in test_dataloader:\n",
    "        loss = compute_loss(model, message_loss_fn, channel_loss_fn, batch)\n",
    "        test_losses[epoch] += loss.data\n",
    "        \n",
    "    print('Computing train loss')\n",
    "    for batch in train_dataloader:\n",
    "        loss = compute_loss(model, message_loss_fn, channel_loss_fn, batch)\n",
    "        train_losses[epoch] += loss.data\n",
    "    \n",
    "    train_losses[epoch] /= len(train_dataloader)\n",
    "    test_losses[epoch] /= len(test_dataloader)\n",
    "    print('Train Loss: %f, Test Loss: %f' %(train_losses[epoch], test_losses[epoch])),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('trained_models_12_3/epoch4.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # Disable dropout to make results repeatable\n",
    "\n",
    "time_steps = 500 # How many time steps do we sample?\n",
    "\n",
    "# Start with a time shift\n",
    "gen_history = torch.zeros((1, 1, 2), dtype=torch.long)\n",
    "gen_history[0, 0, 0] = 387\n",
    "gen_history[0, 0, 1] = -1\n",
    "\n",
    "# Violin\n",
    "instruments = torch.zeros((1, 1), dtype=torch.long)\n",
    "instruments[0, 0] = 2\n",
    "    \n",
    "hidden = (torch.zeros(3, 1, hidden_size), torch.zeros(3, 1, hidden_size))\n",
    "for t in range(0, time_steps):\n",
    "    ret, hidden = model.forward_generate(gen_history[-1].view(1, 1, 2), instruments, hidden)\n",
    "    gen_history = torch.cat((gen_history, ret), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('gen_history.npy', gen_history.squeeze(1).detach().numpy())\n",
    "np.save('gen_instruments.npy', [instrument_numbers[i] for i in instruments[:, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
