{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Haydn\n",
      "Starting Beethoven\n",
      "Starting Cambini\n",
      "Starting Schubert\n",
      "Starting Faure\n",
      "Starting Brahms\n",
      "Starting Dvorak\n",
      "Starting Bach\n",
      "Starting Mozart\n",
      "Starting Ravel\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ce73509b1e43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mfnum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"preprocessed_data_fnames.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "import pretty_midi\n",
    "import os\n",
    "import numpy as np\n",
    "import heapq\n",
    "import pickle\n",
    "\n",
    "# See https://jazz-soft.net/demo/GeneralMidi.html for which instrument each number represents\n",
    "#instruments = [0, 6, 40, 41, 42, 43, 45, 60, 68, 70, 71, 73]\n",
    "\n",
    "num_notes = 128 # Number of pitches in MIDI\n",
    "\n",
    "# We allow the network to step itself forward in time in increments of 10 ms.\n",
    "# Thus the network can shift between 10 ms and 1 s, inclusive\n",
    "num_time_shifts = 100 \n",
    "min_time_shift = 0.01\n",
    "max_time_shift = min_time_shift*num_time_shifts\n",
    "\n",
    "# Represents dynamics. There are 128 possible velocities but the music transformer\n",
    "# uses 32, so that's what we'll use here\n",
    "num_velocities = 32\n",
    "\n",
    "# A message can be NOTE_ON, NOTE_OFF, TIME_SHIFT, or SET_VELOCITY\n",
    "message_dim = 2*num_notes + num_time_shifts + num_velocities\n",
    "\n",
    "# quantize_velocity: takes a MIDI velocity value and puts it into the correct bin\n",
    "# in our reduced representation\n",
    "# ARGUMENTS\n",
    "# velocity: a MIDI velocity between 0 and 127 (inclusive)\n",
    "# RETURN: the quantized velocity\n",
    "def quantize_velocity(velocity):\n",
    "    return int(velocity*num_velocities/128)\n",
    "\n",
    "# note_on_event: generates one-hot encoding for a NOTE_ON event\n",
    "# ARGUMENTS\n",
    "# note: the MIDI number for the note to be played\n",
    "# RETURN: a one-hot encoding of the NOTE_ON message\n",
    "def note_on_event(note):\n",
    "    ret = np.zeros(message_dim)\n",
    "    ret[note] = 1\n",
    "    return ret\n",
    "\n",
    "# note_off_event: generates one-hot encoding for a NOTE_OFF event\n",
    "# ARGUMENTS\n",
    "# note: the MIDI number for the note to be turned off\n",
    "# RETURN: a one-hot encoding of the NOTE_OFF message\n",
    "def note_off_event(note):\n",
    "    ret = np.zeros(message_dim)\n",
    "    ret[num_notes + note] = 1\n",
    "    return ret\n",
    "\n",
    "# velocity_event: generates one-hot encoding for a quantized velocity\n",
    "# ARGUMENTS\n",
    "# velocity: a quantized MIDI velocity according to our quantization scheme\n",
    "# RETURN: a one-hot encoding of the SET_VELOCITY message\n",
    "def velocity_event(velocity):\n",
    "    ret = np.zeros(message_dim)\n",
    "    ret[2*num_notes + velocity] = 1\n",
    "    return ret\n",
    "\n",
    "# time_shift_event: generates one-hot encoding for a TIME_SHIFT event. This function\n",
    "# will throw an error if the shift amount is greater than min_time_shift*num_time_shifts\n",
    "# ARGUMENTS\n",
    "# time_shift: the number of seconds to shift (must be greater than 0)\n",
    "# RETURN: a one-hot encoding of the TIME_SHIFT message\n",
    "def time_shift_event(time_shift):\n",
    "    assert(time_shift > 0 and time_shift <= max_time_shift)\n",
    "    ret = np.zeros(message_dim)\n",
    "    # We subtract 1 here because \"int(np.ceil(time_shift/min_time_shift))\"\n",
    "    # gives us the number of steps to shift by. Since the minimum number of steps\n",
    "    # is 1, index 0 of this segment represents a shift by 1\n",
    "    ret[2*num_notes + num_velocities + int(np.ceil(time_shift/min_time_shift)) - 1] = 1\n",
    "    return ret\n",
    "\n",
    "base_path = 'musicnet_midis/'\n",
    "\n",
    "fnum = 0 # Which file are we writing currently?\n",
    "\n",
    "data_fnames = [] # Save file name corresponding to each numpy array\n",
    "for composer in os.listdir(base_path):\n",
    "    print('Starting ' + composer)\n",
    "    for fname in os.listdir(base_path + composer):\n",
    "        try:\n",
    "            mid = pretty_midi.PrettyMIDI(base_path + composer + '/' + fname)\n",
    "        except:\n",
    "            # There are 7 files that cause an IO error, both with mido and pretty_midi. Haven't looked into why\n",
    "            continue\n",
    "        \n",
    "        # Store data in a numpy array of 2d numpy arrays. Each index in the outer numpy array represents an instrument\n",
    "        # in the file. Each inner numpy array is LxD, where L is the number of MIDI messages associated with the instrument,\n",
    "        # and D is the dimension of our message representation\n",
    "        data = np.array([0 for i in range(len(mid.instruments))], dtype='object') \n",
    "        for i, instrument in enumerate(mid.instruments):\n",
    "            data[i] = []\n",
    "            \n",
    "            # Quantized value of the last velocity message\n",
    "            last_velocity = -1\n",
    "            \n",
    "            # Priority queue of notes to turn off and the times to turn them off.\n",
    "            # Specifically, this is a list of tuples of the form (off_time, pitch),\n",
    "            # where the first element of the list is always the next note to turn off\n",
    "            off_queue = []\n",
    "            for n, note in enumerate(instrument.notes):\n",
    "                # We need to turn off a note\n",
    "                while off_queue and note.start > off_queue[0][0]:\n",
    "                    data[i].append(note_off_event(off_queue[0][1]))\n",
    "                    heapq.heappop(off_queue)\n",
    "                    \n",
    "                velocity = quantize_velocity(note.velocity)\n",
    "                if velocity != last_velocity:\n",
    "                    # We have a new velocity. Add a SET_VELOCITY event\n",
    "                    last_velocity = velocity\n",
    "                    data[i].append(velocity_event(velocity))\n",
    "                \n",
    "                data[i].append(note_on_event(note.pitch))\n",
    "                \n",
    "                # Add this note to the queue of notes needing to be turned off\n",
    "                heapq.heappush(off_queue, (note.end, note.pitch))\n",
    "                \n",
    "                if n == len(instrument.notes) - 1:\n",
    "                    # No more notes left. Flush the off queue\n",
    "                    while off_queue:\n",
    "                        data[i].append(note_off_event(off_queue[0][1]))\n",
    "                        heapq.heappop(off_queue)\n",
    "                else:\n",
    "                    time_shift = min(off_queue[0][0], instrument.notes[n + 1].start) - note.start\n",
    "                    \n",
    "                    # Split large  time shifts into multiple small time shifts\n",
    "                    while (time_shift > max_time_shift):\n",
    "                        data[i].append(time_shift_event(max_time_shift))\n",
    "                        time_shift -= max_time_shift\n",
    "                        \n",
    "                    if (time_shift > 0):\n",
    "                        data[i].append(time_shift_event(time_shift))\n",
    "                    \n",
    "            data[i] = np.array(data[i])\n",
    "        \n",
    "        np.save('preprocessed_data/recording' + str(fnum) + '.npy', data)\n",
    "        \n",
    "        # Also save a numpy array containing the MIDI number for each instrument\n",
    "        instruments = np.array([instrument.program for instrument in mid.instruments])\n",
    "        np.save('preprocessed_data/instruments' + str(fnum) + '.npy', instruments)\n",
    "        \n",
    "        data_fnames.append(composer + '/' + fname)\n",
    "        fnum += 1\n",
    "        \n",
    "pickle.dump(data_fnames, open( \"preprocessed_data_fnames.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arr = np.load('preprocessed_data/recording5.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(test_arr[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beethoven/2522_vcs3_2.mid'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fnames[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Note(start=5.032248, end=5.417328, pitch=52, velocity=70)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mid = pretty_midi.PrettyMIDI(base_path + data_fnames[5])\n",
    "mid.instruments[0].notes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.5"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*70/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(test_arr[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3850800000000003"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5.417328 - 5.032248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m58",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m58"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
