{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_notes = 128\n",
    "num_time_shifts = 100\n",
    "message_dim = 2*num_notes + num_time_shifts\n",
    "program_numbers = [0, 6, 40, 41, 42, 43, 45, 60, 68, 70, 71, 73]\n",
    "max_channels = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(message_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EnsembleTransformer definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://pytorch.org/tutorials/beginner/transformer_tutorial.html.\n",
    "# Only change is the view/expand in forward (accounts for batches)\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.shape[0], :].unsqueeze(1).expand(-1, x.shape[1], -1)\n",
    "        return self.dropout(x)\n",
    "\n",
    "# EnsembleTransformer: takes a history of MIDI messages \n",
    "# for instruments in an ensemble and generates a distribution for the next message,\n",
    "# as well as the instrument (the channel) who should issue the message\n",
    "class EnsembleTransformer(torch.nn.Module):\n",
    "    # CONSTRUCTOR\n",
    "    # ARGUMENTS\n",
    "    # message_dim: dimension of a MIDI message\n",
    "    # embed_dim: dimension of message embedding\n",
    "    # heads: number of attention heads\n",
    "    # attention_layers: number of attention layers\n",
    "    # ff_size: size of the feedforward output at the end of the decoder\n",
    "    def __init__(self, message_dim, embed_dim, heads, attention_layers, ff_size):\n",
    "        super(EnsembleTransformer, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Indicates channel numbers, as well as\n",
    "        # the position of messages in time\n",
    "        self.position_encoding = PositionalEncoding(embed_dim)\n",
    "        \n",
    "        # The decoder computes attention over the message history, using the channel\n",
    "        # encodings as memory\n",
    "        self.embedding = torch.nn.Embedding(message_dim, embed_dim)\n",
    "        decoder_layer = torch.nn.TransformerDecoderLayer(2*embed_dim, heads, ff_size)\n",
    "        self.decoder = torch.nn.TransformerDecoder(decoder_layer, attention_layers)\n",
    "\n",
    "        # The decoding is passed through a linear layer to get the logits for the next message        \n",
    "        self.message_logits = torch.nn.Linear(2*embed_dim, message_dim)\n",
    "        \n",
    "        # The decoding becomes a query for attention across the channels, which is used to\n",
    "        # predict the next channel\n",
    "        self.channel_attention = torch.nn.MultiheadAttention(2*embed_dim, heads)\n",
    "    \n",
    "    # forward: generates a probability distribution for the next MIDI message\n",
    "    # and the channel that issues the message, given a message history for the ensemble\n",
    "    # ARGUMENTS\n",
    "    # history: an LxBx2 tensor, where L is the length of the longest message history in\n",
    "    # the batch, and B is the batch size. The first index along dimension 2 stores the\n",
    "    # message number. The second stores the channel number. This should be END-PADDED\n",
    "    # along dimension 0. All time shifts should be associated with channel -1.\n",
    "    # mask: an LxB tensor, containing True in any locations where history contains\n",
    "    # padding and False elsewhere\n",
    "    # nchan: a length B tensor telling us how many channels are in each instance in the batch\n",
    "    # RETURN: two tensors. The first is LxBxD, representing the distribution for the next message at each time\n",
    "    # step (need to take the softmax to get actual probabilities). The second is LxBxC, representing the\n",
    "    # distribution for the next channel at each time step\n",
    "    def forward(self, history, mask, nchan):\n",
    "        L = history.shape[0] # longest length\n",
    "        B = history.shape[1] # batch size\n",
    "        assert(mask.shape == (L, B))\n",
    "        \n",
    "        max_chan = torch.max(nchan)\n",
    "        channels = torch.zeros((max_chan, B, 2*self.embed_dim))\n",
    "        channels[:, :, :embed_dim] = self.position_encoding(torch.zeros((max_chan, B, self.embed_dim)))\n",
    "        \n",
    "        # Contains False where a channel exists and True otherwise\n",
    "        channel_mask = torch.ones((max_chan, B), dtype=torch.bool)\n",
    "        \n",
    "        for b in range(B):\n",
    "            channel_mask[:nchan[b], b] = False\n",
    "        \n",
    "        # Which messages are time shifts?\n",
    "        time_shift_mask = history[:, :, 1] < 0\n",
    "        \n",
    "        # LxBxD\n",
    "        channel_sel = history[:, :, 1].unsqueeze(2).expand(-1, -1, self.embed_dim).clone()\n",
    "        channel_sel[time_shift_mask] = 0\n",
    "        \n",
    "        channel_tags = torch.gather(channels, 0, channel_sel)\n",
    "        channel_tags[time_shift_mask] = 0\n",
    "        \n",
    "        # LxBxD\n",
    "        decoder_inputs = torch.cat((self.position_encoding(self.embedding(history[:, :, 0])), \\\n",
    "                                    channel_tags), dim=2)\n",
    "        \n",
    "        tgt_mask = torch.triu(torch.ones((L, L), dtype=torch.bool))\n",
    "        tgt_mask.fill_diagonal_(False)\n",
    "        tgt_key_padding_mask = mask.transpose(0, 1)\n",
    "        \n",
    "        decoding = self.decoder(decoder_inputs, channels, \\\n",
    "                                tgt_mask=tgt_mask, \\\n",
    "                                tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                memory_key_padding_mask=channel_mask.transpose(0, 1))\n",
    "        \n",
    "        # LxBxD\n",
    "        message_dist = self.message_logits(decoding)\n",
    "        \n",
    "        # channel_dist (BxLxC) contains the attention weights for each channel.\n",
    "        # We have L queries (the elements of decoding). Our keys and values\n",
    "        # are the instrument embeddings\n",
    "        att_out, channel_probs = self.channel_attention(decoding, \\\n",
    "                                                        channels, channels,\n",
    "                                                        key_padding_mask=channel_mask.transpose(0, 1))\n",
    "        \n",
    "        return message_dist, channel_probs.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests for EnsembleTransformer\n",
    "We train with model.eval() to disable dropout, since these tests try to get the model to overfit to a small sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model to overfit to a single song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "heads = 4\n",
    "attention_layers = 6\n",
    "ff_size = 512\n",
    "\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleTransformer(message_dim, embed_dim, heads, attention_layers, ff_size)    \n",
    "model.eval() # Training with eval just to see if we can overfit without dropout\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('unified_transformer.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "torch.Size([8, 1, 512])\n",
      "torch.Size([499, 1, 256])\n",
      "Loss: 8.239785\n",
      "Starting epoch 1\n",
      "torch.Size([8, 1, 512])\n",
      "torch.Size([499, 1, 256])\n",
      "Loss: 5.928912\n",
      "Starting epoch 2\n",
      "torch.Size([8, 1, 512])\n",
      "torch.Size([499, 1, 256])\n",
      "Loss: 6.054917\n",
      "Starting epoch 3\n",
      "torch.Size([8, 1, 512])\n",
      "torch.Size([499, 1, 256])\n",
      "Loss: 5.482859\n",
      "Starting epoch 4\n",
      "torch.Size([8, 1, 512])\n",
      "torch.Size([499, 1, 256])\n",
      "Loss: 5.128570\n",
      "Starting epoch 5\n",
      "torch.Size([8, 1, 512])\n",
      "torch.Size([499, 1, 256])\n",
      "Loss: 4.949430\n",
      "Starting epoch 6\n",
      "torch.Size([8, 1, 512])\n",
      "torch.Size([499, 1, 256])\n",
      "Loss: 4.846774\n",
      "Starting epoch 7\n",
      "torch.Size([8, 1, 512])\n",
      "torch.Size([499, 1, 256])\n",
      "Loss: 4.692304\n",
      "Starting epoch 8\n",
      "torch.Size([8, 1, 512])\n",
      "torch.Size([499, 1, 256])\n",
      "Loss: 4.658298\n",
      "Starting epoch 9\n",
      "torch.Size([8, 1, 512])\n",
      "torch.Size([499, 1, 256])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-906694758946>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Starting epoch %d'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mmessage_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnchan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mchannel_log_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel_probs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-d1947fccf977>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, history, mask, nchan)\u001b[0m\n\u001b[1;32m    102\u001b[0m                                 \u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                                 \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                                 memory_key_padding_mask=channel_mask.transpose(0, 1))\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# LxBxD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    235\u001b[0m                          \u001b[0mmemory_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                          \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m                          memory_key_padding_mask=memory_key_padding_mask)\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n\u001b[0;32m--> 373\u001b[0;31m                                    key_padding_mask=memory_key_padding_mask)[0]\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m    887\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                 attn_mask=attn_mask)\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   4130\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbsz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4131\u001b[0m     \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4132\u001b[0;31m     \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "recording = np.load('train_unified/recording0.npy', allow_pickle=True)\n",
    "instruments_np = np.load('train_unified/instruments0.npy', allow_pickle=True)\n",
    "\n",
    "nsamples = 500\n",
    "\n",
    "history = torch.tensor(recording[:nsamples], dtype=torch.long).view(-1, 1, 2)\n",
    "mask = torch.zeros((history.shape[0], history.shape[1]), dtype=torch.bool)\n",
    "nchan = torch.tensor([instruments_np.shape[0]])\n",
    "\n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "message_loss = torch.nn.CrossEntropyLoss()\n",
    "channel_loss = torch.nn.NLLLoss(ignore_index=-1)\n",
    "epochs = 500\n",
    "train_losses = np.zeros(epochs)\n",
    "\n",
    "target_messages = history[1:, :, 0].flatten()\n",
    "target_channels = history[1:, :, 1].flatten()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    \n",
    "    message_logits, channel_probs = model(history[:-1], mask[:-1], nchan)\n",
    "    channel_log_probs = torch.log(channel_probs + 1e-8)\n",
    "    \n",
    "    loss = message_loss(message_logits.view(-1, message_dim), target_messages) + \\\n",
    "           channel_loss(channel_log_probs.view(-1, nchan), target_channels)\n",
    "                \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses[epoch] = loss.data\n",
    "    print('Loss: %f' %(loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'unified_transformer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3f2b8defd0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 2!\n",
      "tensor(267)\n",
      "Wrong message at time 3!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 5!\n",
      "tensor(267)\n",
      "Wrong message at time 6!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 8!\n",
      "tensor(267)\n",
      "Wrong message at time 9!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 11!\n",
      "tensor(267)\n",
      "Wrong message at time 12!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 14!\n",
      "tensor(267)\n",
      "Wrong message at time 15!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 17!\n",
      "tensor(267)\n",
      "Wrong message at time 18!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 20!\n",
      "tensor(267)\n",
      "Wrong message at time 21!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 23!\n",
      "tensor(267)\n",
      "Wrong message at time 24!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 26!\n",
      "tensor(267)\n",
      "Wrong message at time 27!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 29!\n",
      "tensor(267)\n",
      "Wrong message at time 30!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 32!\n",
      "tensor(267)\n",
      "Wrong message at time 33!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 35!\n",
      "tensor(267)\n",
      "Wrong message at time 36!\n",
      "tensor(267)\n",
      "Wrong message at time 37!\n",
      "tensor(267)\n",
      "Wrong message at time 38!\n",
      "tensor(267)\n",
      "Wrong message at time 39!\n",
      "tensor(267)\n",
      "Wrong message at time 40!\n",
      "tensor(267)\n",
      "Wrong message at time 41!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 43!\n",
      "tensor(267)\n",
      "Wrong message at time 44!\n",
      "tensor(267)\n",
      "Wrong message at time 45!\n",
      "tensor(267)\n",
      "Wrong message at time 46!\n",
      "tensor(267)\n",
      "Wrong message at time 47!\n",
      "tensor(267)\n",
      "Wrong message at time 48!\n",
      "tensor(267)\n",
      "Wrong message at time 49!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 51!\n",
      "tensor(267)\n",
      "Wrong message at time 52!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 54!\n",
      "tensor(267)\n",
      "Wrong message at time 55!\n",
      "tensor(267)\n",
      "Wrong message at time 56!\n",
      "tensor(267)\n",
      "Wrong message at time 57!\n",
      "tensor(267)\n",
      "Wrong message at time 58!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 60!\n",
      "tensor(267)\n",
      "Wrong message at time 61!\n",
      "tensor(267)\n",
      "Wrong message at time 62!\n",
      "tensor(267)\n",
      "Wrong message at time 63!\n",
      "tensor(267)\n",
      "Wrong message at time 64!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 66!\n",
      "tensor(267)\n",
      "Wrong message at time 67!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 69!\n",
      "tensor(267)\n",
      "Wrong message at time 70!\n",
      "tensor(267)\n",
      "Wrong message at time 71!\n",
      "tensor(267)\n",
      "Wrong message at time 72!\n",
      "tensor(267)\n",
      "Wrong message at time 73!\n",
      "tensor(267)\n",
      "Wrong message at time 74!\n",
      "tensor(267)\n",
      "Wrong message at time 75!\n",
      "tensor(267)\n",
      "Wrong message at time 76!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 78!\n",
      "tensor(267)\n",
      "Wrong message at time 79!\n",
      "tensor(267)\n",
      "Wrong message at time 80!\n",
      "tensor(267)\n",
      "Wrong message at time 81!\n",
      "tensor(267)\n",
      "Wrong message at time 82!\n",
      "tensor(267)\n",
      "Wrong message at time 83!\n",
      "tensor(267)\n",
      "Wrong message at time 84!\n",
      "tensor(267)\n",
      "Wrong message at time 85!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 87!\n",
      "tensor(267)\n",
      "Wrong message at time 88!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 90!\n",
      "tensor(267)\n",
      "Wrong message at time 91!\n",
      "tensor(267)\n",
      "Wrong message at time 92!\n",
      "tensor(267)\n",
      "Wrong message at time 93!\n",
      "tensor(267)\n",
      "Wrong message at time 94!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 96!\n",
      "tensor(267)\n",
      "Wrong message at time 97!\n",
      "tensor(267)\n",
      "Wrong message at time 98!\n",
      "tensor(267)\n",
      "Wrong message at time 99!\n",
      "tensor(267)\n",
      "Wrong message at time 100!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 102!\n",
      "tensor(267)\n",
      "Wrong message at time 103!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 105!\n",
      "tensor(267)\n",
      "Wrong message at time 106!\n",
      "tensor(267)\n",
      "Wrong message at time 107!\n",
      "tensor(267)\n",
      "Wrong message at time 108!\n",
      "tensor(267)\n",
      "Wrong message at time 109!\n",
      "tensor(267)\n",
      "Wrong message at time 110!\n",
      "tensor(267)\n",
      "Wrong message at time 111!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 113!\n",
      "tensor(267)\n",
      "Wrong message at time 114!\n",
      "tensor(267)\n",
      "Wrong message at time 115!\n",
      "tensor(267)\n",
      "Wrong message at time 116!\n",
      "tensor(267)\n",
      "Wrong message at time 117!\n",
      "tensor(267)\n",
      "Wrong message at time 118!\n",
      "tensor(267)\n",
      "Wrong message at time 119!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 121!\n",
      "tensor(267)\n",
      "Wrong message at time 122!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 124!\n",
      "tensor(267)\n",
      "Wrong message at time 125!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 127!\n",
      "tensor(267)\n",
      "Wrong message at time 128!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 130!\n",
      "tensor(267)\n",
      "Wrong message at time 131!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 133!\n",
      "tensor(267)\n",
      "Wrong message at time 134!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 136!\n",
      "tensor(267)\n",
      "Wrong message at time 137!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 139!\n",
      "tensor(267)\n",
      "Wrong message at time 140!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 142!\n",
      "tensor(267)\n",
      "Wrong message at time 143!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 145!\n",
      "tensor(267)\n",
      "Wrong message at time 146!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 148!\n",
      "tensor(267)\n",
      "Wrong message at time 149!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 151!\n",
      "tensor(267)\n",
      "Wrong message at time 152!\n",
      "tensor(267)\n",
      "Wrong message at time 153!\n",
      "tensor(267)\n",
      "Wrong message at time 154!\n",
      "tensor(267)\n",
      "Wrong message at time 155!\n",
      "tensor(267)\n",
      "Wrong message at time 156!\n",
      "tensor(267)\n",
      "Wrong message at time 157!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 159!\n",
      "tensor(267)\n",
      "Wrong message at time 160!\n",
      "tensor(267)\n",
      "Wrong message at time 161!\n",
      "tensor(267)\n",
      "Wrong message at time 162!\n",
      "tensor(267)\n",
      "Wrong message at time 163!\n",
      "tensor(267)\n",
      "Wrong message at time 164!\n",
      "tensor(267)\n",
      "Wrong message at time 165!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 167!\n",
      "tensor(267)\n",
      "Wrong message at time 168!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 170!\n",
      "tensor(267)\n",
      "Wrong message at time 171!\n",
      "tensor(267)\n",
      "Wrong message at time 172!\n",
      "tensor(267)\n",
      "Wrong message at time 173!\n",
      "tensor(267)\n",
      "Wrong message at time 174!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 176!\n",
      "tensor(267)\n",
      "Wrong message at time 177!\n",
      "tensor(267)\n",
      "Wrong message at time 178!\n",
      "tensor(267)\n",
      "Wrong message at time 179!\n",
      "tensor(267)\n",
      "Wrong message at time 180!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 182!\n",
      "tensor(267)\n",
      "Wrong message at time 183!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 185!\n",
      "tensor(267)\n",
      "Wrong message at time 186!\n",
      "tensor(267)\n",
      "Wrong message at time 187!\n",
      "tensor(267)\n",
      "Wrong message at time 188!\n",
      "tensor(267)\n",
      "Wrong message at time 189!\n",
      "tensor(267)\n",
      "Wrong message at time 190!\n",
      "tensor(267)\n",
      "Wrong message at time 191!\n",
      "tensor(267)\n",
      "Wrong message at time 192!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 194!\n",
      "tensor(267)\n",
      "Wrong message at time 195!\n",
      "tensor(267)\n",
      "Wrong message at time 196!\n",
      "tensor(267)\n",
      "Wrong message at time 197!\n",
      "tensor(267)\n",
      "Wrong message at time 198!\n",
      "tensor(267)\n",
      "Wrong message at time 199!\n",
      "tensor(267)\n",
      "Wrong message at time 200!\n",
      "tensor(267)\n",
      "Wrong message at time 201!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 203!\n",
      "tensor(267)\n",
      "Wrong message at time 204!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 206!\n",
      "tensor(267)\n",
      "Wrong message at time 207!\n",
      "tensor(267)\n",
      "Wrong message at time 208!\n",
      "tensor(267)\n",
      "Wrong message at time 209!\n",
      "tensor(267)\n",
      "Wrong message at time 210!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 212!\n",
      "tensor(267)\n",
      "Wrong message at time 213!\n",
      "tensor(267)\n",
      "Wrong message at time 214!\n",
      "tensor(267)\n",
      "Wrong message at time 215!\n",
      "tensor(267)\n",
      "Wrong message at time 216!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 218!\n",
      "tensor(267)\n",
      "Wrong message at time 219!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 221!\n",
      "tensor(267)\n",
      "Wrong message at time 222!\n",
      "tensor(267)\n",
      "Wrong message at time 223!\n",
      "tensor(267)\n",
      "Wrong message at time 224!\n",
      "tensor(267)\n",
      "Wrong message at time 225!\n",
      "tensor(267)\n",
      "Wrong message at time 226!\n",
      "tensor(267)\n",
      "Wrong message at time 227!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 229!\n",
      "tensor(267)\n",
      "Wrong message at time 230!\n",
      "tensor(267)\n",
      "Wrong message at time 231!\n",
      "tensor(267)\n",
      "Wrong message at time 232!\n",
      "tensor(267)\n",
      "Wrong message at time 233!\n",
      "tensor(267)\n",
      "Wrong message at time 234!\n",
      "tensor(267)\n",
      "Wrong message at time 235!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 237!\n",
      "tensor(267)\n",
      "Wrong message at time 238!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 240!\n",
      "tensor(267)\n",
      "Wrong message at time 241!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 243!\n",
      "tensor(267)\n",
      "Wrong message at time 244!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 246!\n",
      "tensor(267)\n",
      "Wrong message at time 247!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 249!\n",
      "tensor(267)\n",
      "Wrong message at time 250!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 252!\n",
      "tensor(267)\n",
      "Wrong message at time 253!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 255!\n",
      "tensor(267)\n",
      "Wrong message at time 256!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 258!\n",
      "tensor(267)\n",
      "Wrong message at time 259!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 261!\n",
      "tensor(267)\n",
      "Wrong message at time 262!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 264!\n",
      "tensor(267)\n",
      "Wrong message at time 265!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 267!\n",
      "tensor(267)\n",
      "Wrong message at time 268!\n",
      "tensor(267)\n",
      "Wrong message at time 269!\n",
      "tensor(267)\n",
      "Wrong message at time 270!\n",
      "tensor(267)\n",
      "Wrong message at time 271!\n",
      "tensor(267)\n",
      "Wrong message at time 272!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 274!\n",
      "tensor(267)\n",
      "Wrong message at time 275!\n",
      "tensor(267)\n",
      "Wrong message at time 276!\n",
      "tensor(267)\n",
      "Wrong message at time 277!\n",
      "tensor(267)\n",
      "Wrong message at time 278!\n",
      "tensor(267)\n",
      "Wrong message at time 279!\n",
      "tensor(267)\n",
      "Wrong message at time 280!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 282!\n",
      "tensor(267)\n",
      "Wrong message at time 283!\n",
      "tensor(267)\n",
      "Wrong message at time 284!\n",
      "tensor(267)\n",
      "Wrong message at time 285!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 287!\n",
      "tensor(267)\n",
      "Wrong message at time 288!\n",
      "tensor(267)\n",
      "Wrong message at time 289!\n",
      "tensor(267)\n",
      "Wrong message at time 290!\n",
      "tensor(267)\n",
      "Wrong message at time 291!\n",
      "tensor(267)\n",
      "Wrong message at time 292!\n",
      "tensor(267)\n",
      "Wrong message at time 293!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 295!\n",
      "tensor(267)\n",
      "Wrong message at time 296!\n",
      "tensor(267)\n",
      "Wrong message at time 297!\n",
      "tensor(267)\n",
      "Wrong message at time 298!\n",
      "tensor(267)\n",
      "Wrong message at time 299!\n",
      "tensor(267)\n",
      "Wrong message at time 300!\n",
      "tensor(267)\n",
      "Wrong message at time 301!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 303!\n",
      "tensor(267)\n",
      "Wrong message at time 304!\n",
      "tensor(267)\n",
      "Wrong message at time 305!\n",
      "tensor(267)\n",
      "Wrong message at time 306!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 308!\n",
      "tensor(267)\n",
      "Wrong message at time 309!\n",
      "tensor(267)\n",
      "Wrong message at time 310!\n",
      "tensor(267)\n",
      "Wrong message at time 311!\n",
      "tensor(267)\n",
      "Wrong message at time 312!\n",
      "tensor(267)\n",
      "Wrong message at time 313!\n",
      "tensor(267)\n",
      "Wrong message at time 314!\n",
      "tensor(267)\n",
      "Wrong message at time 315!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 317!\n",
      "tensor(267)\n",
      "Wrong message at time 318!\n",
      "tensor(267)\n",
      "Wrong message at time 319!\n",
      "tensor(267)\n",
      "Wrong message at time 320!\n",
      "tensor(267)\n",
      "Wrong message at time 321!\n",
      "tensor(267)\n",
      "Wrong message at time 322!\n",
      "tensor(267)\n",
      "Wrong message at time 323!\n",
      "tensor(267)\n",
      "Wrong message at time 324!\n",
      "tensor(267)\n",
      "Wrong message at time 325!\n",
      "tensor(267)\n",
      "Wrong message at time 326!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 328!\n",
      "tensor(267)\n",
      "Wrong message at time 329!\n",
      "tensor(267)\n",
      "Wrong message at time 330!\n",
      "tensor(267)\n",
      "Wrong message at time 331!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 333!\n",
      "tensor(267)\n",
      "Wrong message at time 334!\n",
      "tensor(267)\n",
      "Wrong message at time 335!\n",
      "tensor(267)\n",
      "Wrong message at time 336!\n",
      "tensor(267)\n",
      "Wrong message at time 337!\n",
      "tensor(267)\n",
      "Wrong message at time 338!\n",
      "tensor(267)\n",
      "Wrong message at time 339!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 341!\n",
      "tensor(267)\n",
      "Wrong message at time 342!\n",
      "tensor(267)\n",
      "Wrong message at time 343!\n",
      "tensor(267)\n",
      "Wrong message at time 344!\n",
      "tensor(267)\n",
      "Wrong message at time 345!\n",
      "tensor(267)\n",
      "Wrong message at time 346!\n",
      "tensor(267)\n",
      "Wrong message at time 347!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 349!\n",
      "tensor(267)\n",
      "Wrong message at time 350!\n",
      "tensor(267)\n",
      "Wrong message at time 351!\n",
      "tensor(267)\n",
      "Wrong message at time 352!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 354!\n",
      "tensor(267)\n",
      "Wrong message at time 355!\n",
      "tensor(267)\n",
      "Wrong message at time 356!\n",
      "tensor(267)\n",
      "Wrong message at time 357!\n",
      "tensor(267)\n",
      "Wrong message at time 358!\n",
      "tensor(267)\n",
      "Wrong message at time 359!\n",
      "tensor(267)\n",
      "Wrong message at time 360!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 362!\n",
      "tensor(267)\n",
      "Wrong message at time 363!\n",
      "tensor(267)\n",
      "Wrong message at time 364!\n",
      "tensor(267)\n",
      "Wrong message at time 365!\n",
      "tensor(267)\n",
      "Wrong message at time 366!\n",
      "tensor(267)\n",
      "Wrong message at time 367!\n",
      "tensor(267)\n",
      "Wrong message at time 368!\n",
      "tensor(267)\n",
      "Wrong message at time 369!\n",
      "tensor(267)\n",
      "Wrong message at time 370!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 372!\n",
      "tensor(267)\n",
      "Wrong message at time 373!\n",
      "tensor(267)\n",
      "Wrong message at time 374!\n",
      "tensor(267)\n",
      "Wrong message at time 375!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 377!\n",
      "tensor(267)\n",
      "Wrong message at time 378!\n",
      "tensor(267)\n",
      "Wrong message at time 379!\n",
      "tensor(267)\n",
      "Wrong message at time 380!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 382!\n",
      "tensor(267)\n",
      "Wrong message at time 383!\n",
      "tensor(267)\n",
      "Wrong message at time 384!\n",
      "tensor(267)\n",
      "Wrong message at time 385!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 387!\n",
      "tensor(267)\n",
      "Wrong message at time 388!\n",
      "tensor(267)\n",
      "Wrong message at time 389!\n",
      "tensor(267)\n",
      "Wrong message at time 390!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 392!\n",
      "tensor(267)\n",
      "Wrong message at time 393!\n",
      "tensor(267)\n",
      "Wrong message at time 394!\n",
      "tensor(267)\n",
      "Wrong message at time 395!\n",
      "tensor(267)\n",
      "Wrong message at time 396!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 398!\n",
      "tensor(267)\n",
      "Wrong message at time 399!\n",
      "tensor(267)\n",
      "Wrong message at time 400!\n",
      "tensor(267)\n",
      "Wrong message at time 401!\n",
      "tensor(267)\n",
      "Wrong message at time 402!\n",
      "tensor(267)\n",
      "Wrong message at time 403!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 405!\n",
      "tensor(267)\n",
      "Wrong message at time 406!\n",
      "tensor(267)\n",
      "Wrong message at time 407!\n",
      "tensor(267)\n",
      "Wrong message at time 408!\n",
      "tensor(267)\n",
      "Wrong message at time 409!\n",
      "tensor(267)\n",
      "Wrong message at time 410!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 412!\n",
      "tensor(267)\n",
      "Wrong message at time 413!\n",
      "tensor(267)\n",
      "Wrong message at time 414!\n",
      "tensor(267)\n",
      "Wrong message at time 415!\n",
      "tensor(267)\n",
      "Wrong message at time 416!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 418!\n",
      "tensor(267)\n",
      "Wrong message at time 419!\n",
      "tensor(267)\n",
      "Wrong message at time 420!\n",
      "tensor(267)\n",
      "Wrong message at time 421!\n",
      "tensor(267)\n",
      "Wrong message at time 422!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 424!\n",
      "tensor(267)\n",
      "Wrong message at time 425!\n",
      "tensor(267)\n",
      "Wrong message at time 426!\n",
      "tensor(267)\n",
      "Wrong message at time 427!\n",
      "tensor(267)\n",
      "Wrong message at time 428!\n",
      "tensor(267)\n",
      "Wrong message at time 429!\n",
      "tensor(267)\n",
      "Wrong message at time 430!\n",
      "tensor(267)\n",
      "Wrong message at time 431!\n",
      "tensor(267)\n",
      "Wrong message at time 432!\n",
      "tensor(267)\n",
      "Wrong message at time 433!\n",
      "tensor(267)\n",
      "Wrong message at time 434!\n",
      "tensor(267)\n",
      "Wrong message at time 435!\n",
      "tensor(267)\n",
      "Wrong message at time 436!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 438!\n",
      "tensor(267)\n",
      "Wrong message at time 439!\n",
      "tensor(267)\n",
      "Wrong message at time 440!\n",
      "tensor(267)\n",
      "Wrong message at time 441!\n",
      "tensor(267)\n",
      "Wrong message at time 442!\n",
      "tensor(267)\n",
      "Wrong message at time 443!\n",
      "tensor(267)\n",
      "Wrong message at time 444!\n",
      "tensor(267)\n",
      "Wrong message at time 445!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 447!\n",
      "tensor(267)\n",
      "Wrong message at time 448!\n",
      "tensor(267)\n",
      "Wrong message at time 449!\n",
      "tensor(267)\n",
      "Wrong message at time 450!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 452!\n",
      "tensor(267)\n",
      "Wrong message at time 453!\n",
      "tensor(267)\n",
      "Wrong message at time 454!\n",
      "tensor(267)\n",
      "Wrong message at time 455!\n",
      "tensor(267)\n",
      "Wrong message at time 456!\n",
      "tensor(267)\n",
      "Wrong message at time 457!\n",
      "tensor(267)\n",
      "Wrong message at time 458!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 460!\n",
      "tensor(267)\n",
      "Wrong message at time 461!\n",
      "tensor(267)\n",
      "Wrong message at time 462!\n",
      "tensor(267)\n",
      "Wrong message at time 463!\n",
      "tensor(267)\n",
      "Wrong message at time 464!\n",
      "tensor(267)\n",
      "Wrong message at time 465!\n",
      "tensor(267)\n",
      "Wrong message at time 466!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 468!\n",
      "tensor(267)\n",
      "Wrong message at time 469!\n",
      "tensor(267)\n",
      "Wrong message at time 470!\n",
      "tensor(267)\n",
      "Wrong message at time 471!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 473!\n",
      "tensor(267)\n",
      "Wrong message at time 474!\n",
      "tensor(267)\n",
      "Wrong message at time 475!\n",
      "tensor(267)\n",
      "Wrong message at time 476!\n",
      "tensor(267)\n",
      "Wrong message at time 477!\n",
      "tensor(267)\n",
      "Wrong message at time 478!\n",
      "tensor(267)\n",
      "Wrong message at time 479!\n",
      "tensor(267)\n",
      "Wrong message at time 480!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 482!\n",
      "tensor(267)\n",
      "Wrong message at time 483!\n",
      "tensor(267)\n",
      "Wrong message at time 484!\n",
      "tensor(267)\n",
      "Wrong message at time 485!\n",
      "tensor(267)\n",
      "Wrong message at time 486!\n",
      "tensor(267)\n",
      "Wrong message at time 487!\n",
      "tensor(267)\n",
      "Wrong message at time 488!\n",
      "tensor(267)\n",
      "Wrong message at time 489!\n",
      "tensor(267)\n",
      "Wrong message at time 490!\n",
      "tensor(267)\n",
      "Wrong message at time 491!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 493!\n",
      "tensor(267)\n",
      "Wrong message at time 494!\n",
      "tensor(267)\n",
      "Wrong message at time 495!\n",
      "tensor(267)\n",
      "Wrong message at time 496!\n",
      "tensor(267)\n",
      "tensor(267)\n",
      "Wrong message at time 498!\n",
      "tensor(267)\n",
      "Wrong message at time 499!\n",
      "407\n"
     ]
    }
   ],
   "source": [
    "# Sample from model\n",
    "gen_history = history[0].unsqueeze(0)\n",
    "gen_mask = torch.zeros((1, 1), dtype=torch.bool)\n",
    "model.eval() # Turns off the dropout for evaluation. Need to do this to get repeatable evaluation outputs\n",
    "\n",
    "# Move forward in time\n",
    "wrong_cnt = 0\n",
    "for t in range(1, history.shape[0]):\n",
    "    message_logits, channel_dist = model(history[:t], mask[:t], nchan)\n",
    "    \n",
    "    #message = torch.multinomial(torch.softmax(message_logits[-1].flatten(), dim=0), 1)\n",
    "    #channel = torch.multinomial(channel_dist[-1].flatten(), 1)\n",
    "    \n",
    "    message = torch.argmax(message_logits[-1].flatten())\n",
    "    channel = torch.argmax(channel_dist[-1].flatten())\n",
    "    \n",
    "    append = torch.tensor([message, channel]).view(1, 1, 2)\n",
    "    \n",
    "    gen_history = torch.cat((gen_history, append), dim=0)\n",
    "    \n",
    "    if gen_history[-1, 0, 0] != history[t, 0, 0]:\n",
    "        print('Wrong message at time %d!' %(t))\n",
    "        wrong_cnt += 1\n",
    "        \n",
    "    '''\n",
    "    if gen_history[-1, 0, 1] != history[t, 0, 1]:\n",
    "        print('Wrong channel at time %d!' %(t))\n",
    "        wrong_cnt += 1\n",
    "    '''\n",
    "    \n",
    "    gen_mask = torch.cat((gen_mask, torch.zeros((1, 1), dtype=torch.bool)), dim=0)\n",
    "\n",
    "print(wrong_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 51],\n",
      "        [267],\n",
      "        [179],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [179],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 63],\n",
      "        [267],\n",
      "        [191],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [179],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [179],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 63],\n",
      "        [267],\n",
      "        [191],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 60],\n",
      "        [ 48],\n",
      "        [ 55],\n",
      "        [ 36],\n",
      "        [ 43],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [188],\n",
      "        [176],\n",
      "        [183],\n",
      "        [164],\n",
      "        [171],\n",
      "        [179],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [179],\n",
      "        [ 55],\n",
      "        [ 55],\n",
      "        [ 43],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [183],\n",
      "        [171],\n",
      "        [183],\n",
      "        [ 63],\n",
      "        [267],\n",
      "        [191],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 60],\n",
      "        [ 67],\n",
      "        [ 48],\n",
      "        [ 55],\n",
      "        [ 48],\n",
      "        [ 55],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [188],\n",
      "        [195],\n",
      "        [176],\n",
      "        [183],\n",
      "        [176],\n",
      "        [183],\n",
      "        [179],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [179],\n",
      "        [ 55],\n",
      "        [ 55],\n",
      "        [ 43],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [183],\n",
      "        [171],\n",
      "        [183],\n",
      "        [ 63],\n",
      "        [267],\n",
      "        [191],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 60],\n",
      "        [ 48],\n",
      "        [ 55],\n",
      "        [ 36],\n",
      "        [ 43],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [188],\n",
      "        [176],\n",
      "        [183],\n",
      "        [164],\n",
      "        [171],\n",
      "        [179],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [179],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 63],\n",
      "        [267],\n",
      "        [191],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [179],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [179],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 63],\n",
      "        [267],\n",
      "        [191],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 60],\n",
      "        [ 48],\n",
      "        [ 55],\n",
      "        [ 36],\n",
      "        [ 43],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [188],\n",
      "        [176],\n",
      "        [183],\n",
      "        [164],\n",
      "        [171],\n",
      "        [179],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [179],\n",
      "        [ 55],\n",
      "        [ 55],\n",
      "        [ 43],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [183],\n",
      "        [171],\n",
      "        [183],\n",
      "        [ 63],\n",
      "        [267],\n",
      "        [191],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 60],\n",
      "        [ 67],\n",
      "        [ 48],\n",
      "        [ 55],\n",
      "        [ 48],\n",
      "        [ 55],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [188],\n",
      "        [195],\n",
      "        [176],\n",
      "        [183],\n",
      "        [176],\n",
      "        [183],\n",
      "        [179],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [179],\n",
      "        [ 55],\n",
      "        [ 55],\n",
      "        [ 43],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [183],\n",
      "        [171],\n",
      "        [183],\n",
      "        [ 63],\n",
      "        [267],\n",
      "        [191],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 60],\n",
      "        [ 48],\n",
      "        [ 55],\n",
      "        [ 36],\n",
      "        [ 43],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [188],\n",
      "        [176],\n",
      "        [183],\n",
      "        [164],\n",
      "        [171],\n",
      "        [179],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [179],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 63],\n",
      "        [267],\n",
      "        [191],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [179],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [179],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 63],\n",
      "        [267],\n",
      "        [191],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 60],\n",
      "        [ 55],\n",
      "        [ 36],\n",
      "        [ 43],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [188],\n",
      "        [183],\n",
      "        [164],\n",
      "        [171],\n",
      "        [179],\n",
      "        [ 72],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [200],\n",
      "        [183],\n",
      "        [ 70],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [198],\n",
      "        [179],\n",
      "        [ 55],\n",
      "        [ 55],\n",
      "        [ 43],\n",
      "        [ 68],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [183],\n",
      "        [171],\n",
      "        [196],\n",
      "        [183],\n",
      "        [ 67],\n",
      "        [ 63],\n",
      "        [267],\n",
      "        [195],\n",
      "        [191],\n",
      "        [ 65],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 60],\n",
      "        [ 67],\n",
      "        [ 48],\n",
      "        [ 55],\n",
      "        [ 48],\n",
      "        [ 55],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [188],\n",
      "        [195],\n",
      "        [176],\n",
      "        [183],\n",
      "        [176],\n",
      "        [183],\n",
      "        [193],\n",
      "        [179],\n",
      "        [ 62],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [190],\n",
      "        [183],\n",
      "        [ 63],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [191],\n",
      "        [179],\n",
      "        [ 55],\n",
      "        [ 55],\n",
      "        [ 43],\n",
      "        [ 65],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [183],\n",
      "        [171],\n",
      "        [193],\n",
      "        [183],\n",
      "        [ 67],\n",
      "        [ 63],\n",
      "        [267],\n",
      "        [195],\n",
      "        [191],\n",
      "        [ 68],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 60],\n",
      "        [ 48],\n",
      "        [ 55],\n",
      "        [ 36],\n",
      "        [ 43],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [188],\n",
      "        [176],\n",
      "        [183],\n",
      "        [164],\n",
      "        [171],\n",
      "        [196],\n",
      "        [179],\n",
      "        [ 70],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [198],\n",
      "        [183],\n",
      "        [ 72],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [200],\n",
      "        [179],\n",
      "        [ 74],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [202],\n",
      "        [183],\n",
      "        [ 75],\n",
      "        [ 63],\n",
      "        [267],\n",
      "        [203],\n",
      "        [191],\n",
      "        [ 77],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [205],\n",
      "        [183],\n",
      "        [ 67],\n",
      "        [ 79],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [195],\n",
      "        [207],\n",
      "        [179],\n",
      "        [ 68],\n",
      "        [ 80],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [196],\n",
      "        [208],\n",
      "        [183],\n",
      "        [ 67],\n",
      "        [ 79],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [179],\n",
      "        [ 55],\n",
      "        [ 55],\n",
      "        [ 43],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [183],\n",
      "        [171],\n",
      "        [183],\n",
      "        [ 63],\n",
      "        [267],\n",
      "        [191],\n",
      "        [ 55],\n",
      "        [263],\n",
      "        [195],\n",
      "        [207],\n",
      "        [259],\n",
      "        [183],\n",
      "        [ 60],\n",
      "        [ 48],\n",
      "        [ 55],\n",
      "        [ 36],\n",
      "        [ 43],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [188],\n",
      "        [176],\n",
      "        [183],\n",
      "        [164],\n",
      "        [171],\n",
      "        [179],\n",
      "        [ 72],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [200],\n",
      "        [183],\n",
      "        [ 70],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [198],\n",
      "        [179],\n",
      "        [ 55],\n",
      "        [ 55],\n",
      "        [ 43],\n",
      "        [ 68],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [183],\n",
      "        [171],\n",
      "        [196],\n",
      "        [183],\n",
      "        [ 67],\n",
      "        [ 63],\n",
      "        [267],\n",
      "        [195],\n",
      "        [191],\n",
      "        [ 65],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [183],\n",
      "        [ 60],\n",
      "        [ 67],\n",
      "        [ 48],\n",
      "        [ 55],\n",
      "        [ 48],\n",
      "        [ 55],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [188],\n",
      "        [195],\n",
      "        [176],\n",
      "        [183],\n",
      "        [176],\n",
      "        [183],\n",
      "        [193],\n",
      "        [179],\n",
      "        [ 62],\n",
      "        [ 55],\n",
      "        [267],\n",
      "        [190],\n",
      "        [183],\n",
      "        [ 63],\n",
      "        [ 51],\n",
      "        [267],\n",
      "        [191],\n",
      "        [179]])\n"
     ]
    }
   ],
   "source": [
    "print(history[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test_history.npy', gen_history.squeeze(1).detach().numpy())\n",
    "np.save('test_instruments.npy', [instrument_numbers[i] for i in instruments[:-1, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class MIDIDataset(torch.utils.data.Dataset):\n",
    "    # CONSTRUCTOR: creates a tensor of message chunks and associated instruments.\n",
    "    # Assumes that the directory contains recording0.npy to recordingM.npy\n",
    "    # as well as instruments0.npy to instrumentsM.npy\n",
    "    # ARGUMENTS\n",
    "    # root_dir: the directory to search\n",
    "    # chunk_size: we'll chunk the data into chunks of this size (or less)\n",
    "    # max_channels: what's the largest number of instruments in any file?\n",
    "    def __init__(self, root_dir, chunk_size, max_channels, transform=None):\n",
    "        files = os.listdir(root_dir)\n",
    "        recording_files = []\n",
    "        instrument_files = []\n",
    "        for file in files:\n",
    "            if 'recording' in file:\n",
    "                recording_files.append(os.path.join(root_dir, file))\n",
    "            elif 'instruments' in file:\n",
    "                instrument_files.append(os.path.join(root_dir, file))\n",
    "                \n",
    "        assert(len(recording_files) == len(instrument_files))\n",
    "        recording_files.sort()\n",
    "        instrument_files.sort()\n",
    "        \n",
    "        self.chunks = []\n",
    "        self.masks = []\n",
    "        self.instruments = []\n",
    "        self.inst_masks = []\n",
    "        \n",
    "        ch = 0\n",
    "        for f in range(len(recording_files)):\n",
    "            recording = np.load(recording_files[f], allow_pickle=True)\n",
    "            inst = [instrument_numbers.index(i) for i in np.load(instrument_files[f], allow_pickle=True)]\n",
    "            \n",
    "            nchunks = int(np.ceil(recording.shape[0]/chunk_size))\n",
    "            self.chunks += [torch.zeros((chunk_size, 2), dtype=torch.long) for c in range(nchunks)]\n",
    "            self.masks += [torch.ones(chunk_size, dtype=torch.bool) for c in range(nchunks)]\n",
    "            self.instruments += [torch.zeros(max_channels, dtype=torch.long) for c in range(nchunks)]\n",
    "            self.inst_masks += [torch.ones(max_channels, dtype=torch.long) for c in range(nchunks)]\n",
    "            for chunk_start in range(0, recording.shape[0], chunk_size):\n",
    "                chunk_end = min(chunk_start + chunk_size, recording.shape[0])\n",
    "                size = chunk_end - chunk_start\n",
    "                self.chunks[ch][:size] = torch.tensor(recording[chunk_start:chunk_end], dtype=torch.long)\n",
    "                self.masks[ch][:size] = False\n",
    "                self.instruments[ch][:len(inst)] = torch.tensor(inst, dtype=torch.long)\n",
    "                self.inst_masks[ch][:len(inst)] = False\n",
    "                ch += 1\n",
    "            \n",
    "        self.transform = transform\n",
    "\n",
    "    # __len__\n",
    "    # RETURN: the number of recording chunks in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "\n",
    "    # __getitem__\n",
    "    # ARGUMENTS\n",
    "    # idx: indicates which chunk(s) to get\n",
    "    # RETURN: instance, a dictionary with keys 'history' and 'instruments'\n",
    "    # instance['history'] is an Lx2 tensor containing messages and associated channels\n",
    "    # instance['instruments'] a length N tensor of instrument numbers\n",
    "    # instance['mask'] a length L tensor containing False where messages exist and True otherwise\n",
    "    # instance['inst_mask'] a length N tensor containing False where instruments exist and True otherwise\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        instance = {'history': self.chunks[idx], \\\n",
    "                    'instruments': self.instruments[idx],\n",
    "                    'mask': self.masks[idx],\n",
    "                    'inst_mask': self.inst_masks[idx]}\n",
    "        \n",
    "        if self.transform:\n",
    "            instance = self.transform(instance)\n",
    "            \n",
    "        return instance\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    chunk_size = batch[0]['history'].shape[0]\n",
    "    max_channels = batch[0]['instruments'].shape[0]\n",
    "    sample = {'history': torch.zeros((chunk_size, len(batch), 2), dtype=torch.long), \\\n",
    "              'instruments': torch.ones((max_channels, len(batch)), dtype=torch.long), \\\n",
    "              'mask': torch.ones((chunk_size, len(batch)), dtype=torch.bool),\n",
    "              'inst_mask': torch.ones((max_channels, len(batch)), dtype=torch.bool)}\n",
    "    \n",
    "    for b, instance in enumerate(batch):\n",
    "        sample['history'][:, b] = instance['history']\n",
    "        sample['instruments'][:, b] = instance['instruments']\n",
    "        sample['mask'][:, b] = instance['mask']\n",
    "        sample['inst_mask'][:, b] = instance['inst_mask']\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_loss: computes the loss for the model over the batch\n",
    "# ARGUMENTS\n",
    "# model: EnsembleTransformer model\n",
    "# message_loss_fn: torch.nn.CrossEntropyLoss object\n",
    "# channel_loss_fn: torch.nn.NLLLoss object\n",
    "# batch: see collate_fn definition\n",
    "# RETURN: a scalar loss tensor\n",
    "def compute_loss(model, message_loss_fn, channel_loss_fn, batch):  \n",
    "    max_seq_length = batch['history'].shape[0]\n",
    "\n",
    "    message_logits, channel_dist = model(batch['history'][:-1], batch['mask'][:-1], batch['instruments'], batch['inst_mask'])\n",
    "    log_channel_dist = torch.log(channel_dist + 1e-10)\n",
    "\n",
    "    target_mask = torch.logical_not(batch['mask'][1:])\n",
    "\n",
    "    message_loss = message_loss_fn(message_logits[target_mask], batch['history'][1:, :, 0][target_mask])\n",
    "    channel_loss = channel_loss_fn(log_channel_dist[target_mask], batch['history'][1:, :, 1][target_mask])\n",
    "\n",
    "    return message_loss + channel_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 512\n",
    "heads = 4\n",
    "attention_layers = 6\n",
    "ff_size = 512\n",
    "\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleTransformer(message_dim, embed_dim, num_instruments, heads, attention_layers, ff_size)\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "    \n",
    "model.eval() # Training with eval just to see if we can overfit without dropout\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "chunk_size = 500\n",
    "\n",
    "train_dataset = MIDIDataset('train_unified', chunk_size, max_channels)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "test_dataset = MIDIDataset('test_unified', chunk_size, max_channels)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "message_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "channel_loss_fn = torch.nn.NLLLoss(ignore_index=-1)\n",
    "epochs = 20\n",
    "train_losses = np.zeros(epochs)\n",
    "test_losses = np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    model.train()\n",
    "    for b, batch in enumerate(train_dataloader):\n",
    "        print('Starting iteration %d' %(b))\n",
    "        loss = compute_loss(model, message_loss_fn, channel_loss_fn, batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    torch.save(model.state_dict(), 'unified_transformer_models/epoch' + str(epoch) + '.pth')\n",
    "\n",
    "    print('Computing test loss')\n",
    "    model.eval()\n",
    "    for batch in test_dataloader:\n",
    "        loss = compute_loss(model, message_loss_fn, channel_loss_fn, batch)\n",
    "        test_losses[epoch] += loss.data\n",
    "        \n",
    "    print('Computing train loss')\n",
    "    for batch in train_dataloader:\n",
    "        loss = compute_loss(model, message_loss_fn, channel_loss_fn, batch)\n",
    "        train_losses[epoch] += loss.data\n",
    "    \n",
    "    train_losses[epoch] /= len(train_dataloader)\n",
    "    test_losses[epoch] /= len(test_dataloader)\n",
    "    print('Train Loss: %f, Test Loss: %f' %(train_losses[epoch], test_losses[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # Disable dropout to make results repeatable\n",
    "\n",
    "time_steps = 500 # How many time steps do we sample?\n",
    "\n",
    "# Start with a time shift\n",
    "gen_history = torch.zeros((1, 1, 2), dtype=torch.long)\n",
    "gen_history[0, 0, 0] = 387\n",
    "gen_history[0, 0, 1] = -1\n",
    "\n",
    "# Violin\n",
    "instruments = torch.zeros((1, 1), dtype=torch.long)\n",
    "instruments[0, 0] = 0\n",
    "inst_mask = torch.zeros((1, 1), dtype=torch.bool)\n",
    "\n",
    "gen_mask = torch.zeros((1, 1), dtype=torch.bool)\n",
    "\n",
    "# Move forward in time\n",
    "for t in range(0, time_steps):\n",
    "    message_logits, channel_logits = model(gen_history, gen_mask, instruments)\n",
    "    \n",
    "    message = torch.multinomial(torch.softmax(message_logits[-1].flatten(), dim=0), 1)\n",
    "    channel = torch.multinomial(channel_logits[-1].flatten(), 1)\n",
    "    \n",
    "    append = torch.tensor([message, channel]).view(1, 1, 2)\n",
    "    \n",
    "    gen_history = torch.cat((gen_history, append), dim=0)\n",
    "    \n",
    "    gen_mask = torch.cat((gen_mask, torch.zeros((1, 1), dtype=torch.bool)), dim=0)\n",
    "\n",
    "print(wrong_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('gen_history.npy', gen_history.squeeze(1).detach().numpy())\n",
    "np.save('gen_instruments.npy', [instrument_numbers[i] for i in instruments[:, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
