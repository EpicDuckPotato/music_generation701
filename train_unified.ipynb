{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_notes = 128\n",
    "num_time_shifts = 100\n",
    "num_velocities = 32\n",
    "message_dim = 2*num_notes + num_velocities + num_time_shifts\n",
    "instrument_numbers = [0, 6, 40, 41, 42, 43, 45, 60, 68, 70, 71, 73]\n",
    "num_instruments = len(instrument_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EnsembleTransformer definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://pytorch.org/tutorials/beginner/transformer_tutorial.html.\n",
    "# Only change is the view/expand in forward (accounts for batches)\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.shape[0], :].unsqueeze(1).expand(-1, x.shape[1], -1)\n",
    "        return self.dropout(x)\n",
    "        \n",
    "\n",
    "# EnsembleTransformer: takes a history of MIDI messages \n",
    "# for instruments in an ensemble and generates a distribution for the next message,\n",
    "# as well as the instrument who should issue the message\n",
    "class EnsembleTransformer(torch.nn.Module):\n",
    "    # CONSTRUCTOR\n",
    "    # ARGUMENTS\n",
    "    # message_dim: dimension of a MIDI message\n",
    "    # embed_dim: dimension of message embedding\n",
    "    # num_instruments: number of instrument labels\n",
    "    # heads: number of attention heads\n",
    "    # attention_layers: number of attention layers\n",
    "    # ff_size: size of the feedforward output at the end of the decoder\n",
    "    def __init__(self, message_dim, embed_dim, num_instruments, heads, attention_layers, ff_size):\n",
    "        super(EnsembleTransformer, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Indicates which channel is associated with each instrument, as well as\n",
    "        # the position of messages in time\n",
    "        self.position_encoding = PositionalEncoding(embed_dim)\n",
    "        \n",
    "        # The encoder computes attention over the instrument embeddings\n",
    "        self.i_embedding = torch.nn.Embedding(num_instruments, embed_dim)\n",
    "        encoder_layer = torch.nn.TransformerEncoderLayer(embed_dim, heads, ff_size)\n",
    "        self.encoder = torch.nn.TransformerEncoder(encoder_layer, attention_layers)\n",
    "        \n",
    "        # The decoder computes attention over the message history, using the above\n",
    "        # encoding as memory\n",
    "        self.embedding = torch.nn.Embedding(message_dim, embed_dim)\n",
    "        decoder_layer = torch.nn.TransformerDecoderLayer(embed_dim, heads, ff_size)\n",
    "        self.decoder = torch.nn.TransformerDecoder(decoder_layer, attention_layers)\n",
    "\n",
    "        # The decoding is passed through a linear layer to get the logits for the next message        \n",
    "        self.message_logits = torch.nn.Linear(embed_dim, message_dim)\n",
    "        \n",
    "        # The decoding becomes a query for attention across the instruments, which is used to\n",
    "        # predict the next instrument\n",
    "        self.inst_attention = torch.nn.MultiheadAttention(embed_dim, heads)\n",
    "    \n",
    "    # forward: generates a probability distribution for the next MIDI message\n",
    "    # and the channel that issues the message, given a message history for the instrument ensemble\n",
    "    # ARGUMENTS\n",
    "    # history: an LxBx2 tensor, where L is the length of the longest message history in\n",
    "    # the batch, and B is the batch size. The first index along dimension 2 stores the\n",
    "    # message number. The second stores the channel number. This should be END-PADDED\n",
    "    # along dimension 0. All time shifts should be associated with channel -1.\n",
    "    # mask: an LxB tensor, containing True in any locations where history contains\n",
    "    # padding\n",
    "    # instruments: a NxB tensor indicating the instrument number for each channel, where\n",
    "    # N is the maximum number of channels in the batch. This should be END-PADDED along dimension 0\n",
    "    # inst_mask: contains False where an instrument exists, True where it doesn't\n",
    "    # RETURN: two tensors. The first is LxBxD, representing the distribution for the next message at each time\n",
    "    # step (need to take the softmax to get actual probabilities). The second is LxBxC, representing the\n",
    "    # distribution for the next channel at each time step (need to take the softmax to get actual probabilities)\n",
    "    def forward(self, history, mask, instruments, inst_mask):\n",
    "        L = history.shape[0] # longest length\n",
    "        B = history.shape[1] # batch size\n",
    "        N = instruments.shape[0]\n",
    "        assert(mask.shape == (L, B))\n",
    "        assert(inst_mask.shape == instruments.shape)\n",
    "        \n",
    "        # NxBxD\n",
    "        inst_embed = self.position_encoding(torch.tanh(self.i_embedding(instruments)))\n",
    "        \n",
    "        inst_encoding = self.encoder(inst_embed, src_key_padding_mask=inst_mask.transpose(0, 1))\n",
    "        \n",
    "        # Which messages are time shifts?\n",
    "        time_shift_mask = history[:, :, 1] < 0\n",
    "        \n",
    "        # LxBxD, instrument embedding associated with each message\n",
    "        inst_sel = history[:, :, 1].unsqueeze(2).expand(-1, -1, self.embed_dim).clone()\n",
    "        inst_sel[time_shift_mask] = 0\n",
    "        \n",
    "        inst_tags = torch.gather(inst_embed, 0, inst_sel)\n",
    "        inst_tags[time_shift_mask] = 0\n",
    "        \n",
    "        # LxBxD\n",
    "        decoder_inputs = self.position_encoding(self.embedding(history[:, :, 0])) + inst_tags\n",
    "        \n",
    "        tgt_mask = torch.triu(torch.ones((L, L), dtype=torch.bool))\n",
    "        tgt_mask.fill_diagonal_(False)\n",
    "        tgt_key_padding_mask = mask.transpose(0, 1)\n",
    "        \n",
    "        decoding = self.decoder(decoder_inputs, inst_embed, \\\n",
    "                                tgt_mask=tgt_mask, \\\n",
    "                                tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                memory_key_padding_mask=inst_mask.transpose(0, 1))\n",
    "        \n",
    "        # LxBxD\n",
    "        message_dist = self.message_logits(decoding)\n",
    "        \n",
    "        # channel_dist (BxLxN) contains the attention weights for each instrument.\n",
    "        # We have L queries (the elements of decoding). Our keys and values\n",
    "        # are the instrument embeddings\n",
    "        att_out, channel_dist = self.inst_attention(self.position_encoding(decoding), \\\n",
    "                                                    inst_embed, inst_embed,\n",
    "                                                    key_padding_mask = inst_mask.transpose(0, 1))\n",
    "        \n",
    "        return message_dist, channel_dist.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests for EnsembleTransformer\n",
    "We train with model.eval() to disable dropout, since these tests try to get the model to overfit to a small sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model to overfit to a single song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 512\n",
    "heads = 4\n",
    "attention_layers = 6\n",
    "ff_size = 512\n",
    "\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleTransformer(message_dim, embed_dim, num_instruments, heads, attention_layers, ff_size)\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "    \n",
    "model.eval() # Training with eval just to see if we can overfit without dropout\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('unified_transformer.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recording = np.load('train_unified/recording0.npy', allow_pickle=True)\n",
    "instruments_np = np.load('train_unified/instruments0.npy', allow_pickle=True)\n",
    "\n",
    "nsamples = 500\n",
    "\n",
    "history = torch.tensor(recording[:nsamples], dtype=torch.long).view(-1, 1, 2)\n",
    "mask = torch.zeros((history.shape[0], history.shape[1]), dtype=torch.bool)\n",
    "instruments = torch.tensor([instrument_numbers.index(i) for i in instruments_np], dtype=torch.long).view(-1, 1)\n",
    "inst_mask = torch.zeros(instruments.shape, dtype=torch.bool)\n",
    "\n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "message_loss = torch.nn.CrossEntropyLoss()\n",
    "channel_loss = torch.nn.NLLLoss(ignore_index=-1)\n",
    "epochs = 500\n",
    "train_losses = np.zeros(epochs)\n",
    "\n",
    "target_messages = history[1:, :, 0].flatten()\n",
    "target_channels = history[1:, :, 1].flatten()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    \n",
    "    message_logits, channel_dist = model(history[:-1], mask[:-1], instruments, inst_mask)\n",
    "    channel_log_dist = torch.log(channel_dist + 1e-8)\n",
    "    \n",
    "    loss = message_loss(message_logits.view(-1, message_dim), target_messages) + \\\n",
    "           channel_loss(channel_log_dist.view(-1, num_channels), target_channels)\n",
    "                \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses[epoch] = loss.data\n",
    "    print('Loss: %f' %(loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'unified_transformer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbae2cab190>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAXGklEQVR4nO3deZAc5X3G8e+ve669pNVKuyCtbi4Zy4BgbYTBGDAJZxmnyk5wDOW4XFZcxDHGiR0cX+WqxHbixFfio1S2E2yDXRSGmFAEG7CAxBjBCgQIJCEJJHSyi66V9pqdmTd/zKy0F9Ig7Wjenn4+VVsz09M7+r1Tq2feefvtfs05h4iI+CuodgEiInJkCmoREc8pqEVEPKegFhHxnIJaRMRziUq86IwZM9z8+fMr8dIiIjVp1apVrzvnWid6riJBPX/+fDo7Oyvx0iIiNcnMtrzRcxr6EBHxnIJaRMRzCmoREc8pqEVEPKegFhHxnIJaRMRzCmoREc95FdTffXgDj77UXe0yRES84lVQ/+CRTfx+4+vVLkNExCteBXVgkC9oIQMRkZHKCmozu8XMXjCzNWb2CzPLVKSYwBTUIiJjHDWozawd+CTQ4ZxbDITA9RUpxgwtDSYiMlq5Qx8JoM7MEkA9sKMSxYSBkVdQi4iMctSgds5tB/4FeBXYCex3zv22IsWYoZEPEZHRyhn6mAZcBywAZgENZnbDBPstM7NOM+vs7j62KXaBQUFJLSIySjlDH5cDrzjnup1zQ8DdwDvH7uScW+6c63DOdbS2Tnjt66MKA6OgoQ8RkVHKCepXgaVmVm9mBrwHWFuRYszIFyrxyiIi0VXOGPVK4C7gaeD50u8sr0gxAZr1ISIyRllLcTnnvgx8ucK1FHvUCmoRkVG8OjMxNJ3wIiIylldBHQSGOtQiIqP5FdS61oeIyDieBbWm54mIjKWgFhHxnFdBXTzhpdpViIj4xaug1hi1iMh4fgW1TiEXERnHr6DWGLWIyDheBbVOeBERGc+roA4CdDBRRGQMv4LaTNejFhEZw6ug1vWoRUTG8yqozYy8clpEZBSvgjo0XY9aRGQsr4I60KwPEZFx/ArqQEEtIjKWX0Ft6HrUIiJjeBXUYaCluERExvIqqHUKuYjIeP4FtcaoRURG8SqodT1qEZHxvApq0/WoRUTG8SqoQ41Ri4iM41VQ62CiiMh4fgV1YOQL1a5CRMQvXgV1GOhaHyIiY3kV1IHphBcRkbG8C2rNoxYRGc2/oFZOi4iM4lVQhwGa9SEiMoZXQa3rUYuIjOdXUGvNRBGRcfwKakNj1CIiY3gV1KGGPkRExvEqqIPAAJ30IiIykl9BbcWgVq9aROQwr4I6LPWoldMiIod5FdSlDrVmfoiIjFBWUJtZs5ndZWbrzGytmV1QiWJCDX2IiIyTKHO/7wAPOOfeb2YpoL4SxQyPUatHLSJy2FGD2symABcDfwHgnMsC2UoUMzzro6BrUouIHFLO0MdCoBv4DzN7xsx+ZGYNlSgm1Bi1iMg45QR1AjgX+IFzbgnQC9w6diczW2ZmnWbW2d3dfWzFlHrUuia1iMhh5QT1NmCbc25l6fFdFIN7FOfccudch3Ouo7W19diK0Ri1iMg4Rw1q59wuYKuZnVHa9B7gxYoUYxqjFhEZq9xZH38N3F6a8fEy8JFKFBOWPjbUoxYROaysoHbOrQY6KlsKmOZRi4iM49WZiaHGqEVExvEqqINDQx/VrUNExCdeBXVTOgnA3r6KnE8jIhJJXgX1qW2NAGx87WCVKxER8YdXQT2npZ50IuCl1w5UuxQREW94FdRhYJza1siGLvWoRUSGeRXUANMb0+zrH6p2GSIi3vAuqDOJgMGhfLXLEBHxhn9BnQwZzOkcchGRYd4FdToRMKAetYjIId4FdSYZKqhFREbwMKgDBoY09CEiMszDoA4ZzOVxut6HiAjgYVCnEwEFB0N5BbWICHgY1JlkCMBATuPUIiLgYVCnh4NaBxRFRAAPgzqTKJY0qAOKIiKAh0E93KMe1NCHiAjgYVAP96g1RU9EpMi/oFaPWkRkFO+COq0etYjIKN4FdUazPkRERvE4qNWjFhEBL4O6WFJvNlflSkRE/OBdUM9qrqM+FfL8tv3VLkVExAveBXUyDHj7/BYe3/R6tUsREfGCd0ENcN68aWzq7tUBRRERPA3qOS11AGzf11/lSkREqs/LoG5vrgdg+14FtYiIn0E9rdij3qagFhHxM6hPakqTCIxte/uqXYqISNV5GdSJMGBhawPPb9cUPRERL4Ma4OLTWln5yh729GarXYqISFV5G9TXnDWTXL7Au7+xglVb9lS7HBGRqvE2qJfMncb9N7+LumTIv/1uo+ZUi0hseRvUAItOnsKfnz+XR9Z3s+iLD3DvszuqXZKIyAnndVADfOxdC5k3vTiv+pO/eIYP/PBxNnYdrHJVIiInjjnnJv1FOzo6XGdn56S93mAuz8BQgeWPbeKOla9ycDBHW1OGK956Mp+7ehHJ0PvPGxGRIzKzVc65jgmfi0JQj7Rtbx/ffmgDW3b38tTmvcyammHJvGl0zJvG9W+fS10qrMi/KyJSSTUV1CM9sGYn//3sTlZv3cf2ff3MaEyzdGEL17xtJpcuaju0CIGIiO+OFNSJN/EiIdAJbHfOXTtZxR2PKxfP5MrFMwFY+fJufvqHLTzx8h7ue24nTZkEVy0+mevOaWfpwumEgVW5WhGRY1N2UAM3A2uBKRWq5bicv3A65y+cTi5f4PFNu/n16h3c//wu7uzcRltTmg90zOaGpfOYObXuiK/zyuu9zGrOkE68cW98654+7n12Bx98x1xaGlKT3RQRkVHKGvows9nAbcA/Ap8+Wo/6RA19HM3AUJ6H13ZxzzPbeHhdF4EZl57RymWLTuL9580mlRh9EHL3wUHO+4eHOHtOM9//0Lm0N48Odecc//n4Zr7xm/X0ZfOcPXsqv/7ERSeySSJSo457jNrM7gK+BjQBfztRUJvZMmAZwNy5c8/bsmXLcRU92bbu6ePnT2zhvud2sn1fPydPyfDec2Zx49J5zGmp58UdPVz93f8d9Tt/2jGbv7tyES0NKcyMl7sPctm/PgrAwtYGXu7u5dIzWrn49FauOWsm0xvSGmIRkWNyXEFtZtcCVzvnbjKzS3iDoB7Jlx71RJxzPPpSNz9/YguPrO8mDIy5LfVsOMLc7PbmOi5b1MZFp83gL3+2CoD/+qsLed/3fj9qv2vOmknn5j185/olLF04vaLtEJHacrxB/TXgRiAHZCiOUd/tnLvhjX7H56Aeace+fv59xUYee6mbrp5Bvv+hc7l0URthYAwM5bnt8c3s6cvysz9soS97+BT271x/Dted087f3/M8d6x8dcLXPnlKhuvOmcWpbY0sbp/KW2Z6ObQvIp6YtOl5tdCjnohzjp7+HFPrk2+4z/LHNvHV+9cBsPnr1wBQKDh29gywYl0Xly5q45ZfrubJzXuoT4UsnjWVJzcXLyZlBs11SVoaUlxwynTamjKcPDXD4llTacokmNNSX/lGiojXJmV6Xi0zsyOGNMCyi09hxbpu6kecUBMERntzHTcsnQfAnR+/AOcczhWfW7erh01dvTy3bR992Tyrt+7j3tU76BnIjXrt5vok7c11xZ9pdcyaWses5jrmtBS3TatPEWjsWyS2In3Cy4k2/F6ZHV9o7to/wN6+LM9v309P/xCbd/eyfW8/2/f1s31vP73Z0VcKTARGa1OatqY0rU0Z2qakaUon6MvmGczlOXtOM+86tZVsPs9JUzI0ZY78oSPRcedTW8kVHE+8vJtb/uh0FsxoqHZJUiE1e2ZiLXLO0TOQY/vefrbu7WPHvn66DgzS1TNI14EBug8M0nVgkIODORpSIYEZu0csrmAG86c30JRJkAwDZjSmmD+9gSl1SRrTCZoyCabVp2hpSJFJhiRDIxkGJEIjkwhprk8e9wfR8RjKF3itZ4DZ0zQc1J/N85YvPTBq28fffQq3XrWoShVJJWnoI0LMjKl1SabWJTlz1tEPQDrn2NB1kKc276EhlWDL7j7Wv9ZDfzbPUL743Ip13WTzhbL+/RmNKVqbMiQCI19wmEFLQ4q2pgytTWmaMsU/mULBUXBQcI4ZTWneOmsK0+pTh751TC2Nyb+Z0N/8ei8f+2knG7oO8pkrzuCjFy0gERiJGFx0a8vuXk6akhl12YO1u3rG7ffDRzdx16ptfOTC+Tzz6l5uuvRUNnUdZPa0ei44RTONapV61DExmMtzcCDHgYEce/uy7OnNMjBUIFcokM0VyBUcvYM51u86wN6+LAUHgUHBFU8E6j4wSPfBQYby5f+9pBIBDamQVCIo/oQBqURIQyqkMZOgIZXArBj23QcGeXbrfurTIU2ZBFv3FFegn1qX5G3tU5neWPwW0N5cR2DGnJZ6nHO0TckQGDSmE5gZdcmQdOnfSyeCSIT8jn39vPPrv6OtKc0jn7mE+lRi1MHrsRpS4bjhsaZMglsuP522KWlOP6mJF3f08L4l7SeifJkkGvqQSVEoOLL5AoEZgUFghhns2D/AC9v305vNYRR70Hv7suzaP0BfNk82VyCbL34gDOby9A7m6c3mODhYPKgamDGtPsnZs5v5yEULSIUBN/54Jc7Brp4BCs5xYCCHGbzZP9cwMFJhQDoZHLpNJ8JD95NhcXsyLPbcU6VhoGQYlH4ODw2lStuG7w/39hOBEQZGIjTCYMTjQ7fBiOfHbofbHt/Cz54oniD2lplTaG/O8NDarkNtWPOVK7hj5Ra+ev86Vn3hcjLJkJtuf5pZzRlaG9Mkw4BvPfQShTHvzWltjdSnE7TUJ2nMJGlIhXz0ogWcdlLTsf8RSMUoqCVyCqVhl2y+QCoM6MvmySRDtu7pI5UI2Lm/HzD29WXJFRz92TwF5xgYKpDN5cnmCwwOFRgsfUgMDuWL93PFbYO5w4+H8sVvFMPfLIbyBYZyBYZK93P54gfUUL7wpj8oynXlW09m9rQ6/mfNLl4/OMg7FrTwrT87hwMDubIOIG7sOsDAUIEV67p4dU8f+/uH2Lq3n1QioD+bY9f+AXoGcnz2yjO46ZJTK9MIOS4ao5bIGZ6OOHxxrIZ08U91fim0ZjUf+eJalZIfDvLSN4S8c+QLjly+dFsYvi2MfpyfePtQvkBrY5qlC6cTBMYXrj0T59yhsf0Zjemy6jq1rdhLXtw+dcLnB4byLPriAxX7oJHKUlCLvAlhYIRBWNFrnVdi1s3wNWgq8Q1aKs//Iy0ictyCUviPHceWaFBQi8TAcB+9oB51JCmoRWJgeDRFPepoUlCLxICVplLqaGI0KahFYiIwU486ohTUIjERmMaoo0pBLRITph51ZCmoRWKiOEStpI4iBbVITARmKKajSUEtEhOBFa+hItGjoBaJCc36iC4FtUhMmGZ9RJaCWiQmzEwHEyNKQS0SE8Mr9kj0KKhFYqI460NJHUUKapGY0Akv0aWgFomJwHTCS1QpqEViIjCjUKh2FXIsFNQiMaHpedGloBaJCZ1CHl0KapGYUI86uhTUIjERmGmBl4hSUIvEhBYOiC4FtUhMaB51dCmoRWJCY9TRpaAWiYnADE37iCYFtUhMaIw6uhTUIjFRXDhAQR1FCmqRmNDBxOhSUIvEhFYhjy4FtUhMBIEWDogqBbVITARaiiuyjhrUZjbHzFaY2Voze8HMbj4RhYnI5NIYdXQlytgnB/yNc+5pM2sCVpnZg865Fytcm4hMIk3Pi66j9qidczudc0+X7h8A1gLtlS5MRCZX8WBitauQY/GmxqjNbD6wBFg5wXPLzKzTzDq7u7snqTwRmSyaRx1dZQe1mTUCvwI+5ZzrGfu8c265c67DOdfR2to6mTWKyCTQZU6jq6ygNrMkxZC+3Tl3d2VLEpFK0EWZoqucWR8G/BhY65z7ZuVLEpFKUI86usrpUV8I3AhcZmarSz9XV7guEZlkxRNelNRRdNTpec65/6N4wFhEIszQwcSo0pmJIjFRHKOudhVyLBTUIjERmGndgIhSUIvERGC6el5UKahFYkInvESXglokJsygUKh2FXIsFNQiMWHqUUeWglokJgJNso0sBbVITGiMOroU1CIxEWjhgMhSUIvEhC7KFF0KapGYMF2UKbIU1CIxoaW4oktBLRITusxpdCmoRWJCY9TRpaAWiQn1qKNLQS0SE4Z61FGloBaJCZ3wEl0KapGYCAI09BFRCmqRmDCdmRhZCmqRmNDCAdGloBaJCS1uG10KapGYCLS4bWQpqEViQgsHRJeCWiQmAjO0DHk0KahFYkIXZYouBbVITASBpudFlYJaJCZ0Cnl0KahFYkILB0SXglokJjRGHV0KapGYCMw06SOiFNQiMaEedXQpqEXiojRGret9RI+CWiQmAiveKqejR0EtEhOBFZNawx/Ro6AWiYlDPerqliHHQEEtEhOmHnVkKahFYmJ46EM5HT0KapGYKOW0etQRpKAWiYngUFBXtw5588oKajO70szWm9lGM7u10kWJyOTTrI/oOmpQm1kIfA+4CjgT+KCZnVnpwkRkcpnGqCMrUcY+7wA2OudeBjCzXwLXAS9WsjARmVzDQx9/8v3fEw4PWMukmlaf4s6PXzDpr1tOULcDW0c83gacP3YnM1sGLAOYO3fupBQnIpPnkjPaeObVfeQKhWqXUrOmZJIVed1ygnqij95xX56cc8uB5QAdHR36ciXimQUzGvjuB5dUuww5BuUcTNwGzBnxeDawozLliIjIWOUE9VPAaWa2wMxSwPXAvZUtS0REhh116MM5lzOzTwC/AULgJ865FypemYiIAOWNUeOcux+4v8K1iIjIBHRmooiI5xTUIiKeU1CLiHhOQS0i4jmrxEKXZtYNbDnGX58BvD6J5USB2hwPanM8HGub5znnWid6oiJBfTzMrNM511HtOk4ktTke1OZ4qESbNfQhIuI5BbWIiOd8DOrl1S6gCtTmeFCb42HS2+zdGLWIiIzmY49aRERGUFCLiHjOm6Cu1QV0zewnZtZlZmtGbGsxswfNbEPpdtqI5z5Xeg/Wm9kV1an6+JjZHDNbYWZrzewFM7u5tL1m221mGTN70syeLbX5K6XtNdvmYWYWmtkzZnZf6XFNt9nMNpvZ82a22sw6S9sq22bnXNV/KF4+dROwEEgBzwJnVruuSWrbxcC5wJoR2/4ZuLV0/1bgn0r3zyy1PQ0sKL0nYbXbcAxtngmcW7rfBLxUalvNtpviSkiNpftJYCWwtJbbPKLtnwbuAO4rPa7pNgObgRljtlW0zb70qA8toOucywLDC+hGnnPuMWDPmM3XAbeV7t8GvG/E9l865wadc68AGym+N5HinNvpnHu6dP8AsJbi2ps1225XdLD0MFn6cdRwmwHMbDZwDfCjEZtrus1voKJt9iWoJ1pAt71KtZwIJznndkIx1IC20vaaex/MbD6whGIPs6bbXRoCWA10AQ8652q+zcC3gc8CI1fMrfU2O+C3ZraqtKg3VLjNZS0ccAKUtYBuDNTU+2BmjcCvgE8553rMJmpecdcJtkWu3c65PHCOmTUD95jZ4iPsHvk2m9m1QJdzbpWZXVLOr0ywLVJtLrnQObfDzNqAB81s3RH2nZQ2+9KjjtsCuq+Z2UyA0m1XaXvNvA9mlqQY0rc75+4uba75dgM45/YBjwBXUtttvhB4r5ltpjhceZmZ/ZzabjPOuR2l2y7gHopDGRVtsy9BHbcFdO8FPly6/2Hg1yO2X29maTNbAJwGPFmF+o6LFbvOPwbWOue+OeKpmm23mbWWetKYWR1wObCOGm6zc+5zzrnZzrn5FP/P/s45dwM13GYzazCzpuH7wB8Da6h0m6t9BHXEUdOrKc4O2AR8vtr1TGK7fgHsBIYofrp+FJgOPAxsKN22jNj/86X3YD1wVbXrP8Y2X0Tx691zwOrSz9W13G7gLOCZUpvXAF8qba/ZNo9p/yUcnvVRs22mODPt2dLPC8NZVek26xRyERHP+TL0ISIib0BBLSLiOQW1iIjnFNQiIp5TUIuIeE5BLSLiOQW1iIjn/h+45zcWjpCinwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sample from model\n",
    "gen_history = history[0].unsqueeze(0)\n",
    "gen_mask = torch.zeros((1, 1), dtype=torch.bool)\n",
    "model.eval() # Turns off the dropout for evaluation. Need to do this to get repeatable evaluation outputs\n",
    "\n",
    "# Move forward in time\n",
    "wrong_cnt = 0\n",
    "for t in range(0, history.shape[0] - 1):\n",
    "    message_logits, channel_dist = model(gen_history, gen_mask, instruments, inst_mask)\n",
    "    print(channel_dist[-1])\n",
    "    \n",
    "    #message = torch.multinomial(torch.softmax(message_logits[-1].flatten(), dim=0), 1)\n",
    "    #channel = torch.multinomial(channel_dist[-1].flatten(), 1)\n",
    "    \n",
    "    message = torch.argmax(message_logits[-1].flatten())\n",
    "    channel = torch.argmax(channel_dist[-1].flatten())\n",
    "    \n",
    "    append = torch.tensor([message, channel]).view(1, 1, 2)\n",
    "    \n",
    "    gen_history = torch.cat((gen_history, append), dim=0)\n",
    "    \n",
    "    if gen_history[-1, 0, 0] != history[t + 1, 0, 0]:\n",
    "        print('Wrong message at time %d!' %(t))\n",
    "        wrong_cnt += 1\n",
    "        \n",
    "    if gen_history[-1, 0, 1] != history[t + 1, 0, 1]:\n",
    "        print('Wrong instrument at time %d!' %(t))\n",
    "        wrong_cnt += 1\n",
    "    \n",
    "    gen_mask = torch.cat((gen_mask, torch.zeros((1, 1), dtype=torch.bool)), dim=0)\n",
    "\n",
    "print(wrong_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test_history.npy', gen_history.squeeze(1).detach().numpy())\n",
    "np.save('test_instruments.npy', [instrument_numbers[i] for i in instruments[:-1, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class MIDIDataset(torch.utils.data.Dataset):\n",
    "    # CONSTRUCTOR: creates a list of recording files and a list\n",
    "    # of instrument files in root_dir. Assumes that the directory\n",
    "    # contains recording0.npy to recordingM.npy,\n",
    "    # as well as instruments0.npy to instrumentsM.npy\n",
    "    # ARGUMENTS\n",
    "    # root_dir: the directory to search\n",
    "    # chunk_size: we'll chunk the data into chunks of this size (or less)\n",
    "    def __init__(self, root_dir, chunk_size, transform=None):\n",
    "        files = os.listdir(root_dir)\n",
    "        recording_files = []\n",
    "        instrument_files = []\n",
    "        for file in files:\n",
    "            if 'recording' in file:\n",
    "                recording_files.append(os.path.join(root_dir, file))\n",
    "            elif 'instruments' in file:\n",
    "                instrument_files.append(os.path.join(root_dir, file))\n",
    "                \n",
    "        assert(len(recording_files) == len(instrument_files))\n",
    "        recording_files.sort()\n",
    "        instrument_files.sort()\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.recordings = []\n",
    "        self.instruments = []\n",
    "        for f in range(len(recording_files)):\n",
    "            recording = np.load(recording_files[f], allow_pickle=True)\n",
    "            inst = np.load(instrument_files[f], allow_pickle=True)\n",
    "            for chunk_start in range(0, recording.shape[0], chunk_size):\n",
    "                chunk_end = min(chunk_start + chunk_size, recording.shape[0])\n",
    "                self.recordings.append(recording[chunk_start:chunk_end])\n",
    "                self.instruments.append(inst)\n",
    "\n",
    "    # __len__\n",
    "    # RETURN: number of chunks in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.recordings)\n",
    "\n",
    "    # __getitem__\n",
    "    # ARGUMENTS\n",
    "    # idx: indicates which chunk to get\n",
    "    # RETURN: an instance with keys 'instruments', 'history'\n",
    "    # instance['history'] is an Lx2 numpy array containing messages and associated channels\n",
    "    # instance['instruments'] a numpy array of instrument numbers\n",
    "    def __getitem__(self, idx):\n",
    "        instance = {'history': self.recordings[idx], \\\n",
    "                    'instruments': self.instruments[idx]}\n",
    "        \n",
    "        if self.transform:\n",
    "            instance = self.transform(instance)\n",
    "            \n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn: takes a list of samples from the dataset and turns them into a batch.\n",
    "# ARGUMENTS\n",
    "# batch: a list of dictionaries\n",
    "# RETURN: a sample with keys 'history', 'instruments', and 'mask'\n",
    "# sample['history']: an LxBx2 tensor containing messages and their associated channels\n",
    "# sample['instruments']: a CxB tensor containing instrument numbers for each channel\n",
    "# sample['mask']: an LxB tensor containing False where a message is\n",
    "# valid, and True where it isn't (accounts for variable length sequences\n",
    "# and zero padding)\n",
    "# sample['inst_mask']: an NxB tensor containing False where an instrument exists and True\n",
    "# otherwise\n",
    "def collate_fn(batch):\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    # We size our tensors to accomodate the longest sequence and the largest number of instruments\n",
    "    max_inst = max([instance['instruments'].shape[0] for instance in batch])\n",
    "    longest_len = max([instance['history'].shape[0] for instance in batch])\n",
    "\n",
    "    sample = {'history': torch.zeros((longest_len, batch_size, 2), dtype=torch.long), \\\n",
    "              'instruments': torch.zeros((max_inst, batch_size), dtype=torch.long), \\\n",
    "              'mask': torch.ones((longest_len, batch_size), dtype=torch.bool), \\\n",
    "              'inst_mask': torch.ones((max_inst, batch_size), dtype=torch.bool)}\n",
    "\n",
    "    for b, instance in enumerate(batch):\n",
    "        instrument_idx = [instrument_numbers.index(i) for i in instance['instruments']]\n",
    "        \n",
    "        sample['instruments'][:len(instrument_idx), b] = torch.tensor(instrument_idx, dtype=torch.long)\n",
    "        \n",
    "        sample['inst_mask'][:len(instrument_idx), b] = False\n",
    "        \n",
    "        seq_length = instance['history'].shape[0]\n",
    "        sample['history'][:seq_length, b] = torch.tensor(instance['history'], dtype=torch.long)\n",
    "        sample['mask'][:seq_length, b] = False\n",
    "            \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_loss: computes the loss for the model over the batch\n",
    "# ARGUMENTS\n",
    "# model: EnsembleTransformer model\n",
    "# message_loss_fn: torch.nn.CrossEntropyLoss object\n",
    "# channel_loss_fn: torch.nn.NLLLoss object\n",
    "# batch: see collate_fn definition\n",
    "# RETURN: a scalar loss tensor\n",
    "def compute_loss(model, message_loss_fn, channel_loss_fn, batch):  \n",
    "    max_seq_length = batch['history'].shape[0]\n",
    "\n",
    "    message_logits, channel_dist = model(batch['history'][:-1], batch['mask'][:-1], batch['instruments'], batch['inst_mask'])\n",
    "    log_channel_dist = torch.log(channel_dist + 1e-10)\n",
    "\n",
    "    target_mask = torch.logical_not(batch['mask'][1:])\n",
    "\n",
    "    message_loss = message_loss_fn(message_logits[target_mask], batch['history'][1:, :, 0][target_mask])\n",
    "    channel_loss = channel_loss_fn(log_channel_dist[target_mask], batch['history'][1:, :, 1][target_mask])\n",
    "\n",
    "    return message_loss + channel_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 512\n",
    "heads = 4\n",
    "attention_layers = 6\n",
    "ff_size = 512\n",
    "\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleTransformer(message_dim, embed_dim, num_instruments, heads, attention_layers, ff_size)\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "    \n",
    "model.eval() # Training with eval just to see if we can overfit without dropout\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "chunk_size = 500\n",
    "\n",
    "train_dataset = MIDIDataset('train_unified', chunk_size)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "test_dataset = MIDIDataset('test_unified', chunk_size)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "message_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "channel_loss_fn = torch.nn.NLLLoss(ignore_index=-1)\n",
    "epochs = 20\n",
    "train_losses = np.zeros(epochs)\n",
    "test_losses = np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    model.train()\n",
    "    for b, batch in enumerate(train_dataloader):\n",
    "        print('Starting iteration %d' %(b))\n",
    "        loss = compute_loss(model, message_loss_fn, channel_loss_fn, batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        print(loss.data)\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                print(p.grad.norm())\n",
    "        optimizer.step()\n",
    "        \n",
    "    torch.save(model.state_dict(), 'unified_transformer_models/epoch' + str(epoch) + '.pth')\n",
    "\n",
    "    print('Computing test loss')\n",
    "    model.eval()\n",
    "    for batch in test_dataloader:\n",
    "        loss = compute_loss(model, message_loss_fn, channel_loss_fn, batch)\n",
    "        test_losses[epoch] += loss.data\n",
    "        \n",
    "    print('Computing train loss')\n",
    "    for batch in train_dataloader:\n",
    "        loss = compute_loss(model, message_loss_fn, channel_loss_fn, batch)\n",
    "        train_losses[epoch] += loss.data\n",
    "    \n",
    "    train_losses[epoch] /= len(train_dataloader)\n",
    "    test_losses[epoch] /= len(test_dataloader)\n",
    "    print('Train Loss: %f, Test Loss: %f' %(train_losses[epoch], test_losses[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # Disable dropout to make results repeatable\n",
    "\n",
    "time_steps = 500 # How many time steps do we sample?\n",
    "\n",
    "# Start with a time shift\n",
    "gen_history = torch.zeros((1, 1, 2), dtype=torch.long)\n",
    "gen_history[0, 0, 0] = 387\n",
    "gen_history[0, 0, 1] = -1\n",
    "\n",
    "# Violin\n",
    "instruments = torch.zeros((1, 1), dtype=torch.long)\n",
    "instruments[0, 0] = 0\n",
    "inst_mask = torch.zeros((1, 1), dtype=torch.bool)\n",
    "\n",
    "gen_mask = torch.zeros((1, 1), dtype=torch.bool)\n",
    "\n",
    "# Move forward in time\n",
    "for t in range(0, time_steps):\n",
    "    message_logits, channel_logits = model(gen_history, gen_mask, instruments)\n",
    "    \n",
    "    message = torch.multinomial(torch.softmax(message_logits[-1].flatten(), dim=0), 1)\n",
    "    channel = torch.multinomial(channel_logits[-1].flatten(), 1)\n",
    "    \n",
    "    append = torch.tensor([message, channel]).view(1, 1, 2)\n",
    "    \n",
    "    gen_history = torch.cat((gen_history, append), dim=0)\n",
    "    \n",
    "    gen_mask = torch.cat((gen_mask, torch.zeros((1, 1), dtype=torch.bool)), dim=0)\n",
    "\n",
    "print(wrong_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('gen_history.npy', gen_history.squeeze(1).detach().numpy())\n",
    "np.save('gen_instruments.npy', [instrument_numbers[i] for i in instruments[:, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
