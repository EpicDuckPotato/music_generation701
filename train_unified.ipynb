{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_notes = 128\n",
    "num_time_shifts = 100\n",
    "num_velocities = 32\n",
    "message_dim = 2*num_notes + num_velocities + num_time_shifts\n",
    "instrument_numbers = [0, 6, 40, 41, 42, 43, 45, 60, 68, 70, 71, 73]\n",
    "num_instruments = len(instrument_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Transformer definition\n",
    "Uses absolute position representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://pytorch.org/tutorials/beginner/transformer_tutorial.html.\n",
    "# Only change is the view/expand in forward (accounts for batches)\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.shape[0], :].unsqueeze(1).expand(-1, x.shape[1], -1)\n",
    "        return self.dropout(x)\n",
    "        \n",
    "\n",
    "# EnsembleTransformer: takes a history of MIDI messages \n",
    "# for instruments in an ensemble and generates a distribution for the next message,\n",
    "# as well as the instrument who should issue the message\n",
    "class EnsembleTransformer(torch.nn.Module):\n",
    "    # CONSTRUCTOR\n",
    "    # ARGUMENTS\n",
    "    # message_dim: dimension of a MIDI message\n",
    "    # embed_dim: dimension of message embedding\n",
    "    # num_instruments: number of instrument labels\n",
    "    # heads: number of attention heads\n",
    "    # attention_layers: number of attention layers\n",
    "    # ff_size: size of the feedforward output at the end of the decoder\n",
    "    def __init__(self, message_dim, embed_dim, num_instruments, heads, attention_layers, ff_size):\n",
    "        super(EnsembleTransformer, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.decoder_input_dim = 2*embed_dim\n",
    "        \n",
    "        # We concatenate the tanhed instrument embedding to each input message.\n",
    "        # We have num_instruments + 1 embeddings to account for the dummy instrument\n",
    "        # associated with time shift events\n",
    "        # We could potentially reduce the size of the instrument embedding, but\n",
    "        # for now let's keep it the same size as the message embedding\n",
    "        self.i_embedding = torch.nn.Embedding(num_instruments + 1, embed_dim)\n",
    "        \n",
    "        # Indicates where messages are in time (add this to the message embedding)\n",
    "        self.position_encoding = PositionalEncoding(embed_dim)\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(message_dim, embed_dim)\n",
    "        \n",
    "        # A decoder takes the history of messages (concatenated with their\n",
    "        # associated instrument encoding) and produces a decoding\n",
    "        decoder_layer = torch.nn.TransformerDecoderLayer(self.decoder_input_dim, heads, ff_size)\n",
    "        self.decoder = torch.nn.TransformerDecoder(decoder_layer, attention_layers)\n",
    "        \n",
    "        # The decoding is passed through a linear layer to get the logits for the next message        \n",
    "        self.message_logits = torch.nn.Linear(self.decoder_input_dim, message_dim)\n",
    "        \n",
    "        # The decoding is passed through a different linear layer to get a query for\n",
    "        # instrument-wise attention, which predicts which channel should generate\n",
    "        # this message\n",
    "        self.inst_query = torch.nn.Linear(self.decoder_input_dim, embed_dim)\n",
    "    \n",
    "    # forward: generates a probability distribution for the next MIDI message\n",
    "    # for an instrument, given the message history of the ensemble\n",
    "    # ARGUMENTS\n",
    "    # history: an LxBx2 tensor, where L is the length of the longest message history in\n",
    "    # the batch, and B is the batch size. The first index along dimension 2 stores the\n",
    "    # message number. The second stores the channel number. This should be END-PADDED\n",
    "    # along dimension 0\n",
    "    # mask: an LxB tensor, containing True in any locations where history contains\n",
    "    # padding\n",
    "    # instruments: a CxB tensor indicating the instrument number for each channel, where\n",
    "    # C is the maximum number of channels in the batch. This should be END-PADDED along dimension 0.\n",
    "    # The the last valid channel index should contain num_instruments (indicating the \"time-shift instrument\")\n",
    "    # RETURN: two tensors. The first is LxBxD, representing the distribution for the next message at each time\n",
    "    # step (need to take the softmax to get actual probabilities). The second is LxBxC, representing the\n",
    "    # distribution for the next channel at each time step (need to take the softmax to get actual probabilities)\n",
    "    def forward(self, history, mask, instruments):\n",
    "        L = history.shape[0] # longest length\n",
    "        B = history.shape[1] # batch size\n",
    "        C = instruments.shape[0]\n",
    "        assert(mask.shape == (L, B))\n",
    "        \n",
    "        # CxBxD\n",
    "        inst_embed = torch.tanh(self.i_embedding(instruments))\n",
    "        \n",
    "        # LxBxD, instrument embedding associated with each message\n",
    "        inst_tags = torch.gather(inst_embed, 0, history[:, :, 1].unsqueeze(2).expand(-1, -1, self.embed_dim))\n",
    "        \n",
    "        # LxBxD\n",
    "        message_embed = self.position_encoding(self.embedding(history[:, :, 0]))\n",
    "        \n",
    "        inputs = torch.cat((message_embed, inst_tags), dim=2)\n",
    "        \n",
    "        tgt_mask = torch.triu(torch.ones((L, L), dtype=torch.bool))\n",
    "        tgt_mask.fill_diagonal_(False)\n",
    "        \n",
    "        # No encoder, so no memory\n",
    "        memory = torch.ones((1, B, self.decoder_input_dim))\n",
    "        \n",
    "        tgt_key_padding_mask = mask.transpose(0, 1)\n",
    "        decoding = self.decoder(inputs, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        \n",
    "        # LxBxD\n",
    "        message_dist = self.message_logits(decoding)\n",
    "        \n",
    "        # CxLxBxD\n",
    "        inst_queries = self.inst_query(decoding).unsqueeze(0).expand(C, -1, -1, -1)\n",
    "        \n",
    "        prods = inst_queries*self.position_encoding(inst_embed).unsqueeze(1).expand(-1, L, -1, -1)/self.embed_dim\n",
    "        \n",
    "        # Dot-product attention (LxBxC)\n",
    "        channel_dist = torch.sum(prods, dim=3).permute(1, 2, 0)\n",
    "        \n",
    "        return message_dist, channel_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests for baseline transformer\n",
    "We train with model.eval() to disable dropout, since these tests try to get the model to overfit to a small sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model to overfit to a single song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_size = 512\n",
    "heads = 8\n",
    "attention_layers=  6\n",
    "embed_dim = 256\n",
    "\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleTransformer(message_dim, embed_dim, num_instruments, heads, attention_layers, ff_size)\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "    \n",
    "model.eval() # Training with eval just to see if we can overfit without dropout\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('overfit_single_file.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Loss: 7.689164\n",
      "Starting epoch 1\n",
      "Loss: 5.497065\n",
      "Starting epoch 2\n",
      "Loss: 5.151967\n",
      "Starting epoch 3\n",
      "Loss: 4.922511\n",
      "Starting epoch 4\n",
      "Loss: 4.550541\n",
      "Starting epoch 5\n",
      "Loss: 4.270918\n",
      "Starting epoch 6\n",
      "Loss: 4.170362\n",
      "Starting epoch 7\n",
      "Loss: 4.114559\n",
      "Starting epoch 8\n",
      "Loss: 4.153576\n",
      "Starting epoch 9\n",
      "Loss: 4.109065\n",
      "Starting epoch 10\n",
      "Loss: 4.041800\n",
      "Starting epoch 11\n",
      "Loss: 4.040359\n",
      "Starting epoch 12\n",
      "Loss: 4.070396\n",
      "Starting epoch 13\n",
      "Loss: 4.086609\n",
      "Starting epoch 14\n",
      "Loss: 4.077425\n",
      "Starting epoch 15\n",
      "Loss: 4.050911\n",
      "Starting epoch 16\n",
      "Loss: 4.024955\n",
      "Starting epoch 17\n",
      "Loss: 4.015656\n",
      "Starting epoch 18\n",
      "Loss: 4.023703\n",
      "Starting epoch 19\n",
      "Loss: 4.030677\n",
      "Starting epoch 20\n",
      "Loss: 4.023449\n",
      "Starting epoch 21\n",
      "Loss: 4.010141\n",
      "Starting epoch 22\n",
      "Loss: 4.002806\n",
      "Starting epoch 23\n",
      "Loss: 4.002307\n",
      "Starting epoch 24\n",
      "Loss: 4.003944\n",
      "Starting epoch 25\n",
      "Loss: 4.005427\n",
      "Starting epoch 26\n",
      "Loss: 4.006385\n",
      "Starting epoch 27\n",
      "Loss: 4.006069\n",
      "Starting epoch 28\n",
      "Loss: 4.003937\n",
      "Starting epoch 29\n",
      "Loss: 4.000935\n",
      "Starting epoch 30\n",
      "Loss: 3.998765\n",
      "Starting epoch 31\n",
      "Loss: 3.998184\n",
      "Starting epoch 32\n",
      "Loss: 3.998501\n",
      "Starting epoch 33\n",
      "Loss: 3.998266\n",
      "Starting epoch 34\n",
      "Loss: 3.996521\n",
      "Starting epoch 35\n",
      "Loss: 3.993913\n",
      "Starting epoch 36\n",
      "Loss: 3.992121\n",
      "Starting epoch 37\n",
      "Loss: 3.992044\n",
      "Starting epoch 38\n",
      "Loss: 3.992994\n",
      "Starting epoch 39\n",
      "Loss: 3.993604\n",
      "Starting epoch 40\n",
      "Loss: 3.993148\n",
      "Starting epoch 41\n",
      "Loss: 3.992022\n",
      "Starting epoch 42\n",
      "Loss: 3.991179\n",
      "Starting epoch 43\n",
      "Loss: 3.991076\n",
      "Starting epoch 44\n",
      "Loss: 3.991243\n",
      "Starting epoch 45\n",
      "Loss: 3.990986\n",
      "Starting epoch 46\n",
      "Loss: 3.990243\n",
      "Starting epoch 47\n",
      "Loss: 3.989471\n",
      "Starting epoch 48\n",
      "Loss: 3.989001\n",
      "Starting epoch 49\n",
      "Loss: 3.988790\n",
      "Starting epoch 50\n",
      "Loss: 3.988658\n",
      "Starting epoch 51\n",
      "Loss: 3.988549\n",
      "Starting epoch 52\n",
      "Loss: 3.988468\n",
      "Starting epoch 53\n",
      "Loss: 3.988351\n",
      "Starting epoch 54\n",
      "Loss: 3.988120\n",
      "Starting epoch 55\n",
      "Loss: 3.987822\n",
      "Starting epoch 56\n",
      "Loss: 3.987604\n",
      "Starting epoch 57\n",
      "Loss: 3.987508\n",
      "Starting epoch 58\n",
      "Loss: 3.987401\n",
      "Starting epoch 59\n",
      "Loss: 3.987154\n",
      "Starting epoch 60\n",
      "Loss: 3.986822\n",
      "Starting epoch 61\n",
      "Loss: 3.986581\n",
      "Starting epoch 62\n",
      "Loss: 3.986505\n",
      "Starting epoch 63\n",
      "Loss: 3.986488\n",
      "Starting epoch 64\n",
      "Loss: 3.986406\n",
      "Starting epoch 65\n",
      "Loss: 3.986243\n",
      "Starting epoch 66\n",
      "Loss: 3.986078\n",
      "Starting epoch 67\n",
      "Loss: 3.985964\n",
      "Starting epoch 68\n",
      "Loss: 3.985879\n",
      "Starting epoch 69\n",
      "Loss: 3.985770\n",
      "Starting epoch 70\n",
      "Loss: 3.985634\n",
      "Starting epoch 71\n",
      "Loss: 3.985498\n",
      "Starting epoch 72\n",
      "Loss: 3.985373\n",
      "Starting epoch 73\n",
      "Loss: 3.985267\n",
      "Starting epoch 74\n",
      "Loss: 3.985187\n",
      "Starting epoch 75\n",
      "Loss: 3.985120\n",
      "Starting epoch 76\n",
      "Loss: 3.985044\n",
      "Starting epoch 77\n",
      "Loss: 3.984944\n",
      "Starting epoch 78\n",
      "Loss: 3.984846\n",
      "Starting epoch 79\n",
      "Loss: 3.984769\n",
      "Starting epoch 80\n",
      "Loss: 3.984703\n",
      "Starting epoch 81\n",
      "Loss: 3.984626\n",
      "Starting epoch 82\n",
      "Loss: 3.984529\n",
      "Starting epoch 83\n",
      "Loss: 3.984438\n",
      "Starting epoch 84\n",
      "Loss: 3.984372\n",
      "Starting epoch 85\n",
      "Loss: 3.984318\n",
      "Starting epoch 86\n",
      "Loss: 3.984255\n",
      "Starting epoch 87\n",
      "Loss: 3.984186\n",
      "Starting epoch 88\n",
      "Loss: 3.984120\n",
      "Starting epoch 89\n",
      "Loss: 3.984062\n",
      "Starting epoch 90\n",
      "Loss: 3.984005\n",
      "Starting epoch 91\n",
      "Loss: 3.983946\n",
      "Starting epoch 92\n",
      "Loss: 3.983889\n",
      "Starting epoch 93\n",
      "Loss: 3.983830\n",
      "Starting epoch 94\n",
      "Loss: 3.983775\n",
      "Starting epoch 95\n",
      "Loss: 3.983723\n",
      "Starting epoch 96\n",
      "Loss: 3.983675\n",
      "Starting epoch 97\n",
      "Loss: 3.983632\n",
      "Starting epoch 98\n",
      "Loss: 3.983586\n",
      "Starting epoch 99\n",
      "Loss: 3.983536\n",
      "Starting epoch 100\n",
      "Loss: 3.983493\n",
      "Starting epoch 101\n",
      "Loss: 3.983454\n",
      "Starting epoch 102\n",
      "Loss: 3.983413\n",
      "Starting epoch 103\n",
      "Loss: 3.983370\n",
      "Starting epoch 104\n",
      "Loss: 3.983328\n",
      "Starting epoch 105\n",
      "Loss: 3.983293\n",
      "Starting epoch 106\n",
      "Loss: 3.983255\n",
      "Starting epoch 107\n",
      "Loss: 3.983222\n",
      "Starting epoch 108\n",
      "Loss: 3.983187\n",
      "Starting epoch 109\n",
      "Loss: 3.983153\n",
      "Starting epoch 110\n",
      "Loss: 3.983121\n",
      "Starting epoch 111\n",
      "Loss: 3.983091\n",
      "Starting epoch 112\n",
      "Loss: 3.983060\n",
      "Starting epoch 113\n",
      "Loss: 3.983032\n",
      "Starting epoch 114\n",
      "Loss: 3.983003\n",
      "Starting epoch 115\n",
      "Loss: 3.982975\n",
      "Starting epoch 116\n",
      "Loss: 3.982949\n",
      "Starting epoch 117\n",
      "Loss: 3.982924\n",
      "Starting epoch 118\n",
      "Loss: 3.982899\n",
      "Starting epoch 119\n",
      "Loss: 3.982877\n",
      "Starting epoch 120\n",
      "Loss: 3.982852\n",
      "Starting epoch 121\n",
      "Loss: 3.982831\n",
      "Starting epoch 122\n",
      "Loss: 3.982807\n",
      "Starting epoch 123\n",
      "Loss: 3.982786\n",
      "Starting epoch 124\n",
      "Loss: 3.982768\n",
      "Starting epoch 125\n",
      "Loss: 3.982749\n",
      "Starting epoch 126\n",
      "Loss: 3.982730\n",
      "Starting epoch 127\n",
      "Loss: 3.982710\n",
      "Starting epoch 128\n",
      "Loss: 3.982694\n",
      "Starting epoch 129\n",
      "Loss: 3.982674\n",
      "Starting epoch 130\n",
      "Loss: 3.982658\n",
      "Starting epoch 131\n",
      "Loss: 3.982643\n",
      "Starting epoch 132\n",
      "Loss: 3.982628\n",
      "Starting epoch 133\n",
      "Loss: 3.982614\n",
      "Starting epoch 134\n",
      "Loss: 3.982599\n",
      "Starting epoch 135\n",
      "Loss: 3.982585\n",
      "Starting epoch 136\n",
      "Loss: 3.982571\n",
      "Starting epoch 137\n",
      "Loss: 3.982559\n",
      "Starting epoch 138\n",
      "Loss: 3.982547\n",
      "Starting epoch 139\n",
      "Loss: 3.982533\n",
      "Starting epoch 140\n",
      "Loss: 3.982523\n",
      "Starting epoch 141\n",
      "Loss: 3.982513\n",
      "Starting epoch 142\n",
      "Loss: 3.982499\n",
      "Starting epoch 143\n",
      "Loss: 3.982490\n",
      "Starting epoch 144\n",
      "Loss: 3.982481\n",
      "Starting epoch 145\n",
      "Loss: 3.982469\n",
      "Starting epoch 146\n",
      "Loss: 3.982460\n",
      "Starting epoch 147\n",
      "Loss: 3.982450\n",
      "Starting epoch 148\n",
      "Loss: 3.982442\n",
      "Starting epoch 149\n",
      "Loss: 3.982434\n",
      "Starting epoch 150\n",
      "Loss: 3.982425\n",
      "Starting epoch 151\n",
      "Loss: 3.982418\n",
      "Starting epoch 152\n",
      "Loss: 3.982410\n",
      "Starting epoch 153\n",
      "Loss: 3.982403\n",
      "Starting epoch 154\n",
      "Loss: 3.982395\n",
      "Starting epoch 155\n",
      "Loss: 3.982389\n",
      "Starting epoch 156\n",
      "Loss: 3.982382\n",
      "Starting epoch 157\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-eaa57ee7b81d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss: %f'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "recording = np.load('preprocessed_data_unified/recording318.npy', allow_pickle=True)\n",
    "instruments_np = np.load('preprocessed_data_unified/instruments318.npy', allow_pickle=True)\n",
    "\n",
    "nsamples = 100\n",
    "\n",
    "history = torch.tensor(recording[:nsamples], dtype=torch.long).view(-1, 1, 2)\n",
    "mask = torch.zeros((history.shape[0], history.shape[1]), dtype=torch.bool)\n",
    "instruments = torch.tensor([instrument_numbers.index(i) for i in instruments_np] + [num_instruments], dtype=torch.long).view(-1, 1)\n",
    "\n",
    "num_channels = instruments.shape[0]\n",
    "\n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 300\n",
    "train_losses = np.zeros(epochs)\n",
    "\n",
    "target_messages = history[1:, :, 0].flatten()\n",
    "target_channels = history[1:, :, 1].flatten()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    \n",
    "    message_logits, channel_logits = model(history[:-1], mask[:-1], instruments)\n",
    "    \n",
    "    loss = loss_fn(message_logits.view(-1, message_dim), target_messages) + \\\n",
    "           loss_fn(channel_logits.view(-1, num_channels), target_channels)\n",
    "                \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses[epoch] = loss.data\n",
    "    print('Loss: %f' %(loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'overfit_single_instrument.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sample from model\n",
    "gen_history = history[0].unsqueeze(0)\n",
    "mask = torch.zeros((1, 1), dtype=torch.bool)\n",
    "model.eval() # Turns off the dropout for evaluation. Need to do this to get repeatable evaluation outputs\n",
    "\n",
    "# Move forward in time\n",
    "wrong_cnt = 0\n",
    "for t in range(1, history.shape[0]):\n",
    "    message_logits, channel_logits = model(gen_history, mask, instruments)\n",
    "    \n",
    "    # 1xD\n",
    "    message_probs = torch.nn.functional.softmax(message_logits[-1], dim=1)\n",
    "    channel_probs = torch.nn.functional.softmax(channel_logits[-1], dim=1)\n",
    "    \n",
    "    #next_message = torch.multinomial(message_probs, 1).view(1, 1, 1)\n",
    "    #next_channel = torch.multinomial(channel_probs, 1).view(1, 1, 1)\n",
    "    \n",
    "    # We still get 25 wrong if we use argmax\n",
    "    next_message = torch.argmax(message_probs, 1).view(1, 1, 1)\n",
    "    next_channel = torch.argmax(channel_probs, 1).view(1, 1, 1)\n",
    "    \n",
    "    append = torch.cat((next_message, next_channel), dim=2)\n",
    "    \n",
    "    gen_history = torch.cat((gen_history, append), dim=0)\n",
    "    \n",
    "    if gen_history[-1, 0, 0] != history[t, 0, 0]:\n",
    "        print('Wrong message at time %d!' %(t))\n",
    "        wrong_cnt += 1\n",
    "        \n",
    "    if gen_history[-1, 0, 1] != history[t, 0, 1]:\n",
    "        print('Wrong instrument at time %d!' %(t))\n",
    "        wrong_cnt += 1\n",
    "    \n",
    "    mask = torch.cat((mask, torch.zeros((1, 1), dtype=torch.bool)), dim=0)\n",
    "\n",
    "print(wrong_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test_history.npy', gen_history.squeeze(1).detach().numpy())\n",
    "np.save('test_instruments.npy', [instrument_numbers[i] for i in instruments[:-1, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class MIDIDataset(torch.utils.data.Dataset):\n",
    "    # CONSTRUCTOR: creates a list of recording files and a list\n",
    "    # of instrument files in root_dir. Assumes that the directory\n",
    "    # contains recording0.npy to recordingM.npy,\n",
    "    # as well as instruments0.npy to instrumentsM.npy\n",
    "    # ARGUMENTS\n",
    "    # root_dir: the directory to search\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        files = os.listdir(root_dir)\n",
    "        self.recordings = []\n",
    "        self.instrument_files = []\n",
    "        for file in files:\n",
    "            if 'recording' in file:\n",
    "                self.recordings.append(os.path.join(root_dir, file))\n",
    "            elif 'instruments' in file:\n",
    "                self.instrument_files.append(os.path.join(root_dir, file))\n",
    "                \n",
    "        assert(len(self.recordings) == len(self.instrument_files))\n",
    "        self.recordings.sort()\n",
    "        self.instrument_files.sort()\n",
    "        self.transform = transform\n",
    "\n",
    "    # __len__\n",
    "    # RETURN: the number of recording files in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.recordings)\n",
    "\n",
    "    # __getitem__\n",
    "    # ARGUMENTS\n",
    "    # idx: indicates which file to get\n",
    "    # RETURN: an instance with keys 'instruments', 'history'\n",
    "    # instance['history'] is an Lx2 numpy array containing messages and associated channels\n",
    "    # instance['instruments'] a numpy array of instrument numbers\n",
    "    def __getitem__(self, idx):\n",
    "        instance = {'history': np.load(self.recordings[idx], allow_pickle=True), \\\n",
    "                    'instruments': np.load(self.instrument_files[idx], allow_pickle=True)}\n",
    "        \n",
    "        if self.transform:\n",
    "            instance = self.transform(instance)\n",
    "            \n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn: takes a list of samples from the dataset and turns them into a batch.\n",
    "# ARGUMENTS\n",
    "# batch: a list of dictionaries\n",
    "# RETURN: a sample with keys 'history', 'instruments', and 'mask'\n",
    "# sample['history']: an LxBx2 tensor containing messages\n",
    "# sample['instruments']: a CxB tensor containing instrument numbers for each channel\n",
    "# sample['mask']: an LxB tensor containing False where a message is\n",
    "# valid, and True where it isn't (accounts for variable length sequences\n",
    "# and zero padding)\n",
    "# sample['nchan']: a length B tensor containing the number of channels for each batch\n",
    "# element (including the dummy time-shift channel)\n",
    "def collate_fn(batch):\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    # We size our tensors to accomodate the longest sequence and the largest number of channels\n",
    "    # (including the dummy time shift channel)\n",
    "    max_channels = max([instance['instruments'].shape[0] for instance in batch]) + 1\n",
    "    longest_len = max([instance['history'].shape[0] for instance in batch])\n",
    "\n",
    "    sample = {'history': torch.zeros((longest_len, batch_size, 2), dtype=torch.long), \\\n",
    "              'instruments': torch.zeros((max_channels, batch_size), dtype=torch.long), \\\n",
    "              'mask': torch.ones((longest_len, batch_size), dtype=torch.bool), \\\n",
    "              'nchan': torch.zeros(batch_size, dtype=torch.long)}\n",
    "\n",
    "    for b, instance in enumerate(batch):\n",
    "        instrument_idx = [instrument_numbers.index(i) for i in instance['instruments']]\n",
    "        \n",
    "        sample['instruments'][:len(instrument_idx), b] = torch.tensor(instrument_idx, dtype=torch.long)\n",
    "        # Add dummy channel\n",
    "        sample['instruments'][len(instrument_idx), b] = num_instruments\n",
    "        \n",
    "        # +1 to account for udmmy channel\n",
    "        sample['nchan'][b] = len(instrument_idx) + 1\n",
    "        \n",
    "        seq_length = instance['history'].shape[0]\n",
    "        sample['history'][:seq_length, b] = torch.tensor(instance['history'], dtype=torch.long)\n",
    "        sample['mask'][:seq_length, b] = False\n",
    "            \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_size = 512\n",
    "heads = 8\n",
    "attention_layers=  6\n",
    "embed_dim = 256\n",
    "grad_clip = 10\n",
    "\n",
    "model = EnsembleTransformer(message_dim, embed_dim, len(instrument_numbers), heads, attention_layers, ff_size)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "chunk_size = 200\n",
    "\n",
    "dataset = MIDIDataset('preprocessed_data_unified')\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "epochs = 20\n",
    "train_losses = np.zeros(epochs)\n",
    "test_losses = np.zeros(epochs) # TODO: train/test split\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    n_iter = 0\n",
    "    for b, batch in enumerate(dataloader):\n",
    "        max_seq_length = batch['history'].shape[0]\n",
    "        \n",
    "        num_targets = max_seq_length - 1 # Messages start from t = 0, but we start generating at t = 1\n",
    "            \n",
    "        print('Starting iteration %d' %(b))\n",
    "        \n",
    "        message_logits, channel_logits = model(batch['history'][:-1], batch['mask'][:-1], batch['instruments'])\n",
    "        \n",
    "        target_mask = torch.logical_not(batch['mask'][1:])\n",
    "        \n",
    "        num_valid_targets = target_mask.sum()\n",
    "        \n",
    "        target_messages = batch['history'][1:, :, 0][target_mask]\n",
    "    \n",
    "        message_loss = loss_fn(message_logits[target_mask], target_messages)/num_valid_targets\n",
    "        \n",
    "        channel_losses = torch.zeros(batch_size)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            channel_logits_i = channel_logits[:, :, :batch['nchan'][i]][target_mask]\n",
    "            target_channels = batch['history'][1:, i, 1][target_mask]\n",
    "            channel_losses[i] = loss_fn(channel_logits_i, target_channels)/num_valid_targets\n",
    "            \n",
    "        loss = message_loss + channel_losses\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses[epoch] = loss.data\n",
    "    \n",
    "    torch.save(model.state_dict(), 'trained_models_12_3/epoch' + str(epoch) + '.pth')\n",
    "    train_losses[epoch] /= n_iter\n",
    "    print('Loss: %f' %(train_losses[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('trained_models_12_2/epoch19.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # Disable dropout to make results repeatable\n",
    "\n",
    "time_steps = 200 # How many time steps do we sample?\n",
    "\n",
    "# Start with a time shift\n",
    "gen_history = torch.zeros((1, 1, 2), dtype=torch.long)\n",
    "gen_history[0, 0, 0] = 387\n",
    "\n",
    "# Violin\n",
    "instruments = torch.zeros((1, 1), dtype=torch.long)\n",
    "instruments[0, 0] = 2\n",
    "\n",
    "mask = torch.zeros((1, 1), dtype=torch.bool)\n",
    "\n",
    "# Move forward in time\n",
    "for t in range(1, time_steps):\n",
    "    message_logits, channel_logits = model(gen_history, mask, instruments)\n",
    "    \n",
    "    # 1xD\n",
    "    message_probs = torch.nn.functional.softmax(message_logits[-1], dim=1)\n",
    "    channel_probs = torch.nn.functional.softmax(channel_logits[-1], dim=1)\n",
    "    \n",
    "    next_message = torch.multinomial(message_probs, 1).view(1, 1, 1)\n",
    "    next_channel = torch.multinomial(channel_probs, 1).view(1, 1, 1)\n",
    "    \n",
    "    append = torch.cat((next_message, next_channel), dim=2)\n",
    "    \n",
    "    gen_history = torch.cat((gen_history, append), dim=0)\n",
    "    \n",
    "    mask = torch.cat((mask, torch.zeros((1, 1), dtype=torch.bool)), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('gen_history.npy', gen_history.squeeze(1).detach().numpy())\n",
    "np.save('gen_instruments.npy', [instrument_numbers[i] for i in instruments[:-1, 0]])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
