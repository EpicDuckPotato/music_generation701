{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import functools\n",
    "from IPython import display as ipythondisplay\n",
    "from tqdm import tqdm\n",
    "!apt-get install abcmidi timidity > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If CPU\n",
    "dev = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If GPU\n",
    "dev = torch.device('gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "instrument_numbers = [0, 6, 40, 41, 42, 43, 45, 60, 68, 70, 71, 73]\n",
    "num_instruments = len(instrument_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class MIDIDataset(torch.utils.data.Dataset):\n",
    "    # CONSTRUCTOR: creates an array of message chunks. Assumes that the directory\n",
    "    # contains recording0.npy to recordingM.npy\n",
    "    # ARGUMENTS\n",
    "    # root_dir: the directory to search\n",
    "    # chunk_size: we'll chunk the data into chunks of this size (or less)\n",
    "    def __init__(self, root_dir, chunk_size, transform=None):\n",
    "        recording_files = []\n",
    "        instrument_files = []\n",
    "        for file in os.listdir(root_dir):\n",
    "            if 'recording' in file:\n",
    "                recording_files.append(os.path.join(root_dir, file))\n",
    "            elif 'instruments' in file:\n",
    "                instrument_files.append(os.path.join(root_dir, file))\n",
    "                \n",
    "        assert(len(recording_files) == len(instrument_files))\n",
    "        recording_files.sort()\n",
    "        instrument_files.sort()\n",
    "        \n",
    "        self.chunks = []\n",
    "        self.masks = []\n",
    "        self.instruments = []\n",
    "        \n",
    "        ch = 0\n",
    "        for f in range(len(recording_files)):\n",
    "            data = np.load(recording_files[f])\n",
    "            inst = [instrument_numbers.index(i) for i in np.load(instrument_files[f])]\n",
    "            nchunks = int(np.ceil(data.shape[0]/chunk_size))\n",
    "            self.chunks += [torch.zeros((chunk_size, 2), dtype=torch.long, device=dev) for c in range(nchunks)]\n",
    "            self.masks += [torch.ones(chunk_size, dtype=torch.bool, device=dev) for c in range(nchunks)]\n",
    "            self.instruments += [torch.tensor(inst, dtype=torch.long, device=dev) for c in range(nchunks)]\n",
    "            for chunk_start in range(0, data.shape[0], chunk_size):\n",
    "                chunk_end = min(chunk_start + chunk_size, data.shape[0])\n",
    "                size = chunk_end - chunk_start\n",
    "                self.chunks[ch][:size] = torch.tensor(data[chunk_start:chunk_end]).to(dev)\n",
    "                self.masks[ch][:size] = False\n",
    "                ch += 1\n",
    "            \n",
    "            if f%100 == 0:\n",
    "                print(f)\n",
    "            \n",
    "        self.transform = transform\n",
    "\n",
    "    # __len__\n",
    "    # RETURN: the number of chunks in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "\n",
    "    # __getitem__\n",
    "    # ARGUMENTS\n",
    "    # idx: indicates which chunk to get\n",
    "    # RETURN: instance, a dictionary with keys 'history,' 'mask,' and 'instruments'.\n",
    "    # Both values associated with these keys are length L tensors\n",
    "    def __getitem__(self, idx):  \n",
    "        instance = {'history': self.chunks[idx], \\\n",
    "                    'mask': self.masks[idx],\n",
    "                    'instruments': self.instruments[idx]}\n",
    "\n",
    "        if self.transform:\n",
    "            instance = self.transform(instance)\n",
    "            \n",
    "        return instance\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    chunk_size = batch[0]['history'].shape[0]\n",
    "    ninst = [instance['instruments'].shape[0] for instance in batch]\n",
    "    max_inst = max(ninst)\n",
    "    B = len(batch)\n",
    "    sample = {'history': torch.zeros((chunk_size, 2, B), dtype=torch.long, device=dev), \\\n",
    "              'mask': torch.ones((chunk_size, B), dtype=torch.bool, device=dev),\n",
    "              'instruments': torch.zeros((max_inst, B), dtype=torch.long, device=dev), \\\n",
    "              'inst_mask': torch.ones((max_inst, B), dtype=torch.bool, device=dev)}\n",
    "    \n",
    "    for b, instance in enumerate(batch):\n",
    "        sample['history'][:, :, b] = instance['history']\n",
    "        sample['mask'][:, b] = instance['mask']\n",
    "        sample['instruments'][:ninst[b], b] = instance['instruments']\n",
    "        sample['inst_mask'][:ninst[b], b] = False\n",
    "        \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2545"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = MIDIDataset('train_unified', 1000)\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "636"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = MIDIDataset('test_unified', 1000)\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiGRU: generates a sequence of MIDI messages and their associated channels\n",
    "class MultiGRU(torch.nn.Module):\n",
    "    # CONSTRUCTOR\n",
    "    # ARGUMENTS\n",
    "    # message_dim: dimension of a MIDI message\n",
    "    # embed_dim: dimension of message embedding\n",
    "    # hidden_size: hidden size of recurrent unit\n",
    "    # recurrent_layers: number of recurrent units stacked on top of each other\n",
    "    # num_instruments: number of program numbers in dataset\n",
    "    def __init__(self, message_dim, embed_dim, hidden_size, recurrent_layers, num_instruments):\n",
    "        super(MultiGRU, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embedding = torch.nn.Embedding(message_dim, embed_dim)\n",
    "        self.inst_embedding = torch.nn.Embedding(num_instruments, embed_dim)\n",
    "        self.rnn = torch.nn.GRU(embed_dim, hidden_size, num_layers=recurrent_layers)\n",
    "        self.message_logits = torch.nn.Linear(hidden_size, message_dim)\n",
    "        \n",
    "        # Instrument-wise attention\n",
    "        self.inst_logits = torch.nn.Linear(hidden_size, embed_dim)\n",
    "        self.inst_attention = torch.nn.MultiheadAttention(embed_dim, heads)\n",
    "    \n",
    "    # forward: generates a probability distribution for the next MIDI message at each time step in a sequence\n",
    "    # ARGUMENTS\n",
    "    # history: an Lx2xB tensor, where L is the length of the longest message sequence in\n",
    "    # the batch, and B is the batch size. This should be END-PADDED along dimension 0.\n",
    "    # The first index along dimension 1 contains the message, the second contains the channel.\n",
    "    # All time-shifts should be associated with channel -1\n",
    "    # mask: LxB, contains False where messages exist, and True otherwise\n",
    "    # instruments: NxB, instrument numbers for each instance\n",
    "    # inst_mask: NxB, contains False where instruments exist, and True otherwise\n",
    "    # RETURN: an LxBxD tensor representing the logits for the next message in each batch\n",
    "    def forward(self, history, mask, instruments, inst_mask):\n",
    "        L = history.shape[0]\n",
    "        B = history.shape[1]\n",
    "        \n",
    "        inst_embed = self.inst_embedding(instruments)\n",
    "        message_embed = self.embedding(history[:, 0, :])\n",
    "        \n",
    "        time_shift_mask = history[:, 1, :] < 0\n",
    "        channel_sel = history[:, 1, :].clone()\n",
    "        channel_sel[time_shift_mask] = 0\n",
    "        \n",
    "        # LxBxD\n",
    "        inst_tags = torch.gather(inst_embed, 0, channel_sel.unsqueeze(2).expand(-1, -1, self.embed_dim))\n",
    "        inst_tags[time_shift_mask] = 0\n",
    "        \n",
    "        inputs = message_embed + inst_tags + torch.sum(inst_embed, dim=0).unsqueeze(0).expand(L, -1, -1)\n",
    "        \n",
    "        src_mask = torch.triu(torch.ones((history.shape[0], history.shape[0]), dtype=torch.bool))\n",
    "        src_mask.fill_diagonal_(False)\n",
    "        \n",
    "        rnn_out, hidden = self.gru(inputs)\n",
    "        \n",
    "        message_logits = self.message_logits(rnn_out)\n",
    "        inst_logits = torch.tanh(self.inst_logits(rnn_out))\n",
    "        \n",
    "        # attn_weights is BxLxN\n",
    "        attn_output, attn_weights = self.inst_attention(inst_logits, inst_embed, inst_embed, key_padding_mask=inst_mask.transpose(0, 1))\n",
    "        return message_logits, attn_weights.transpose(0, 1)\n",
    "    \n",
    "    # forward: generates a probability distribution for the next MIDI message at each time step in a sequence, as well as the associated channel\n",
    "    # ARGUMENTS\n",
    "    # history: an Lx2xB tensor, where L is the length of the longest message sequence in\n",
    "    # the batch, and B is the batch size. This should be END-PADDED along dimension 0.\n",
    "    # The first index along dimension 1 contains the message, the second contains the channel.\n",
    "    # All time-shifts should be associated with channel -1\n",
    "    # mask: LxB, contains False where messages exist, and True otherwise\n",
    "    # instruments: NxB, instrument numbers for each instance\n",
    "    # inst_mask: NxB, contains False where instruments exist, and True otherwise\n",
    "    # RETURN: an LxBxD tensor representing the logits for the next message in each batch\n",
    "    def forward(self, history, mask, instruments, inst_mask):\n",
    "        L = history.shape[0]\n",
    "        B = history.shape[1]\n",
    "        \n",
    "        inst_embed = self.inst_embedding(instruments)\n",
    "        message_embed = self.embedding(history[:, 0, :])\n",
    "        \n",
    "        time_shift_mask = history[:, 1, :] < 0\n",
    "        channel_sel = history[:, 1, :].clone()\n",
    "        channel_sel[time_shift_mask] = 0\n",
    "        \n",
    "        # LxBxD\n",
    "        inst_tags = torch.gather(inst_embed, 0, channel_sel.unsqueeze(2).expand(-1, -1, self.embed_dim))\n",
    "        inst_tags[time_shift_mask] = 0\n",
    "        \n",
    "        inputs = message_embed + inst_tags + torch.sum(inst_embed, dim=0).unsqueeze(0).expand(L, -1, -1)\n",
    "        \n",
    "        rnn_out, hidden = self.rnn(inputs)\n",
    "        \n",
    "        message_logits = self.message_logits(rnn_out)\n",
    "        inst_logits = torch.tanh(self.inst_logits(rnn_out))\n",
    "        \n",
    "        # attn_weights is BxLxN\n",
    "        attn_output, attn_weights = self.inst_attention(inst_logits, inst_embed, inst_embed, key_padding_mask=inst_mask.transpose(0, 1))\n",
    "        return message_logits, attn_weights.transpose(0, 1)\n",
    "    \n",
    "    # generate: predicts the next MIDI message and channel, given the previous message and channel, as well as the instrument set\n",
    "    # and previous hidden state\n",
    "    # ARGUMENTS\n",
    "    # prev_token: a Lx2x1 tensor. \n",
    "    # The first index along dimension 1 contains the message, the second contains the channel.\n",
    "    # All time-shifts should be associated with channel -1\n",
    "    # instruments: Nx1, instrument numbers\n",
    "    # hidden: previous hidden state\n",
    "    # RETURN: a 1x2x1 tensor containing the predicted next message and channel, as well as the new hidden state\n",
    "    def generate(self, prev_token, instruments, hidden):\n",
    "        L = prev_token.shape[0]\n",
    "        inst_embed = self.inst_embedding(instruments)\n",
    "        message_embed = self.embedding(prev_token[:, 0, :])\n",
    "        \n",
    "        time_shift_mask = prev_token[:, 1, :] < 0\n",
    "        channel_sel = prev_token[:, 1, :].clone()\n",
    "        channel_sel[time_shift_mask] = 0\n",
    "        \n",
    "        # Lx1xD\n",
    "        inst_tags = torch.gather(inst_embed, 0, channel_sel.unsqueeze(2).expand(-1, -1, self.embed_dim))\n",
    "        inst_tags[time_shift_mask] = 0\n",
    "        \n",
    "        inputs = message_embed + inst_tags + torch.sum(inst_embed, dim=0).unsqueeze(0).expand(L, -1, -1)\n",
    "        \n",
    "        rnn_out, new_hidden = self.rnn(inputs)\n",
    "        \n",
    "        message_logits = self.message_logits(rnn_out)\n",
    "        inst_logits = torch.tanh(self.inst_logits(rnn_out))\n",
    "        \n",
    "        # attn_weights is BxLxN\n",
    "        attn_output, attn_weights = self.inst_attention(inst_logits, inst_embed, inst_embed)\n",
    "        \n",
    "        # Now it's LxBxN\n",
    "        attn_weights = attn_weights.transpose(0, 1)\n",
    "        \n",
    "        new_message = torch.multinomial(torch.nn.functional.softmax(message_logits[-1].flatten(), dim=0), 1).view(1, 1)\n",
    "        new_channel = torch.multinomial(attn_weights[-1].flatten(), 1).view(1, 1)\n",
    "        new_token = torch.cat((new_message, new_channel), dim=1).unsqueeze(2)\n",
    "        return new_token, new_hidden\n",
    "\n",
    "def compute_loss(model, batch):\n",
    "    message_logits, channel_weights = model(batch['history'][:-1], batch['mask'][:-1], batch['instruments'], batch['inst_mask'])\n",
    "    \n",
    "    target_mask = torch.logical_not(batch['mask'][1:].flatten())\n",
    "    target_messages = batch['history'][1:, 0].flatten()\n",
    "    message_loss = torch.nn.functional.cross_entropy(message_logits.view(-1, message_dim)[target_mask], target_messages[target_mask])\n",
    "    \n",
    "    target_channels = batch['history'][1:, 1].flatten()\n",
    "    max_inst = batch['instruments'].shape[0]\n",
    "    channel_loss = torch.nn.functional.nll_loss(channel_weights.reshape(-1, max_inst)[target_mask], target_channels[target_mask], ignore_index=-1)\n",
    "    \n",
    "    return message_loss + channel_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization parameters:\n",
    "epochs = 10\n",
    "batch_size = 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Model parameters: \n",
    "num_notes = 128\n",
    "num_time_shifts = 100\n",
    "message_dim = 2*num_notes + num_time_shifts\n",
    "embed_dim = 512\n",
    "hidden_size = 1024\n",
    "recurrent_layers = 3\n",
    "ff_size = 512\n",
    "\n",
    "# Checkpoint location: \n",
    "checkpoint_dir = 'multi_gru_checkpoints'\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "model = MultiGRU(message_dim, embed_dim, hidden_size, recurrent_layers, num_instruments).to(dev)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses = [0 for epoch in range(epochs)]\n",
    "test_losses = [0 for epoch in range(epochs)]\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch %d' %(epoch))\n",
    "    model.train()\n",
    "    for b, batch in enumerate(train_dataloader):\n",
    "        if b%100 == 0:\n",
    "            print('Starting iteration %d' %(b))\n",
    "        loss = compute_loss(model, batch)\n",
    "        optimizer.zero_grad()\n",
    "        print(loss.data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    torch.save(model.state_dict(), checkpoint_dir + '/epoch' + str(epoch) + '.pth')\n",
    "        \n",
    "    model.eval()\n",
    "    print('Computing test loss')\n",
    "    for b, batch in enumerate(test_dataloader):\n",
    "        test_losses[epoch] += compute_loss(model,  batch).data\n",
    "        \n",
    "    test_losses[epoch] /= len(test_dataloader)\n",
    "        \n",
    "    print('Computing train loss')\n",
    "    for b, batch in enumerate(train_dataloader):\n",
    "        train_losses[epoch] += compute_loss(model, batch).data\n",
    "        \n",
    "    train_losses[epoch] /= len(train_dataloader)\n",
    "        \n",
    "    print('Train loss: %f, Test loss: %f' %(train_losses[epoch], test_losses[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "COLOR = 'white'\n",
    "plt.rcParams['text.color'] = COLOR\n",
    "plt.rcParams['axes.labelcolor'] = COLOR\n",
    "plt.rcParams['xtick.color'] = COLOR\n",
    "plt.rcParams['ytick.color'] = COLOR\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 100 # 200 e.g. is really fine, but slower\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Loss (Nats)')\n",
    "plt.title('Training Loss')\n",
    "plt.savefig('transformer_baseline_train_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Loss (Nats)')\n",
    "plt.title('Test Loss')\n",
    "plt.savefig('transformer_baseline_test_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(checkpoint_dir + '/train_losses.npy', np.array(train_losses))\n",
    "np.save(checkpoint_dir + '/test_losses.npy', np.array(test_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiGRU(message_dim, embed_dim, hidden_size, recurrent_layers, num_instruments).to(dev)\n",
    "model.load_state_dict(torch.load(checkpoint_dir + '/epoch9.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from model\n",
    "def generate_music(model, primer, instruments, gen_length=1000):\n",
    "    gen = primer.clone() # Lx2x1\n",
    "    hidden = None\n",
    "    for i in range(gen_length):\n",
    "        new_token, hidden = model.generate(gen, instruments, hidden)\n",
    "        gen = torch.cat((gen, new_token), dim=0)\n",
    "        \n",
    "    return gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'message' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-521b5aef69fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minstruments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprimer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_music\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstruments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-a4a2c2465012>\u001b[0m in \u001b[0;36mgenerate_music\u001b[0;34m(model, primer, instruments, gen_length)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mnew_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstruments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-6e3cb443a46f>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prev_token, instruments, hidden)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mnew_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mnew_channel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mnew_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'message' is not defined"
     ]
    }
   ],
   "source": [
    "instruments = torch.tensor([0], dtype=torch.long, device=dev).view(1, 1)\n",
    "primer = torch.tensor([16, 0], dtype=torch.long, device=dev).view(1, 2, 1)\n",
    "gen = generate_music(model, primer, instruments, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('multi_transformer_midi.npy', generated_music.squeeze(2).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 16,  66, 210, 198, 204, 200, 235,  76, 179, 193])\n"
     ]
    }
   ],
   "source": [
    "print(gen[:10, 0, :].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
